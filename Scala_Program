  
  Print all values of a range:
---------------------------------------
scala> (1 to 10).foreach(println)
1
2
3
4
5
6
7
8
9
10

scala> Array.range(1,10).foreach(println)
1
2
3
4
5
6
7
8
9


def table(x:Int)={
val l=List.range(1,10) 
l.map([Int,Int]e=> e*l)
}

println(l)}}
  Print all not null strings: (for with iterator guard)
 ------------------------------------
 scala> for{arr<-a.split(",") if(arr.length>0)} println(arr)
scala
spark
hadoop
spark
bigdata

scala> a
res8: String = scala,spark,hadoop,,spark,bigdata

For with iterator guard:**********

Print even nos. between 1 to 10
-----------------------------------------
scala> for(i<-1 to 10  if(i%2==0))  println(i)
2
4
6
8
10

 
Write a program to convert uppercase:
-----------------------------------------------

scala> sArr
res12: Array[String] = Array(scala, spark, hadoop, bigdata)

scala> for(a<-sArr) println(a.toUpperCase)
SCALA
SPARK
HADOOP
BIGDATA

solu:2----------                          ^

scala> for(a<-sArr) yield a.toUpperCase
res16: Array[String] = Array(SCALA, SPARK, HADOOP, BIGDATA)
 
 Q. Print into uppercase and length(like in the form of Tuple)
 -----------------------------------------------------------------------
 scala> sArr
res17: Array[String] = Array(scala, spark, hadoop, bigdata)

scala> for(a<-sArr) yield(a,a.length,a.toUpperCase)
res18: Array[(String, Int, String)] = Array((scala,5,SCALA), (spark,5,SPARK), (hadoop,6,HADOOP), (bigdata,7,BIGDATA))

Q: conver to upper case for the strings which are greater than 5 and contains b in it
--------------------------------

scala> sArr
res30: Array[String] = Array(scala, spark, hadoop, bigdata)

scala> for(a<-sArr if(a.length>5 && a.contains("b"))) yield a.toUpperCase
res31: Array[String] = Array(BIGDATA)

using HOF (but not able to convert to uppercase)
-----------------------------------------
scala> a
res17: List[String] = List(SADA, SSDA, sadaf)

scala> a.filter(e=>e.length>4 && e.contains("d")).map(e=>e.toUpperCase)
res16: List[String] = List(SADAF)

Q. Display the count of numbers	between 1 to 100 which are not divisble by 2,3 and 5	
---------------------------------------------------------
  
		scala> def count:Int={
		 | val out = for(i <- 1 to 100 if((i%2 != 0) && (i%3 != 0) && (i%5 != 0))) yield i
		 | out.length
		 | }
		count: Int

		scala> count
		res8: Int = 26
2>using HOF:
-----------------------
scala> def cnt={val inp=List.range(1,100)
     | inp.count(ele=>{ele%2!=0 && ele%3!=0 && ele%5!=0})}
cnt: Int

scala> cnt
res11: Int = 26
	
		Q. Generate all possible 3 digit numbers with the combination of 1,2,3.
		---------------------------------------------------------------------------------------
	scala>  for(i<-1 to 3; j<-1 to 3; k<-1 to 3) println(s"$i $j $k")
1 1 1
1 1 2
1 1 3
1 2 1
1 2 2
1 2 3
1 3 1
1 3 2
1 3 3
2 1 1
2 1 2
2 1 3
2 2 1
2 2 2
2 2 3
2 3 1
2 3 2
2 3 3
3 1 1
3 1 2
3 1 3
3 2 1
3 2 2
3 2 3
3 3 1
3 3 2
3 3 3

Q. to print the table of any number.
---------------------------------------

scala> def tableno(x:Int)= {
     |       for(i<-1 to 10) println(s"$x*$i = ${x*i}")}
tableno: (x: Int)Unit

scala> tableno(8)
8*1 = 8
8*2 = 16
8*3 = 24
8*4 = 32
8*5 = 40
8*6 = 48
8*7 = 56
8*8 = 64
8*9 = 72
8*10 = 80

Q: Arithmetic calculator
---------------------------------------
scala> def Arithops(x:Int,y:Int,op:Char):Any={

 op match {
  case '+' => x+y
  case '*' => x*y
  case '-' => x-y
  case '/' => x/y
  case '%' => x%y 
  case _ => "unknown operator"}}
  
slou-2------------------------------------ 
scala> def arithop(x:Int,y:Int,z:Char)={
     | z match{
     | case '+'=>x+y
     | case '-'=>x-y
     | case '/'=>x/y
     | case '%'=>x%y
     | case '*'=>x*y
     | case x=> s"unknown operation $x"}
     | }
arithop: (x: Int, y: Int, z: Char)Any

scala> arithop(2,3,'+')
res38: Any = 5

scala> arithop(2,3,'*')
res39: Any = 6

scala> arithop(2,3,'/')
res40: Any = 0

scala> arithop(2,3,'%')
res41: Any = 2

scala> arithop(2,3,'-')
res42: Any = -1 
Q:- matching days in a week:- (using case )
----------------------------------
scala> def weekday(x:String)={
     |   x match {
     |  case "monday"| "tuesday" | "wednesday" | "thursday" | "friday" => "Weekdaay"
     | case "saturday" | "sunday" => "Weekend"
     | case _ => "wrong entry"}}
	 
weekday: (x: String)String


scala> weekday("monday")
res15: String = Weekdaay

scala> weekday("sunonday")
res16: String = wrong entry

scala> weekday("sunday")
 res17: String = Weekend
 
using Map collection:-
--------------------------------
scala> val weekday = Map("monday"-> "weekday","tuesday"-> "weekday", "wednseday"->"weekday","thursda
y" -> "weekday","friday"-> "weekday" ,"saturday"->"weekend","sunday"->"weekend")
weekday: scala.collection.immutable.Map[String,String] = Map(wednseday -> weekday, sunday -> weekend
, tuesday -> weekday, monday -> weekday, friday -> weekday, thursday -> weekday, saturday -> weekend
)

scala> val day= weekday("monday")
day: String = weekday

scala> val day= weekday("sunday")
day: String = weekend

 
  To find max of 3 nos.:
  -----------------------------
  scala> val (a,b,c)=(12,13,3)
a: Int = 12
b: Int = 13
c: Int = 3
scala> if (a>b && a>c) a  else if (b>a && b>c) b else c
res33: Int = 13
    or---
scala> if (a>b && a>c) println(" a  is greater") else if (b>a && b>c) (" b  is greater") else (" c is greater")
res38: Any = " b  is greater"

or-----------

if(a>b & a>c) println("a is greater") else if(b>a & b>c) println("b is greater") else println("c is greater")
-----------------------------
scala> if(a>b) max = a else max=b

		scala> max
		res25: Int = 78
     		
		scala is EOL(Expression Oriented Language)
		if else can be treated as expression for immutability
		
		Statement does not return anything, expression returns.
		
		**If u are expecting if-else should return something, use it as expression.Assign the o/p of if-else to a variable.
		
scala> val out=if (a>b && a>c) println(" a  is greater") else if (b>a && b>c) (" b  is greater") els
e (" c is greater")
out: Any = " b  is greater"

scala> out
res45: Any = " b  is greater"

Nested function:		
------------------------------------
def max(w:Int,x:Int,y:Int,z:Int) = {
def max(a:Int,b:Int,c:Int) = {
if(a>b && b>c)  a 
else if(b>c &&b>a)  b 
else c}
max(w,max(x,y,z))}

def max(a: Int, b: Int, c: Int) = {      
        def max(x: Int, y: Int) = {if (x > y) x else y
     }
println( max)
        max(a, max(b, c))

      }

max(a,max(max(b,c),d))
def max (a:AnyVal,b:AnyVal,c:AnyVal) = {
def max (x:AnyVal,y:AnyVal) = {
if(a>b){ println("hi, a is greater")}
else  b }
max(a,max(b,c))}

max of 5 numbers---------------------------------
scala> def max(a:Int,b:Int,c:Int,d:Int,e:Int)={
     | def max(a:Int,b:Int)=if(a>b) a else b
     | max(e,max(d,max(a,max(b,c))))}
max: (a: Int, b: Int, c: Int, d: Int, e: Int)Int

scala> max(0,2,-1,-7,80)
res53: Int = 80
***************

scala> def add(x:Int,y:Int)={println(x+y)
     | "hello"}
add: (x: Int, y: Int)String

scala> add(3,4)
7
res6: String = hello


def sum(x:Int,y:Int)={var sum=0
if (x>0 ||y>0) {
sum=x+y
println(sum)
}
else{
println("0 value")}}
--------------------------
 def max (a:Int,b:Int)={
if(a>b)
{ println ( s”$a is greater”)
}
else {
println(b+”is greater”)
}
}

def mult(a:Int=5,b:Int,c:Int) =  println(a*b*c)	

FInd sum of all values-----------------------
solu-1  *************
def mult(a:Int*) =  {
var sum=1
for(arg<-a)
{ sum=sum*arg
}
println(“total sum is:” + sum)
}
solu-2 (using HOF reduce)
**************
scala> def sum(a:Int*)={
     | a.reduce(_+_)}
sum: (a: Int*)Int

scala> sum(1,2)
res54: Int = 3

scala> sum(2,4,5,6)
res55: Int = 17

Factorial:
---------------------
val n=4
def fact(n: Int): Int= {
 var f=1
for (a <- 1 to n) {
f= f*a;
}  
f 
}
Factorial
  def factorial(n: Int): Int = { 
          
        var f = 1
        for(i <- 1 to n) 
        { 
            f = f * i; 
        } 
          
        return f 
    } 
  

println(s”factorial of $n is $a”)}
n=4
def fact(n:Int)={
var a=1;var f=0
while(n > 1 && a <= n) {
f = f+ a *n 
println( f )
a=a+1 
}
 }


Functions:
a.isInstanceOf[Int]
Boolean = true

Class: 9-Jul-19
************************************
def mul[T](a:T,b:T)= a match{
case x:Int=> x*b.asInstanceOf[Int] ; 
case x:Float=> x*b.asInstanceOf[Float]
case x:Double=> x*b.asInstanceOf[Double]
case x=> "sorry,wrong data"}

Functional Object::

scala> var obj:Int=>Int=doubler
obj: Int => Int = <function1>

scala> obj
res1: Int => Int = <function1>

scala> obj(10)
res2: Int 

scala>  def f1()="hi"
f1: ()String

scala> val ob:()=>String=f1
ob: () => String = <function0>

scala> ob()
res3: String = hi

Anonymous /literal Function
********************************
scala> val obj2=(x:Int)=>x+10
obj2: Int => Int = <function1>

scala> obj2(3)
res4: Int = 13

scala> def add(x:Int,y:Int)=x+y
add: (x: Int, y: Int)Int

scala> val ob:(Int,Int)=>Int=add
ob: (Int, Int) => Int = <function2>

scala> ob(1,2)
res6: Int = 3

scala> val ob2=(x:Int,y:Int)=x+y
<console>:1: error: ';' expected but '=' found.
val ob2=(x:Int,y:Int)=x+y
                     ^

scala> val ob2=(x:Int,y:Int)=>x+y
ob2: (Int, Int) => Int = <function2>

scala> ob2(1,2)
res7: Int = 3scala>


functions as object:
****************************
def max(a:Int,b:Int)={if(a>b) a else b}
var maxob: (Int,Int) => Int = max

def even(x:Int)={if(x%2==0)
println(s"$x is even")
else 
println(s"$x is odd")
}
val evob:Int=>Unit=even
scala> val evob:Int=>Unit=even
evob: Int => Unit = <function1>

anonymous function:

val ob=(x:Int)=>{if(x%2==0)
println(s"$x is even")
else 
println(s"$x is odd")
}


->

val add=(x:Double,y:Double)=>{println(x+y)}


while(x<=10
 {
 if(x<=5) x
 } 
println(x)

Q-Anonymous & HOF (Take 2 integers and find out greaeter or less i.e 2 Int and 1 Boolean)
---------------------------------------------------------------------------------------------
scala> def max[T1,T2](a:T1,b:T1,F:(T1,T1)=>T2)=F(a,b)
max: [T1, T2](a: T1, b: T1, F: (T1, T1) => T2)T2

scala> max[Int,Boolean](10,20,(x,y)=>x>y)
res55: Boolean = false

------------ All are Int----------------

scala> def max[T1](a:T1,b:T1,F:(T1,T1)=>T1)=F(a,b)
max: [T1](a: T1, b: T1, F: (T1, T1) => T1)T1

scala> max[Int](10,20,(x,y)=>x max y)
res58: Int = 20


Class: 10-Jul-19
***************************************
higher order function::
------------------------
(Verbose way)
scala> def arithmeticops(a:Int,b:Int,f:(Int,Int)=>Int)=f(a,b)
arithmeticops: (a: Int, b: Int, f: (Int, Int) => Int)Int

scala> def add(x:Int,y:Int)=x+y
add: (x: Int, y: Int)Int

scala> arithmeticops(10,20,add)
res0: Int = 30

using anonymous function:
-----------------------
def arithemticops(x:Int,y:Int,f:(Int,Int)=>Int)=f(x,y)

arithmeticops(10,20,(x:Int,y:Int)=> x+y)

or-----------simpler way

arithmeticops(10,20,(x,y)=> x+y)


arithmeticops(30,40,(x,y)=>x*y)

arithmeticops(30,40,(x,y)=> {
println(x,y)
x*y})

arithmetic calc using higher order function: do it
********************************************************** 
def arithemticops(x:Int,y:Int,f:(Int,Int)=>Int)=f(x,y)
arithmeticops(10,20,(x,y)=> {
val add=x+y
val sub=x-y
val mul=x*y
val div=x/y
add
sub
mul
div})

Higher order with Typed parameter (2 type parameters)
_____________________________________
def arithmeticops[T1,T2](a:T1,b:T1,f:(T1,T1)=>T2)=f(a,b)

arithmeticops[Int,Int](10,20,_+_)

scala> def arithmeticops[T1,T2](a:T1,b:T1,f:(T1,T1)=>T2)=f(a,b)
arithmeticops: [T1, T2](a: T1, b: T1, f: (T1, T1) => T2)T2

scala> arithmeticops[Int,Int](10,20,_+_)
res9: Int = 30

scala> arithmeticops[Int,Boolean](10,20,_>_)
res10: Boolean = false

scala> arithmeticops[Int,Boolean](10,20,_<_)
res11: Boolean = true

scala> arithmeticops[Double,Double](10,20,_+_)
res12: Double = 30.0

scala> arithmeticops[Double,Double](10.0,20,_+_)
res13: Double = 30.0

Higher order with Typed parameter (3 type parameters)
-------------------------------

def arithmeticops[T1,T2,T3](a:T1,b:T2,f:(T1,T2)=>T3)=f(a,b)

arithmeticops[Double,Int,Boolean](30.0,20,_>_)

scala> def arithmeticops[T1,T2,T3](a:T1,b:T2,f:(T1,T2)=>T3)=f(a,b)
arithmeticops: [T1, T2, T3](a: T1, b: T2, f: (T1, T2) => T3)T3

scala> arithmeticops[Double,Int,Boolean](30.0,20,_>_)
res14: Boolean = true

scala> arithmeticops[Double,Boolean](30.0,20,_>_)
<console>:13: error: wrong number of type parameters for method arithmeticops: [
T1, T2, T3](a: T1, b: T2, f: (T1, T2) => T3)T3
       arithmeticops[Double,Boolean](30.0,20,_>_)
                    ^

scala> arithmeticops[Double,Boolean](30.0,20,_+_)
<console>:13: error: wrong number of type parameters for method arithmeticops: [
T1, T2, T3](a: T1, b: T2, f: (T1, T2) => T3)T3
       arithmeticops[Double,Boolean](30.0,20,_+_)
                    ^

scala> arithmeticops[Double,Int,Boolean](30.0,20,_+_)
<console>:13: error: type mismatch;
 found   : Double
 required: Boolean
       arithmeticops[Double,Int,Boolean](30.0,20,_+_)
                                                  ^

scala> arithmeticops[Double,Int,Int](30.0,20,_+_)
<console>:13: error: type mismatch;
 found   : Double
 required: Int
       arithmeticops[Double,Int,Int](30.0,20,_+_)
                                              ^

scala> arithmeticops[Double,Int,Double](30.0,20,_+_)
res19: Double = 50.0

scala> arithmeticops[Double,Int,Boolean](30.0,20,_<_)
res20: Boolean = false

scala> arithmeticops[Int,Double,Double](10,20,(x,y)=> x+y)
res58: Double = 30.0


to find out count of even nos. using higher order count function(inbuilt)
-----------------------------------
scala> val arr=Array(1,2,3,4,5,6,7,8)
arr: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8)

scala> arr.count(ele=>ele%2==0)
res21: Int = 4

scala> arr.count(ele=>{
     | println(ele)
     | ele%2==0})
1
2
3
4
5
6
7
8
res23: Int = 4

home
--------------------------

scala> for(a<- arr){
     | if (a%2==0)
     |
     | count=count+1
     | println(count)
     | }
0
1
1
2
2
3
3
4
4


var count=0;
//val out= 
for(b<- arr){
if (b%2==0){
//println(b)
count = count+1
count
}}
//println(count)

Display count of nos between 1 to 100 not divisible by 2 and 3 and 5:
-----------------------------------
scala> var countr=0
countr: Int = 0

scala> for(a<-1 to 10){if(a%2!=0 && a%3!=0 && a%5!=0)
     | countr=countr+1 ; println(countr)
     | }
1
1
1
1
1
1
2
2
2
2

scala> var countr=0
countr: Int = 0

scala> for(a<-1 to 100){
      if(a%2!=0 && a%3!=0 && a%5!=0)
      countr=countr+1
	  countr
      }

scala> countr
res16: Int = 26

var countr=0 ;var t=0
for(a<-1 to 10){
     if(a%2!=0 && a%3!=0 && a%5!=0)
      countr=countr+1
	  countr
      }
Q Count the total number which is not divisible by 2,3,& 5 between 1 to 100
-----------------------------------------------------------------------------------------
	  var c=0 
for(a<-1 to 100
     if(a%2!=0 && a%3!=0 &&a%5!=0)) 
	 c=c+1
	 c
      
	  
      for(i<- 1 to 10){
	  if(i%2==0)
	  println(i)}
	  
	  var counter=0
	  for(i<- 1 to 10
	  if(i%2==0))
	  //println(i)
	  counter=counter+1
	  counter
	  
Prime no. series between 1 to 100
---------------
	  
for(i<-1 to 100){
var count=0
for(j<-i to 1 by -1){
if(i%j==0)
count=count+1
}
if(count==2)
println(i)}

5th prime no. (not working)
------------------
var p=0
for(i<-1 to 10){
var count=0
for(j<-i to 1 by -1){
if(i%j==0)
count=count+1
}
if(count==2) {
//println(i)
p=p+1 
} 
println(i) }
 
//if(p==5)
//println(p)}



Print array :

val arr = Array(1,2,3,4,5,6,7)

def add(x:Int*)= {var sum=0 
for(a <- x){  
sum = sum +a
//sum
}}



scala> def add(x:Int*)= {var sum=0
     | for(a <- x)
     | sum = sum +a
     | sum
     | }
add: (x: Int*)Int

scala> add(3,4)
res2: Int = 7

scala> def add(x:Int*)= {var sum=0
      for(a <- x){
      sum = sum +a
      }
	 sum }
add: (x: Int*)Unit

scala> add(3,4)

for(i<-8 to 1 by -2 ; j<-i to 1 by -1)
print("*")
println(" ")

for(i<-8 to 1 by -2){
for (j<-i to 1 by -1)
{
print(" * ")
}
println(" ")}

for(i<- 1 to 5 by 1 ) {
{for(j<-1 to 8)
print(" * ")
}}

Print::
*****************************

scala> for(i<-8 to 1 by -2){
     | for (j<-i to 1 by -1)
     | {
     | print(" * ")
     | }
     | println(" ")}
 *  *  *  *  *  *  *  *
 *  *  *  *  *  *
 *  *  *  *
 *  *
 
 Claa=11-jul
 ************************
 
 scala> val l=List(1,2,3,4)
l: List[Int] = List(1, 2, 3, 4)

scala> l(0)
res0: Int = 1

scala> l(1)
res1: Int = 2

scala> l(3)
res2: Int = 4

scala> l(4)
java.lang.IndexOutOfBoundsException: 4
  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65
)
  at scala.collection.immutable.List.apply(List.scala:84)
  ... 33 elided

  scala> val ls=List("scala","spark","hadoop")
ls: List[String] = List(scala, spark, hadoop)

                     ^

scala> val ls=List("scala",1,20.5)
ls: List[Any] = List(scala, 1, 20.5)

scala> l
res7: List[Int] = List(1, 2, 3, 4)

scala> ls
res8: List[String] = List(scala, spark, hadoop)

scala> l.head
res9: Int = 1

scala> l.last
res10: Int = 4

scala> l.tail
res11: List[Int] = List(2, 3, 4)

scala> l.init
res12: List[Int] = List(1, 2, 3)

scala> val l = List(1,2,3,4,5,6,7,8,9,10)
l: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l
res13: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l.take(3)
res14: List[Int] = List(1, 2, 3)

scala>

scala> l.takeRight(5)
res15: List[Int] = List(6, 7, 8, 9, 10)

scala> l.contains(3)
res16: Boolean = true

scala> l.contains(5)
res17: Boolean = true

scala> l.contains(3)
res16: Boolean = true

scala> l.contains(5)
res17: Boolean = true

scala> l
res18: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l.take(6)
res19: List[Int] = List(1, 2, 3, 4, 5, 6)

scala> l.take(6).takeRight(3)
res20: List[Int] = List(4, 5, 6)

scala> l.reverse
res21: List[Int] = List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1)

scala> l
res22: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l.indexOf(6)
res23: Int = 5

scala> l.min
res24: Int = 1


scala> l.size
res26: Int = 10

scala> l.max
res27: Int = 10

scala> l
res64: List[Int] = List(1, 24, 3, 5, 65)

scala> l.drop(2)
res65: List[Int] = List(3, 5, 65)

scala> l.sum
res29: Int = 55


scala> l.seq
res31: scala.collection.immutable.LinearSeq[Int] = List(1, 2, 3, 4, 5, 6, 7, 8,
9, 10)

scala> l.foreach
   def foreach[U](f: A => U): Unit
contains and exists works in similar but exists is a HOF where contains is normal function
----------------------------------
scala> l
res66: List[Int] = List(1, 24, 3, 5, 65)

scala> l.contains(24)
res67: Boolean = true

scala> l.exists(ele=>ele==5)
res68: Boolean = true


Collect function in List------------
--------------------------------

scala> l1
res59: List[Any] = List(Dip, 1, AD, 2, asdd, 4)

scala> l1.collect {case name:String=>name}
res60: List[String] = List(Dip, AD, asdd)

scala> l1.collect {case id:Int=>id}
res61: List[Int] = List(1, 2, 4)

Find function in List
-----------------------

scala> l1
res69: List[Any] = List(Dip, 1, AD, 2, asdd, 4)


Intersect and union
--------------------------

scala> l1
res73: List[Any] = List(Dip, 1, AD, 2, asdd, 4)

scala> l
res74: List[Int] = List(1, 24, 3, 5, 65)

scala> l1 intersect l
res75: List[Any] = List(1)

scala> val l2=List(1,2,3,4)
l2: List[Int] = List(1, 2, 3, 4)

scala> l intersect l1
res76: List[Int] = List(1)

scala> l intersect l2
res77: List[Int] = List(1, 3)

scala> l union l1
res78: List[Any] = List(1, 24, 3, 5, 65, Dip, 1, AD, 2, asdd, 4)

scala> l1.find(_=="Dip")
res71: Option[Any] = Some(Dip)

scala> l1.find(_=="Dip").get
res72: Any = Dip


Q to print all nos in list
------------------------------------
scala> for(ele<-l) println(ele)
1
2
3
4
5
6
7
8
9
10

Q- to give output back (multiply all nos with 2)
-------------------------------------------------------------
scala> for(ele<-l) yield ele*2
res33: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)

scala> l
res34: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> res33
res35: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)


String and it's length:
---------------------------------
scala> ls
res38: List[String] = List(scala, spark, hadoop)

scala> val ls1=for(s<-ls) yield s.length
ls1: List[Int] = List(5, 5, 6)

scala> val ls1=for(s<-ls) yield (s,s.length)
ls1: List[(String, Int)] = List((scala,5), (spark,5), (hadoop,6))


Higher order with collection:
****************************************8

scala> l.foreach
   def foreach[U](f: A => U): Unit

scala> l.foreach
   def foreach[U](f: A => U): Unit

scala> l.foreach
<console>:12: error: missing arguments for method foreach in class List;
follow this method with `_' if you want to treat it as a partially applied funct
ion
       l.foreach
         ^

scala> l
res40: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l.foreach
<console>:12: error: missing arguments for method foreach in class List;
follow this method with `_' if you want to treat it as a partially applied funct
ion
       l.foreach
         ^

scala> l.foreach
   def foreach[U](f: A => U): Unit

scala> l.foreach
   def foreach[U](f: A => U): Unit

scala> l.foreach(x=>x+1)

scala> l.foreach(x=>println(x+1))
2
3
4
5
6
7
8
9
10
11

scala> l.foreach(x=>println(x*10)
     | )
10
20
30
40
50
60
70
80
90
100

scala> l
res45: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> l.foreach(x=> {
     | val y= x
     | println(x*10)}
     | )
10
20
30
40
50
60
70
80
90
100

Q: MAP function gives output after multiploication by 10
----------------------------------------------------------------

scala> val lmul10=l.map(x=>x*10)
lmul10: List[Int] = List(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)


Q: MAP function gives output after reversing the string
----------------------------------------------------------------

scala> ls
res48: List[String] = List(scala, spark, hadoop)

scala> val lsR= ls.map(s=> s.reverse)
lsR: List[String] = List(alacs, kraps, poodah)

Q: Take the below list and derive the no, as even or odd
-----------------------------------------------------------------------------

scala> l
res49: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala>  val numl= l.map(x=>
     |  (x,if(x%2==0) "even"  else "odd"))
numl: List[(Int, String)] = List((1,odd), (2,even), (3,odd), (4,even), (5,odd),
(6,even), (7,odd), (8,even), (9,odd), (10,even))


class- 12-jul-19
**********************************
scala> val l= List.range(1,11)
l: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> val out=l.map(ele=>if (ele%2==0)  "even" else "odd")
out: List[String] = List(odd, even, odd, even, odd, even, odd, even, odd, even)

scala> val out=l.map(ele=>if (ele%2==0)  ele)
out: List[AnyVal] = List((), 2, (), 4, (), 6, (), 8, (), 10)

scala> val out=l.map(ele=>if (ele%2==0)  ele else "odd")
out: List[Any] = List(odd, 2, odd, 4, odd, 6, odd, 8, odd, 10)

scala> l.filter
filter   filterNot

scala> l.filter
   def filter(p: A => Boolean): Repr

scala> val leven=l.filter(ele=> ele%2==0 )
leven: List[Int] = List(2, 4, 6, 8, 10)

Find out the nos which are  odd
--------------------------------------
scala> val leven=l.filter(ele=> ele%2!=0 )
leven: List[Int] = List(1, 3, 5, 7, 9)

scala> val leven=l.filterNot(ele=> ele%2==0 )
leven: List[Int] = List(1, 3, 5, 7, 9)

scala> val ls=List("scala","spark","hadoop","bigdata")
ls: List[String] = List(scala, spark, hadoop, bigdata)

Filter out strings which contains data
------------------------------------------------
scala> val outd=ls.filter(ele=> ele.contains("d"))
outd: List[String] = List(hadoop, bigdata)

Filter out strings which  does not contains data
------------------------------------------------
scala> val outd=ls.filterNot(ele=> ele.contains("d"))
outd: List[String] = List(scala, spark)

Find the list of values which is greater than 5
---------------------------------------------------------
scala> val out= l.filter(ele=> ele>5)
out: List[Int] = List(6, 7, 8, 9, 10)


Reduce HOF:
-----------------------

scala> val out= l.reduce((acc,iter)=>acc max iter)
out: Int = 10

scala> l
res3: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> val fact= l.reduce((acc,iter)=> acc * iter)
fact: Int = 3628800

Q:  Write a program to find factorial of a number in functional way:
----------------------------------------------------------------------------------

scala> def factorial(n:Int)={
     | val l=List.range(1,n+1)
     | l.reduce((acc,iter)=>acc*iter)
     | }
factorial: (n: Int)Int

scala> factorial(3)
res4: Int = 6

scala> factorial(4)
res5: Int = 24

above question 2nd approach
--------------------------------------------------
scala> def factorial(n:Int)=List.range(1,n+1).reduce((a,b)=>a*b)
factorial: (n: Int)Int



 above question 3rd approach (Best approach)
 ----------------------------------
scala> def factorial(n:Int)=List.range(1,n+1).reduce(_*_)
factorial: (n: Int)Int

scala> factorial(3)
res6: Int = 6

Concatenate below strings in collection:(using HOF)
----------------------------------------------
scala> ls
res8: List[String] = List(scala, spark, hadoop, bigdata)

scala> val out= ls.reduce((accu,iter)=> accu +"||"+ iter)
out: String = scala||spark||hadoop||bigdata

Concatenate below strings in collection: (using built in method)
----------------------------------------------

scala> ls.mkString
res9: String = scalasparkhadoopbigdata

scala> ls.mkString("|")
res10: String = scala|spark|hadoop|bigdata

HOF Partition (like split by in pig)
-----------------------------------------

scala> val lemp= List((1,"emp1",10000.0),(2,"emp2",5000),(3,"emp3",4000))
lemp: List[(Int, String, AnyVal)] = List((1,emp1,10000.0), (2,emp2,5000), (3,emp3,4000))



scala> lemp
res11: List[(Int, String, AnyVal)] = List((1,emp1,10000.0), (2,emp2,5000), (3,emp3,4000))

scala> l.partition(e=>e%2==0)
res12: (List[Int], List[Int]) = (List(2, 4, 6, 8, 10),List(1, 3, 5, 7, 9))

scala> val (even,odd)=l.partition(e=>e%2==0)
even: List[Int] = List(2, 4, 6, 8, 10)
odd: List[Int] = List(1, 3, 5, 7, 9)

scala> even
res13: List[Int] = List(2, 4, 6, 8, 10)

scala> odd
res14: List[Int] = List(1, 3, 5, 7, 9)


Distinct method:
-------------------------------	   
scala> val ldup = List(1,2,3,4,5,4,3,2)
ldup: List[Int] = List(1, 2, 3, 4, 5, 4, 3, 2)

scala> ldup.distinct

Union method: (contains duplicates  as union all)
-------------------

scala> val lnum= even union odd
lnum: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)

scala>

scala> val lnum= even.union(odd)
lnum: List[Int] = List(2, 4, 6, 8, 10, 1, 3, 5, 7, 9)

res15: List[Int] = List(1, 2, 3, 4, 5)

Count HOF
-----------------------------
	   
	scala> lemp
res19: List[(Int, String, AnyVal)] = List((1,emp1,10000.0), (2,emp2,5000), (3,emp3,4000))

scala> val out= lemp.count(t=> t._3 >4000)
<console>:11: error: value > is not a member of AnyVal
       val out= lemp.count(t=> t._3 >4000)
                                    ^

scala> val lemp= List((1,"emp1",10000),(2,"emp2",3000),(3,"emp3",40000))
lemp: List[(Int, String, Int)] = List((1,emp1,10000), (2,emp2,3000), (3,emp3,40000))

scala> val out= lemp.count(t=> t._3 >2000)
out: Int = 3

scala> val out= lemp.count(t=> t._3 >20000)
out: Int = 1   
Occurence of element (like group by)

find out min and max of array   of 1 to 10
-------------------------------------------------------------
 scala> def minmax(a:Array[Int])= (a.min,a.max)
minmax: (a: Array[Int])(Int, Int)

scala> minmax(Array.range(1,10))
res9: (Int, Int) = (1,9)

find out min and max of List of 1 to 1000
-------------------------------------------------------------
scala> def Minmax(a:List[Int])= (a.min,a.max)
Minmax: (a: List[Int])(Int, Int)

scala> Minmax(List.range(1,1000))
res11: (Int, Int) = (1,999)

scala> val arr= Array(1,"abc",2)
arr: Array[Any] = Array(1, abc, 2)

scala> val arr= Array((1,"abc",2),(2,"dfe",3))
arr: Array[(Int, String, Int)] = Array((1,abc,2), (2,dfe,3))

scala> arr._1
<console>:12: error: value _1 is not a member of Array[(Int, String, Int)]
       arr._1
           ^

scala> arr(0)
res13: (Int, String, Int) = (1,abc,2)

scala> arr(0)._1
res14: Int = 1

scala> arr(0)._2
res15: String = abc

Max of 4 nos using nested function:
---------------------------------------------

def max(x:Int,y:Int,z:Int,w:Int)={
def max(a:Int,b:Int)={
if(a>b) a else b}
max(x,max(w,max(y,z)))}

scala> max(3,6,8,3)
res2: Int = 8

scala> max(3,3,3,3)
res3: Int = 3

Typed parametr Function
--------------------------------------

scala> def add[T](a:T,b:T)= {a match{
     |  case x:Int => x* b.asInstanceOf[Int]
     |  case x:Float => x* b.asInstanceOf[Float]
     |  case x:Double => x* b.asInstanceOf[Double]
     |  case _ => "sorry "
     |  }}
add: [T](a: T, b: T)Any

scala> add(2.0,3.0)
res25: Any = 6.0

Generic function for Double
********************************

scala> def double[T](x:T)={x match{
     | case a:Int=> a*2
     | case a:Float=>a*2
     | case a:Double=>a*2
     | case a:Short=>a*2
     | case b=>s"unknown $b"}}
double: [T](x: T)Any

scala> double(30.8)
res49: Any = 61.6

HOF:
-----------
scala> def hof(s:String,f:String=>Int)=f(s)
hof: (s: String, f: String => Int)Int
*****
scala> hof("dsffg",x=> {
     | println(s"length is " + x.length)
     | println("index of d is "  +  x.indexOf("d"))
     | x.lastIndexOf("f")
     |  })
length is 5
index of d is 0
res61: Int = 3



Assignment:=========

	Write a program that has 2 functions, namely sum and avg
a)	sum takes a list and sums of all the integers in the list
b)	avg calls the sum to calculate average of the list passed to it as an argument

Invoke sum function to print the sum
Invoke avg function to print the average.

Modify the avg function to have an anonymous function passed to it as an argument.
----------------------------------------------------------------------------------------------------

Solution:  using HOF
-----------------------------------
scala> def sum(x:List[Double])= x.reduce((accu,iter)=>accu+iter)
sum: (x: List[Double])Double

scala> def avg(x:List[Double],f:List[Double]=>Double)=f(x)/x.length
avg: (x: List[Double], f: List[Double] => Double)Double

Calling fun:
scala> avg(List(1,2,3,4,5,6),sum)
res11: Double = 3.5

scala> sum(List(1,2,3,4,5))
res12: Double = 15.0

Calling avg function using anonymous function
************************************************

scala> avg(List(10,20,30,40,50,-60),(x=> x.reduce((accu,iter)=>accu+iter)))
res14: Double = 15.0

solution:2
---------------------------
scala> def sum(x:List[Double])= x.reduce((accu,iter)=>accu+iter)
sum: (x: List[Double])Double

scala> def avg(x:List[Double])={
     | val average=sum(x:List[Double])/x.length
     | average}
avg: (x: List[Double])Double


10)	A function named sum accepts a list and calls a function (say anonymous) that returns it the highest number.
    Let the sum function do the following:
    a) print the sum and
    b) return the highest number.
-----------------------------------
scala> def max(x:List[Double])= x.reduce((acc,itr)=> acc max itr)
max: (x: List[Double])Double

scala> max(List(1,2,4,-8))
res15: Int = 4


scala> def sum_max(x:List[Double],f:List[Double]=>Double) = { println("Total sum is " + x.sum )
     | f(x)
     | }
sum_max: (x: List[Double], f: List[Double] => Double)Double

scala> sum_max(List(12,34,56,9,-90),max) 
Total sum is 21.0
res10: Double = 56.0

---- Using Anonymous function-----------

scala> def sum_max(x:List[Double],f:List[Double]=>Double) = { println(x.sum )
     | f(x)
     | }
sum_max: (x: List[Double], f: List[Double] => Double)Double

scala> sum_max(List(1,2,3,-5,20.5),(x=>x.reduce((accu,iter)=> accu max iter)))
     
21.5
res12: Double = 20.5

 *********More simpler form**********
scala> sum_max(List(1,2,3,-5,20.5),(_.reduce((accu,iter)=> accu max iter)))
21.5
res14: Double = 20.5
 
  
  Class- 17-Jul-2019
  --------------------------------
  
  scala> val l=List(1,1,2,3,4,4,5,5,6,6,7,7)
l: List[Int] = List(1, 1, 2, 3, 4, 4, 5, 5, 6, 6, 7, 7)


scala> l.groupBy
   def groupBy[K](f: A => K): immutable.Map[K,Repr]

   groupBy------------------
   
scala> val grpData=l.groupBy(e=>e)
grpData: scala.collection.immutable.Map[Int,List[Int]] = Map(5 -> List(5, 5), 1 -> List(1, 1), 6 ->
List(6, 6), 2 -> List(2), 7 -> List(7, 7), 3 -> List(3), 4 -> List(4, 4))


scala> grpData.foreach(println)
(5,List(5, 5))
(1,List(1, 1))
(6,List(6, 6))
(2,List(2))
(7,List(7, 7))
(3,List(3))
(4,List(4, 4))

scala> val grpcnt=grpData.map(t=>(t._1,t._2))
grpcnt: scala.collection.immutable.Map[Int,List[Int]] = Map(5 -> List(5, 5), 1 -> List(1, 1), 6 -> L
ist(6, 6), 2 -> List(2), 7 -> List(7, 7), 3 -> List(3), 4 -> List(4, 4))

scala> val grpcnt=grpData.map(t=>(t._1,t._2.size))
grpcnt: scala.collection.immutable.Map[Int,Int] = Map(5 -> 2, 1 -> 2, 6 -> 2, 2 -> 1, 7 -> 2, 3 -> 1
, 4 -> 2)

scala> grpcnt.foreach(println)
(5,2)
(1,2)
(6,2)
(2,1)
(7,2)
(3,1)
(4,2)


Q- Distribution on age basis:
*********************************************
scala> val l1=List((1,"hello",54),(2,"hai",23),(3,"scala",54),(4,"spark",5),(5,"hadoop",23))
l1: List[(Int, String, Int)] = List((1,hello,54), (2,hai,23), (3,scala,54), (4,spark,5), (5,hadoop,2
3))

scala> val grpage=l1.groupBy(ele=>ele._3)
grpage: scala.collection.immutable.Map[Int,List[(Int, String, Int)]] = Map(23 -> List((2,hai,23), (5
,hadoop,23)), 5 -> List((4,spark,5)), 54 -> List((1,hello,54), (3,scala,54)))

scala> grpage.foreach(println)
(23,List((2,hai,23), (5,hadoop,23)))
(5,List((4,spark,5)))
(54,List((1,hello,54), (3,scala,54)))

scala> val grpagecnt=grpage.map(t=>(t._1,t._2.size))
grpagecnt: scala.collection.immutable.Map[Int,Int] = Map(23 -> 2, 5 -> 1, 54 -> 2)

scala> grpagecnt.foreach(println)
(23,2)
(5,1)
(54,2)


All below syntax are same: (verbose way & shorthand notaion)
************************************************

scala> grpagecnt.foreach(println)
(23,2)
(5,1)
(54,2)

scala> grpagecnt.foreach(e=>println(e))
     
(23,2)
(5,1)
(54,2)

scala> grpagecnt.foreach(println(_))   // using underscore shorthand notaion(possible only when 1 argument is there)
(23,2)
(5,1)
(54,2)

scala> grpagecnt.foreach(println) //shorthand notaion
(23,2)
(5,1)
(54,2)


Creating list another way----- elements using double colon ::
****************************************************************

scala> val l=1::2::3
<console>:10: error: value :: is not a member of Int
       val l=1::2::3
  In above case :: method it's calling , so throwing error

  scala> val l=1::2::3::Nil
l: List[Int] = List(1, 2, 3)

scala> val l=1::2::3::List()
l: List[Int] = List(1, 2, 3)

scala> val l=1::2::3::List(1,2)
l: List[Int] = List(1, 2, 3, 1, 2)

scala> val l=1::2::3::List(1,2),List(3,4)
<console>:1: error: ';' expected but ',' found.
val l=1::2::3::List(1,2),List(3,4)
                        ^

scala> val l=1::2::3::List(1,2)::List(3,4)
l: List[Any] = List(1, 2, 3, List(1, 2), 3, 4)

scala> val l=1::2::3::List(1,2)::Nil
l: List[Any] = List(1, 2, 3, List(1, 2))


scala> val l="AB"::"as"::"SAD"::Nil
l: List[String] = List(AB, as, SAD)


Set  (no order, uses hashing, no duplicates,if duplicates then latest one will be appended)
************

scala> val s= Set(10,20,34,56,87,65)
s: scala.collection.immutable.Set[Int] = Set(10, 56, 20, 65, 34, 87)

scala> s(1)
res13: Boolean = false

scala> s.contains(1)
res14: Boolean = false

scala> s.contains(10)
res15: Boolean = true


Q: add 10 to all elements in set:
----------------------------------------
scala> val s= Set(10,20,34,56,87,65,10)
s: scala.collection.immutable.Set[Int] = Set(10, 56, 20, 65, 34, 87)

scala> val add=s.map(ele=>ele+10)
add: scala.collection.immutable.Set[Int] = Set(20, 97, 44, 66, 75, 30)

scala> add.foreach(println)
20
97
44
66
75
30

Q: Sum of all elements in the set:
-----------------------------------------
scala> s
res22: scala.collection.immutable.Set[Int] = Set(10, 56, 20, 65, 34, 87)

scala> val sum=s.reduce((e1,e2)=>e1+e2)
sum: Int = 272

scala> val sum=s.reduce(_+_)
sum: Int = 272


Functions in set
****************************
scala> s
res24: scala.collection.immutable.Set[Int] = Set(10, 56, 20, 65, 34, 87)

scala> s.toList
res25: List[Int] = List(10, 56, 20, 65, 34, 87)

scala> set
res28: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)


scala> set.exists(value=> value==2)
res30: Boolean = true
scala> val l=List(1,2,3,4)
l: List[Int] = List(1, 2, 3, 4)

scala> l.toSet
res26: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)

scala> s.init
res27: scala.collection.immutable.Set[Int] = Set(10, 56, 20, 65, 34)

scala> s.tail
res28: scala.collection.immutable.Set[Int] = Set(56, 20, 65, 34, 87)

scala> s.contains(65)
res29: Boolean = true

scala> s.last
res30: Int = 87

scala> s.head
res31: Int = 10

scala> set
res31: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)

scala> set2
res32: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)

scala> set ++ set2
res33: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)

scala> set --set2
res34: scala.collection.immutable.Set[Int] = Set()

scala> set2 -- set
res35: scala.collection.immutable.Set[Int] = Set(5)


scala> set ++ List(1,2,123)
res37: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 123, 4)

scala> set union set2
res38: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4)

scala> set intersect set2
res39: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)

scala> set -- Set(1,10)
res44: scala.collection.immutable.Set[Int] = Set(2, 3, 4)

scala> set -3
res45: scala.collection.immutable.Set[Int] = Set(1, 2, 4)

scala> set
res46: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)

scala> set -10
res47: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)

scala> set -1
res48: scala.collection.immutable.Set[Int] = Set(2, 3, 4)


Assisgnment :Get how many Teen, adult and senior citizen are available based upon age group:
-------------------------------------------------------------------------------------------------------------------------

scala> val l=List((1,"Dip",35),(2,"Satya",40),(3,"Ram",12),(4,"Hari",65),(5,"Deb",13))
l: List[(Int, String, Int)] = List((1,Dip,35), (2,Satya,40), (3,Ram,12), (4,Hari,65), (5,Deb,13))

scala> l
res0: List[(Int, String, Int)] = List((1,Dip,35), (2,Satya,40), (3,Ram,12), (4,Hari,65), (5,Deb,13))


scala> val grp=l.groupBy(ele=>ele._3)
grp: scala.collection.immutable.Map[Int,List[(Int, String, Int)]] = Map(65 -> List((4,Hari,65)), 13
-> List((5,Deb,13)), 12 -> List((3,Ram,12)), 35 -> List((1,Dip,35)), 40 -> List((2,Satya,40)))

scala> grp.foreach(println)
(65,List((4,Hari,65)))
(13,List((5,Deb,13)))
(12,List((3,Ram,12)))
(35,List((1,Dip,35)))
(40,List((2,Satya,40)))


scala> val addgrp=grp.map(ele=> (ele,if(ele._1>60) "senior citizen"
      else if(ele._1>=20 && ele._1<=60) "adult"
      else "Teen"))
newgrp: scala.collection.immutable.Map[(Int, List[(Int, String, Int)]),String] = Map((40,List((2,Sat
ya,40))) -> adult, (13,List((5,Deb,13))) -> Teen, (65,List((4,Hari,65))) -> senior citizen, (12,List
((3,Ram,12))) -> Teen, (35,List((1,Dip,35))) -> adult)


scala> addgrp.foreach(println)
((40,List((2,Satya,40))),adult)
((13,List((5,Deb,13))),Teen)
((65,List((4,Hari,65))),senior citizen)
((12,List((3,Ram,12))),Teen)
((35,List((1,Dip,35))),adult)


scala> val newgrp=grp.map(ele=> (if(ele._1>60) "senior citizen"
     |       else if(ele._1>=20 && ele._1<=60) "adult"
     |       else "Teen"))
newgrp: scala.collection.immutable.Iterable[String] = List(senior citizen, Teen, Teen, adult, adult)


scala> newgrp
res23: scala.collection.immutable.Iterable[String] = List(senior citizen, Teen, Teen, adult, adult)

scala> val group=newgrp.groupBy(e=>e)
group: scala.collection.immutable.Map[String,scala.collection.immutable.Iterable[String]] = Map(Teen
 -> List(Teen, Teen), adult -> List(adult, adult), senior citizen -> List(senior citizen))



scala> group.foreach(println)
(Teen,List(Teen, Teen))
(adult,List(adult, adult))
(senior citizen,List(senior citizen))

scala> val total_count= group.map(ele=> (ele._1,ele._2.size))
total_count: scala.collection.immutable.Map[String,Int] = Map(Teen -> 2, adult -> 2, senior citizen
-> 1)

scala> total_count.foreach(println)
(Teen,2)
(adult,2)
(senior citizen,1)

Q- sort the emp data according name, age 
-------------------------------------------------
scala> val l=List((1,"Dip",35),(2,"Satya",40),(3,"Ram",12),(4,"Hari",65),(5,"Deb",13))
l: List[(Int, String, Int)] = List((1,Dip,35), (2,Satya,40), (3,Ram,12), (4,Hari,65), (5,Deb,13))

scala> l
res26: List[(Int, String, Int)] = List((1,Dip,35), (2,Satya,40), (3,Ram,12), (4,Hari,65), (5,Deb,13)
)
Sort according to age:
------------------------------
scala> l.sortBy(ele=> ele._3)
res27: List[(Int, String, Int)] = List((3,Ram,12), (5,Deb,13), (1,Dip,35), (2,Satya,40), (4,Hari,65)
)

sort according to name:
---------------------------------

scala> l.sortBy(ele=> ele._2)
res28: List[(Int, String, Int)] = List((5,Deb,13), (1,Dip,35), (4,Hari,65), (3,Ram,12), (2,Satya,40)
)

sort according to length of the name:
--------------------------------------------
scala> l.sortBy(ele=> ele._2.length)
res30: List[(Int, String, Int)] = List((1,Dip,35), (3,Ram,12), (5,Deb,13), (4,Hari,65), (2,Satya,40)
)

sort according to length of the name descending
--------------------------------------------------------
scala> l.sortBy(ele=> -ele._2.length)
res31: List[(Int, String, Int)] = List((2,Satya,40), (4,Hari,65), (1,Dip,35), (3,Ram,12), (5,Deb,13)
)


inbuilt function examples:
************************************
scala> "heel"drop(2)
res36: String = el

scala> "heel"drop(2).take(1)
<console>:11: error: value take is not a member of Int
       "heel"drop(2).take(1)
                     ^

scala> "heel"drop(2).take(1).capitalize
<console>:11: error: value take is not a member of Int
       "heel"drop(2).take(1).capitalize
                     ^

scala> "heel".take(2)
res39: String = he

scala> "heel".take(2).capitalize
res40: String = He

scala> "heel".take(2).toUpperCase
res41: String = HE

scala> "heel".capitalize
res42: String = Heel

scala> "heel".toUpperCase
res43: String = HEEL

scala> "heel".take(3)
res44: String = hee

scala> "heel".take(3) .tail
res45: String = ee

scala> "heel".take(3) .tail.UpperCase
<console>:11: error: value UpperCase is not a member of String
       "heel".take(3) .tail.UpperCase
                            ^

scala> "heel".take(3) .tail.toUpperCase
res47: String = EE

Comparing 2 strings---------------------

scala> val s1="hello"
s1: String = hello

scala> val s2="Hello"
s2: String = Hello

scala> s1==s2
res48: Boolean = false

scala> s1.Capitalize==s2
<console>:13: error: value Capitalize is not a member of String
       s1.Capitalize==s2
          ^

scala> s1.capitalize==s2
res50: Boolean = true

Comparison using null string:-------------------------------------

cala> val s1=null
1: Null = null

cala> val s2=null
2: Null = null

cala> s1==s2
console>:13: warning: comparing values of types Null and Null using `==' will always yield true
      s1==s2
        ^
es51: Boolean = true

cala> s1.toUpperCase==s2.toUpperCase
console>:13: error: value toUpperCase is not a member of Null
      s1.toUpperCase==s2.toUpperCase
         ^
console>:13: error: value toUpperCase is not a member of Null
      s1.toUpperCase==s2.toUpperCase
                         ^
Ignore Case---------------

scala> "hello".equalsIgnoreCase("HEllo")
res53: Boolean = true

Multiline-------------

scala> val str="""This is dvs
     | techonologies. Hadoop coaching center
     | spark scala"""
str: String =
This is dvs
techonologies. Hadoop coaching center
spark scala

use split-----------------

scala> val s="egg,milk,curd,rice"
s: String = egg,milk,curd,rice

scala> s.split(",")
res54: Array[String] = Array(egg, milk, curd, rice)

scala> s.split(",").foreach(println)
egg
milk
curd
rice

Conver to upper case using HOF
---------------------------------------

scala> "hello,world".toUpperCase
res56: String = HELLO,WORLD

scala> "hello,world".map(x=>x.toUpperCase)
<console>:11: error: value toUpperCase is not a member of Char
       "hello,world".map(x=>x.toUpperCase)
                              ^

scala> "hello,world".map(x => x.toUpperCase)
<console>:11: error: value toUpperCase is not a member of Char
       "hello,world".map(x => x.toUpperCase)
                                ^

scala> "hello,world".map(_.toUpperCase)
<console>:11: error: value toUpperCase is not a member of Char
       "hello,world".map(_.toUpperCase)
                           ^

scala> "hello,world".map(_.toUpper)
res60: String = HELLO,WORLD

scala> "hello,world".map(x=>x.toUpper)
res61: String = HELLO,WORLD

using filter and map HOF----------------------------
************************************
scala> "hello,world".filter(x=>x!='l').map(x=>x.toUpper)
res63: String = HEO,WORD

scala> "hello,world".filter(_!='l').map(_.toUpper)
res64: String = HEO,WORD

scala> "hello,world".filter(_!='l').map(_.toUpper).foreach(println)
H
E
O
,
W
O
R
D

scala> "hello,world".filter(_!='l').map(_.toUpper).foreach(print)
HEO,WORD

own logic to increment char:
***********************************
scala> def increment(s:String)=s.map(x=>(x+1).toChar)
increment: (s: String)String

scala> "abc".increment
<console>:11: error: value increment is not a member of String
       "abc".increment
             ^

scala> increment("abs")
res77: String = bct

To get max values of byte Int etc...
-----------------------------------------

scala> Byte.MaxValue
res83: Byte = 127

scala> Byte.MinValue
res84: Byte = -128

scala> Float.MaxValue
res85: Float = 3.4028235E38

class:18-Jul-19
****************************************

Map collection:
---------------------------

scala> val x=Map(1->"scala",2->"spark",3->"hadoop",4->"bigdata")
x: scala.collection.immutable.Map[Int,String] = Map(1 -> scala, 2 -> spark, 3 -> hadoop, 4 -> bigdat
a)

to get key,value pair
--------------------------
scala> x.take(1)
res1: scala.collection.immutable.Map[Int,String] = Map(1 -> scala)

to get particular value (use only key)
-------------------------
scala> x(1)
res2: String = scala

scala> x(4)
res3: String = bigdata
to see all keys
--------------------------
scala> x.keys
res4: Iterable[Int] = Set(1, 2, 3, 4)
to see all values
---------------------------
scala> x.values
res8: Iterable[String] = MapLike(scala, spark, hadoop, bigdata)

if key doesnot exist it will give error when trying to acess to avoid this go for getorelse method
---------------------------------------------------------------------
scala> m(5)
java.util.NoSuchElementException: key not found: 5
  at scala.collection.MapLike$class.default(MapLike.scala:228)
  at scala.collection.AbstractMap.default(Map.scala:59)
  at scala.collection.MapLike$class.apply(MapLike.scala:141)
  at scala.collection.AbstractMap.apply(Map.scala:59)
  ... 33 elided

  If key exists get method gives optn as some and if key doesnot exist it gives option as none
  --------------------------------------------------------------------------------------------------------
  scala> m.get(1)
res11: Option[String] = Some(scala)

scala> m.get(6)
res12: Option[String] = None


If key exists give 1st value {if part) and if key doesnot exist goes for 2nd part(else part which is default)
---------------------------------------------------------------------------------------------
scala> m.getOrElse(1,"unknown key")
res13: String = scala

scala> m.getOrElse(1)
<console>:12: error: not enough arguments for method getOrElse: (key: Int, default: => B1)B1.
Unspecified value parameter default.
       m.getOrElse(1)
                  ^

scala> m.getOrElse(5,"unknown key")
res15: String = unknown key

If u want to get actual value using get method
-----------------------------------------------------

scala> m.get(1)
res16: Option[String] = Some(scala)

scala> m.get(1).get
res17: String = scala


using match :(only Some or None is posiible from m.get(1) as it gives option which is like Typed Pattern)
---------------------
scala> m.get(1).get
res17: String = scala

scala> m.get(1) match{
     | case Some(x)=>println(s"value is $x")
     | case None=> println("key not found")
     | }
value is scala

scala> m.get(12) match{
     | case Some(x)=>println(s"value is $x")
     | case None=> println("key not found")
     | }
key not found

use contains  method before get()
--------------------------------------
scala> m.contains(3)
res23: Boolean = true

scala> m.contains(63)
res24: Boolean = false

to print all k val
---------------------------
scala> m.foreach(println)
(1,scala)
(2,spark)
(3,hadoop)
(4,bigdata)

scala> m.foreach(kv=>println(kv))
(1,scala)
(2,spark)
(3,hadoop)
(4,bigdata)
to print only keys
--------------------------
scala> m.foreach(kv=>println(kv._1))
    
1
2
3
4

 To print only values
 ----------------------------
 scala> m.foreach(kv=>println(kv._2))
     
scala
spark
hadoop
bigdata

Q- convert values to uppercase:
---------------------------------------
scala> m.map(kv=>(kv._1,kv._2.toUpperCase))
res30: scala.collection.immutable.Map[Int,String] = Map(1 -> SCALA, 2 -> SPARK, 3 -> HADOOP, 4 -> BI
GDATA)


scala> m.map(kv=>(kv._1,kv._2.capitalize))
res32: scala.collection.immutable.Map[Int,String] = Map(1 -> Scala, 2 -> Spark, 3 -> Hadoop, 4 -> Bi
gdata)


To get only values or apply functions only on value no key(but (k,v) pair internally relation will be there)
-------------------------------------------------------------------------------------------------

scala> m.mapValues(v=>v.capitalize)
res33: scala.collection.immutable.Map[Int,String] = Map(1 -> Scala, 2 -> Spark, 3 -> Hadoop, 4 -> Bi
gdata)

scala> m.mapValues(v=>(k,v.capitalize))
<console>:12: error: not found: value k
       m.mapValues(v=>(k,v.capitalize))
                       ^
					   
Get only those strings which length is 5 (apply filter on value basis)
------------------------------------------
scala> m.filter(t=>t._2.size==5)
res35: scala.collection.immutable.Map[Int,String] = Map(1 -> scala, 2 -> spark)
		

Use of FilterKeys (no acess to values here)
----------------------------

scala> m.filterKeys(k=>k==5)
res36: scala.collection.immutable.Map[Int,String] = Map()

scala> m.filterKeys(k=>k==1)
res37: scala.collection.immutable.Map[Int,String] = Map(1 -> scala)

(same thing we can do without filterKeys)******** 
scala> m.filter(k=>k._1==1)
res38: scala.collection.immutable.Map[Int,String] = Map(1 -> scala)


Functions in Map:
****************************************
scala> m
res39: scala.collection.immutable.Map[Int,String] = Map(1 -> scala, 2 -> spark, 3 -> hadoop, 4 -> bi
gdata)

scala> m.take(2)
res40: scala.collection.immutable.Map[Int,String] = Map(1 -> scala, 2 -> spark)


scala> m.takeRight(2)
res41: scala.collection.immutable.Map[Int,String] = Map(3 -> hadoop, 4 -> bigdata)

scala> m.drop(2)
res42: scala.collection.immutable.Map[Int,String] = Map(3 -> hadoop, 4 -> bigdata)

scala> m.dropRight(2)
res43: scala.collection.immutable.Map[Int,String] = Map(1 -> scala, 2 -> spark)



Index Sequence collection:: (Vector) ( duplicate ,insertion order maintains and high performance due to index) (Most commonly used ) 
*********************************************

scala>  val v=Vector(1,3,14,42,2235)
v: scala.collection.immutable.Vector[Int] = Vector(1, 3, 14, 42, 2235)

scala> val v1=Vector.range(3,8)
v1: scala.collection.immutable.Vector[Int] = Vector(3, 4, 5, 6, 7)

scala> v(0)
res44: Int = 1

scala> v.head
res45: Int = 1

scala> v.tail
res46: scala.collection.immutable.Vector[Int] = Vector(3, 14, 42, 2235)

ALl methods and Hof is same as LinearSeq

Few functions:
-=--------------------------------
scala> v
res47: scala.collection.immutable.Vector[Int] = Vector(1, 3, 14, 42, 2235)

scala> val mul=v.map(e=>e*2)
mul: scala.collection.immutable.Vector[Int] = Vector(2, 6, 28, 84, 4470)

search opreation:
---------------------------------
scala> v.contains(2)
res48: Boolean = false

scala> v.contains(42)
res49: Boolean = true

If u r manipulating like insert, append ,update,delete then go for List but if u want to search  then go for Vector.

range:
____________________________________
scala> val r=1 to 10
r: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> val r=1 until 10
r: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9)

Internally range method uses until---
---------------------------------------

scala> val r=1 to 10
r: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> val r=1 until 10
r: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala> val a=List.range(1,10)
a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala> val a=(1 to 10).toList
a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala> val a=(1 until 10).toList
a: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala> val l= List.range(1,10 by -2)
<console>:10: error: value by is not a member of Int
       val l= List.range(1,10 by -2)
                              ^

scala> val l= List.range(1,10,2)
l: List[Int] = List(1, 3, 5, 7, 9)

scala> val l= List.range(1,10,-2)
l: List[Int] = List()

scala> val l= List.range(10,1,-2)
l: List[Int] = List(10, 8, 6, 4, 2)

scala> val l=  10 to 1 by -2
l: scala.collection.immutable.Range = Range(10, 8, 6, 4, 2)

Word count in scala:
*****************************
scala> import scala.io.Source  (mandatory in REPL shell)


scala> Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList    (getLines- Read line by line, toList- converts to List)
res56: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> Source.fromFile("C:\\Users\\Satyabrat\\Desktop\\input\\inp.txt").getLines.toList
res26: List[String] = List(This is Spark ad scala training institute, Here we do
 practicals, Here we get more knoweldge regarding scala and spark)


slou-1(best way)
-------------------------
scala> val wc=wordCountInput.flatMap(e=>e.split(" ")).groupBy(x=>x).map(t=>(t._1,t._2.size))
wc: scala.collection.immutable.Map[String,Int] = Map(is -> 4, This -> 2, Scala -> 2, Technologies ->
 1, lightening -> 1, training -> 1, computer -> 1, framework -> 1, Bigdata -> 1, cluster -> 1, with
-> 1, Spark -> 3, fast -> 1, trang -> 1, part -> 1, of -> 1)


total word count in text file:----------
scala> val wc=wordCountInput.flatMap(e=>e.split(" ")).count(t=>t!=null)
wc: Int = 23

solu-2
-----------
scala> val input=Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList
input: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> input
res57: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)


scala> input.foreach(println)
This is Scala training
This is Spark with Scala trang
Spark is part of Bigdata Technologies
Spark is lightening fast cluster computer framework

scala>  val wordlist=input.map(ele=>ele.split(" "))
wordlist: List[Array[String]] = List(Array(This, is, Scala, training), Array(This, is, Spark, with,
Scala, trang), Array(Spark, is, part, of, Bigdata, Technologies), Array(Spark, is, lightening, fast,
 cluster, computer, framework))
 
scala> wordlist.size
res61: Int = 4



scala> val newword= wordlist.flatten
newword: List[String] = List(This, is, Scala, training, This, is, Spark, with, Scala, trang, Spark,
is, part, of, Bigdata, Technologies, Spark, is, lightening, fast, cluster, computer, framework)

scala> val newgrp=newword.groupBy(ele=> ele)
newgrp: scala.collection.immutable.Map[String,List[String]] = Map(is -> List(is, is, is, is), This -
> List(This, This), Scala -> List(Scala, Scala), Technologies -> List(Technologies), lightening -> L
ist(lightening), training -> List(training), computer -> List(computer), framework -> List(framework
), Bigdata -> List(Bigdata), cluster -> List(cluster), with -> List(with), Spark -> List(Spark, Spar
k, Spark), fast -> List(fast), trang -> List(trang), part -> List(part), of -> List(of))

scala> newgrp.foreach(println)
(is,List(is, is, is, is))
(This,List(This, This))
(Scala,List(Scala, Scala))
(Technologies,List(Technologies))
(lightening,List(lightening))
(training,List(training))
(computer,List(computer))
(framework,List(framework))
(Bigdata,List(Bigdata))
(cluster,List(cluster))
(with,List(with))
(Spark,List(Spark, Spark, Spark))
(fast,List(fast))
(trang,List(trang))
(part,List(part))
(of,List(of))



scala> val count= newgrp.map(t=>(t._1,t._2.size))
count: scala.collection.immutable.Map[String,Int] = Map(is -> 4, This -> 2, Scala -> 2, Technologies
 -> 1, lightening -> 1, training -> 1, computer -> 1, framework -> 1, Bigdata -> 1, cluster -> 1, wi
th -> 1, Spark -> 3, fast -> 1, trang -> 1, part -> 1, of -> 1)

scala> count.foreach(println)
(is,4)
(This,2)
(Scala,2)
(Technologies,1)
(lightening,1)
(training,1)
(computer,1)
(framework,1)
(Bigdata,1)
(cluster,1)
(with,1)
(Spark,3)
(fast,1)
(trang,1)
(part,1)
(of,1)



Class- 19/07/2019
****************************************

FlatMap works when transformation logic is a collection and it will split the daata and flatten the data in single step. in above word count program instead of map and Flatten step go 
for flatMap.

scala> import scala.io.Source  (mandatory in REPL shell)


scala> Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList    (getLines- Read line by line, toList- converts to List)
res56: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> val input=Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList
input: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> input
res57: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)


scala> input.foreach(println)
This is Scala training
This is Spark with Scala trang
Spark is part of Bigdata Technologies
Spark is lightening fast cluster computer framework

scala> val wordlist=input.map(l=>l.toUpperCase)
wordlist: List[String] = List(THIS IS SCALA TRAINING, THIS IS SPARK WITH SCALA TRANG, SPARK IS PART
OF BIGDATA TECHNOLOGIES, SPARK IS LIGHTENING FAST CLUSTER COMPUTER FRAMEWORK)

scala> val wordlist=input.flatMap(l=>l.toUpperCase)
wordlist: List[Char] = List(T, H, I, S,  , I, S,  , S, C, A, L, A,  , T, R, A, I, N, I, N, G, T, H,
I, S,  , I, S,  , S, P, A, R, K,  , W, I, T, H,  , S, C, A, L, A,  , T, R, A, N, G, S, P, A, R, K,
, I, S,  , P, A, R, T,  , O, F,  , B, I, G, D, A, T, A,  , T, E, C, H, N, O, L, O, G, I, E, S, S, P,
 A, R, K,  , I, S,  , L, I, G, H, T, E, N, I, N, G,  , F, A, S, T,  , C, L, U, S, T, E, R,  , C, O,
M, P, U, T, E, R,  , F, R, A, M, E, W, O, R, K)

scala> val wordlist = input.flatMap(l=>l)
wordlist: List[Char] = List(T, h, i, s,  , i, s,  , S, c, a, l, a,  , t, r, a, i, n, i, n, g, T, h,
i, s,  , i, s,  , S, p, a, r, k,  , w, i, t, h,  , S, c, a, l, a,  , t, r, a, n, g, S, p, a, r, k,
, i, s,  , p, a, r, t,  , o, f,  , B, i, g, d, a, t, a,  , T, e, c, h, n, o, l, o, g, i, e, s, S, p,
 a, r, k,  , i, s,  , l, i, g, h, t, e, n, i, n, g,  , f, a, s, t,  , c, l, u, s, t, e, r,  , c, o,
m, p, u, t, e, r,  , f, r, a, m, e, w, o, r, k)

scala> val wordlist = input.flatMap(l=>l.split(" "))
wordlist: List[String] = List(This, is, Scala, training, This, is, Spark, with, Scala, trang, Spark,
 is, part, of, Bigdata, Technologies, Spark, is, lightening, fast, cluster, computer, framework)
 
 scala> wordlist.foreach(println)
This
is
Scala
training
This
is
Spark
with
Scala
trang
Spark
is
part
of
Bigdata
Technologies
Spark
is
lightening
fast
cluster
computer
framework

scala> val group=wordlist.groupBy(e=>e)
group: scala.collection.immutable.Map[String,List[String]] = Map(is -> List(is, is, is, is), This ->
 List(This, This), Scala -> List(Scala, Scala), Technologies -> List(Technologies), lightening -> Li
st(lightening), training -> List(training), computer -> List(computer), framework -> List(framework)
, Bigdata -> List(Bigdata), cluster -> List(cluster), with -> List(with), Spark -> List(Spark, Spark
, Spark), fast -> List(fast), trang -> List(trang), part -> List(part), of -> List(of))

scala> group.foreach(println)
(is,List(is, is, is, is))
(This,List(This, This))
(Scala,List(Scala, Scala))
(Technologies,List(Technologies))
(lightening,List(lightening))
(training,List(training))
(computer,List(computer))
(framework,List(framework))
(Bigdata,List(Bigdata))
(cluster,List(cluster))
(with,List(with))
(Spark,List(Spark, Spark, Spark))
(fast,List(fast))
(trang,List(trang))
(part,List(part))
(of,List(of))

scala> val count=group.map(t=>(t._1,t._2.size))
count: scala.collection.immutable.Map[String,Int] = Map(is -> 4, This -> 2, Scala -> 2, Technologies
 -> 1, lightening -> 1, training -> 1, computer -> 1, framework -> 1, Bigdata -> 1, cluster -> 1, wi
th -> 1, Spark -> 3, fast -> 1, trang -> 1, part -> 1, of -> 1)

or

scala> val count=group.mapValues(v=>v.size)
count: scala.collection.immutable.Map[String,Int] = Map(is -> 4, This -> 2, Scala -> 2, Technologies
 -> 1, lightening -> 1, training -> 1, computer -> 1, framework -> 1, Bigdata -> 1, cluster -> 1, wi
th -> 1, Spark -> 3, fast -> 1, trang -> 1, part -> 1, of -> 1)

Total Count of word:
***********************************

scala> import scala.io.Source  (mandatory in REPL shell)


scala> Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList    (getLines- Read line by line, toList- converts to List)
res56: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> val input=Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList
input: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> input
res57: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)


scala> input.foreach(println)
This is Scala training
This is Spark with Scala trang
Spark is part of Bigdata Technologies
Spark is lightening fast cluster computer framework


scala> val wordlist = input.flatMap(l=>l.split(" "))
wordlist: List[String] = List(This, is, Scala, training, This, is, Spark, with, Scala, trang, Spark,
 is, part, of, Bigdata, Technologies, Spark, is, lightening, fast, cluster, computer, framework)
 
 scala> wordlist.foreach(println)
This
is
Scala
training
This
is
Spark
with
Scala
trang
Spark
is
part
of
Bigdata
Technologies
Spark
is
lightening
fast
cluster
computer
framework

scala> val totalcount=wordlist.count(x=>x!=null)
totalcount: Int = 23




mutable collection:
---------------------------------
wildcard aracter as _(underscore  not * as java)
scala> import scala.collection.mutable._
import scala.collection.mutable._


scala> val l=ListBuffer(1,2,3,4)
l: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2, 3, 4)

scala> val a=ArrayBuffer(2,2,35,64)
a: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(2, 2, 35, 64)


scala> import scala.collection.mutable.ListBuffer
import scala.collection.mutable.ListBuffer

scala> val lBuff=ListBuffer(1,3,4,4)
lBuff: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 3, 4, 4)

WE can change values as it's mutable:
*******************************************
scala> lBuff(2)=20

scala> lBuff
res1: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 3, 20, 4)

If u want to use as immutable while u r in immutable packae verbose way

---------------------------------------------------
scala> val l=scala.collection.immutable.List(1,2,4)
l: List[Int] = List(1, 2, 4)


To append data at end:
**************************

scala> lBuff
res4: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 3, 20, 4, 50)

scala> lBuff.append(250)

scala> lBuff
res6: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 3, 20, 4, 50, 250)

To append data at beginning:
**************************

scala> lBuff
res6: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 3, 20, 4, 50, 250)

scala> lBuff.prepend(250)

scala> lBuff
res8: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 1, 3, 20, 4, 50, 250)

add data in specific position
***************************************

definition of insert
-------------
scala> lBuff.insert
   def insert(n: Int, elems: A*): Unit
   
 Insert 3 elements
------------------------- 
scala> lBuff.insert(2,678,344,222)

scala> lBuff
res10: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 1, 678, 344, 222, 3, 20, 4, 50, 25
0)

Remove
------------------
lBuff.remove(position)

scala> lBuff
res10: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 1, 678, 344, 222, 3, 20, 4, 50, 25
0)

scala> lBuff.remove(1)
res11: Int = 1

scala> lBuff
res12: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 678, 344, 222, 3, 20, 4, 50, 250)

Remove 2 elements from specific position
--------------------------------------------------
lBuff(position,no. of elements)

scala> lBuff
res12: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 678, 344, 222, 3, 20, 4, 50, 250)

scala> lBuff.contains(250)
res16: Boolean = true

scala> lBuff
res17: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 222, 3, 20, 4, 50, 250)

scala> lBuff.takeRight(2)
res18: scala.collection.mutable.ListBuffer[Int] = ListBuffer(50, 250)

scala> lBuff
res19: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 222, 3, 20, 4, 50, 250)


scala> lBuff.remove(1,2)

scala> lBuff
res14: scala.collection.mutable.ListBuffer[Int] = ListBuffer(250, 222, 3, 20, 4, 50, 250)


Mutable Map:(only do in Key basis)
------------------------
 scala> import scala.collection.mutable._
import scala.collection.mutable._

scala> val m= Map(1->"Scala",2->"Spark")
m: scala.collection.mutable.Map[Int,String] = Map(2 -> Spark, 1 -> Scala)

scala> m(1)="SCALAA"

scala> m
res3: scala.collection.mutable.Map[Int,String] = Map(2 -> Spark, 1 -> SCALAA)

scala> m(1)
res4: String = SCALAA

Put & += is same (to add Single data)
---------------------------------------
scala> m.put(4,"Big")
res5: Option[String] = None

scala> m
res6: scala.collection.mutable.Map[Int,String] = Map(2 -> Spark, 4 -> Big, 1 -> SCALAA)

scala> m.put(4,"Big")
res7: Option[String] = Some(Big)

scala> m+=2->"sca"
res8: m.type = Map(2 -> sca, 4 -> Big, 1 -> SCALAA)

scala> m+=3 -> "aadawa"
res32: m.type = Map(3 -> aadawa)

scala> m
res33: scala.collection.mutable.Map[Int,String] = Map(3 -> aadawa)

scala> m+=4 -> "fdgfaadawa"
res34: m.type = Map(4 -> fdgfaadawa, 3 -> aadawa)

To add multiple elements: (put is only for adding 1 elemet)
*********************************
scala> m+=(4 -> "fdgfaadawa",5->"qjqdqkjdd")
res35: m.type = Map(5 -> qjqdqkjdd, 4 -> fdgfaadawa, 3 -> aadawa)

To remove (remove or -=)
---------------------

scala> m.remove(2)
res25: Option[String] = None

scala> m
res26: scala.collection.mutable.Map[Int,String] = Map(4 -> Big, 1 -> ghhj, 3 -> ghhj)

Remove multiple elements:
-----------------------------------

scala> m-=(4,5 )
res38: m.type = Map(3 -> aadawa)

Mutable Set :
----------------------
scala> val s= Set(1,24,5,5,66)
s: scala.collection.mutable.Set[Int] = Set(66, 1, 5, 24)

scala> s
res40: scala.collection.mutable.Set[Int] = Set(66, 1, 5, 24)

scala> s+=34
res41: s.type = Set(66, 1, 34, 5, 24)

scala> s-=2
res42: s.type = Set(66, 1, 34, 5, 24)

scala> s-=1
res43: s.type = Set(66, 34, 5, 24)

scala> s+=(23,45,56)
res44: s.type = Set(66, 45, 34, 5, 56, 24, 23)

scala> s-=(23,45,56)
res45: s.type = Set(66, 34, 5, 24)


scala> m.remove(1)
res27: Option[String] = Some(ghhj)

scala> m
res28: scala.collection.mutable.Map[Int,String] = Map(4 -> Big, 3 -> ghhj)

scala> m-=1
res29: m.type = Map(4 -> Big, 3 -> ghhj)

scala> m-=4
res30: m.type = Map(3 -> ghhj)

scala> m
res31: scala.collection.mutable.Map[Int,String] = Map(3 -> ghhj)

scala> m
res9: scala.collection.mutable.Map[Int,String] = Map(2 -> sca, 4 -> Big, 1 -> SCALAA)

Q- Take a list and fine out lesser number than threshold number 
---------------------------------------------------

scala> def lm(x:List[Int],max:Int)={
     | val out= x.partition(e=> (e<max))
     | println(out)}
lm: (x: List[Int], max: Int)Unit

scala> lm(List.range(1,10),4)
(List(1, 2, 3),List(4, 5, 6, 7, 8, 9))


Assignment: Read empname & technologies from emp file and produce output in form of employee distribution across technologies:(ETL->2 ,Hadoop->3  likewise) k
*********************************************************************


solu-1 (most convinient)
---------------------------------
scala> input
res29: List[String] = List(Dipti,Hadoop, Manish ,Hadoop, Ram,ETL, Hari,QA, Dip,DB2, Ayush,DB2, Radha
,DB2, Harish,QA)

scala> val out=input.map(e=>e.split(",")(1)).groupBy(x=>x).map(t=>(t._1,t._2.size))
out: scala.collection.immutable.Map[String,Int] = Map(DB2 -> 3, ETL -> 1, Hadoop -> 2, QA -> 2)

solu-2 
------------
scala> val out=input.flatMap(e=>e.split(",")).groupBy(x=>x).map(t=>(t._1,t._2.size)).filterKeys(k=>k
=="DB2"| k=="DB2"|k=="QA"|k=="Hadoop"|k=="ETL")
out: scala.collection.immutable.Map[String,Int] = Map(ETL -> 1, DB2 -> 3, QA -> 2, Hadoop -> 2)

solu-3 (verbose way)
------------------------------------
scala> val input=Source.fromFile("C:\\Users\\DSahoo\\Desktop\\emp.csv").getLines.toList
input: List[String] = List(Dipti,Hadoop, Manish ,Hadoop, Ram,ETL, Hari,QA, Dip,DB2, Ayush,DB2, Radha
,DB2, Harish,QA)

scala> input.foreach(println)
Dipti,Hadoop
Manish ,Hadoop
Ram,ETL
Hari,QA
Dip,DB2
Ayush,DB2
Radha,DB2
Harish,QA



scala> input(1)
res4: String = Manish ,Hadoop

scala> val wordList=input.flatMap(e=>e)
wordList: List[Char] = List(D, i, p, t, i, ,, H, a, d, o, o, p, M, a, n, i, s, h,  , ,, H, a, d, o,
o, p, R, a, m, ,, E, T, L, H, a, r, i, ,, Q, A, D, i, p, ,, D, B, 2, A, y, u, s, h, ,, D, B, 2, R, a
, d, h, a, ,, D, B, 2, H, a, r, i, s, h, ,, Q, A)

scala> val wordList=input.flatMap(e=>e.split(","))
wordList: List[String] = List(Dipti, Hadoop, "Manish ", Hadoop, Ram, ETL, Hari, QA, Dip, DB2, Ayush,
 DB2, Radha, DB2, Harish, QA)

scala> wordList.foreach(println)
Dipti
Hadoop
Manish
Hadoop
Ram
ETL
Hari
QA
Dip
DB2
Ayush
DB2
Radha
DB2
Harish
QA

scala> val group=wordList.groupBy(e=>e)
group: scala.collection.immutable.Map[String,List[String]] = Map(Radha -> List(Radha), Harish -> Lis
t(Harish), ETL -> List(ETL), Hari -> List(Hari), DB2 -> List(DB2, DB2, DB2), "Manish " -> List("Mani
sh "), Ayush -> List(Ayush), Dipti -> List(Dipti), QA -> List(QA, QA), Hadoop -> List(Hadoop, Hadoop
), Ram -> List(Ram), Dip -> List(Dip))


scala> group.foreach(println)(ETL))
(Hari,List(Hari))
(DB2,List(DB2, DB2, DB2))
(Manish ,List(Manish ))
(Ayush,List(Ayush))
(Dipti,List(Dipti))
(QA,List(QA, QA))
(Hadoop,List(Hadoop, Hadoop))
(Ram,List(Ram))
(Dip,List(Dip))

         

scala> val count=group.map(ele=>(ele._1,ele._2.size))
count: scala.collection.immutable.Map[String,Int] = Map(Radha -> 1, Harish -> 1, ETL -> 1, Hari -> 1
, DB2 -> 3, "Manish " -> 1, Ayush -> 1, Dipti -> 1, QA -> 2, Hadoop -> 2, Ram -> 1, Dip -> 1)

scala> count.foreach(println)
(Radha,1)
(Harish,1)
(ETL,1)
(Hari,1)
(DB2,3)
(Manish ,1)
(Ayush,1)
(Dipti,1)
(QA,2)
(Hadoop,2)
(Ram,1)
(Dip,1)


scala> val output=count.filterKeys(k=> k=="ETL"|k=="DB2"|k=="QA"|k=="Hadoop")
output: scala.collection.immutable.Map[String,Int] = Map(ETL -> 1, DB2 -> 3, QA -> 2, Hadoop -> 2)

scala> output.foreach(println)
(ETL,1)
(DB2,3)
(QA,2)
(Hadoop,2)
 
 
 
 (2 flavours of split)  split(",") && split(",")(1) )=> splits the data and gives only 1st position data
-------------------------------------------------------------------------------

scala> a.split
   def split(String): Array[String]   def split(String, Int): Array[String]


scala> input
res17: List[String] = List(Dipti,Hadoop, Manish ,Hadoop, Ram,ETL, Hari,QA, Dip,DB2, Ayush,DB2, Radha
,DB2, Harish,QA)

scala> val out=input.map(e=>e.split(",")(1))
out: List[String] = List(Hadoop, Hadoop, ETL, QA, DB2, DB2, DB2, QA)



scala> val out=input.map(e=>e.split(",")(0))
out: List[String] = List(Dipti, "Manish ", Ram, Hari, Dip, Ayush, Radha, Harish)

scala> input.map(l=>l.split(","))
res19: List[Array[String]] = List(Array(Dipti, Hadoop), Array("Manish ", Hadoop), Array(Ram, ETL), A
rray(Hari, QA), Array(Dip, DB2), Array(Ayush, DB2), Array(Radha, DB2), Array(Harish, QA))

scala> input.map(l=>l.split(",")(1))
res20: List[String] = List(Hadoop, Hadoop, ETL, QA, DB2, DB2, DB2, QA)

scala> val wordCountInput= Source.fromFile("C:\\Users\\DSahoo\\Desktop\\dummy.txt").getLines.toList
wordCountInput: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is
 part of Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> wordCountInput.map(e=>e.split(",")(0))
res25: List[String] = List(This is Scala training, This is Spark with Scala trang, Spark is part of
Bigdata Technologies, Spark is lightening fast cluster computer framework)

scala> wordCountInput.map(e=>e.split(" ")(0))
res26: List[String] = List(This, This, Spark, Spark)

scala> wordCountInput.map(e=>e.split(" ")(2))
res27: List[String] = List(Scala, Spark, part, lightening)

scala> wordCountInput.map(e=>e.split(" ")(2) (1))
res28: List[Char] = List(c, p, a, i)

Mutable collections:
----------------------------------
scala> import scala.collection.mutable.{Set=>MSet,Map=>MMap}
import scala.collection.mutable.{Set=>MSet, Map=>MMap}

scala> import scala.collection.mutable.{Set=>MSet,Map=>MMap,List=>MList}
<console>:14: error: object List is not a member of package scala.collection.mutable
       import scala.collection.mutable.{Set=>MSet,Map=>MMap,List=>MList}
              ^

scala> import scala.collection.mutable.{Set=>MSet,Map=>MMap}
import scala.collection.mutable.{Set=>MSet, Map=>MMap}

scala> val a=Array(1,2,3,4)
a: Array[Int] = Array(1, 2, 3, 4)

scala> val L=List(1,342,23,54)
L: List[Int] = List(1, 342, 23, 54)

scala> val l=ListBuffer(12,41,45,22)
<console>:15: error: not found: value ListBuffer
       val l=ListBuffer(12,41,45,22)
             ^

scala> import scala.collection.mutable._
import scala.collection.mutable._

scala> val l=ListBuffer(12,41,45,22)
l: scala.collection.mutable.ListBuffer[Int] = ListBuffer(12, 41, 45, 22)

scala> val a=Array(1,2,3,4)
a: Array[Int] = Array(1, 2, 3, 4)

scala> val L=List(1,342,23,54)
L: List[Int] = List(1, 342, 23, 54)

scala> val a=ArrayBuffer(1,2,3,4)
a: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 2, 3, 4)

scala> val s=Set(12,325,366)
s: scala.collection.mutable.Set[Int] = Set(12, 325, 366)

scala> val S=MSet(222,442,455,666)
S: scala.collection.mutable.Set[Int] = Set(222, 455, 666, 442)

scala> val map=Map(1->12,2->23,3->33)
map: scala.collection.mutable.Map[Int,Int] = Map(2 -> 23, 1 -> 12, 3 -> 33)

scala> val m=MMap(1->12,2->23,3->33)
m: scala.collection.mutable.Map[Int,Int] = Map(2 -> 23, 1 -> 12, 3 -> 33)

scala> L
res46: List[Int] = List(1, 342, 23, 54)

scala> l
res47: scala.collection.mutable.ListBuffer[Int] = ListBuffer(12, 41, 45, 22)

scala> l+=2
res48: l.type = ListBuffer(12, 41, 45, 22, 2)

scala> l
res49: scala.collection.mutable.ListBuffer[Int] = ListBuffer(12, 41, 45, 22, 2)

scala> L+=3
<console>:20: error: value += is not a member of List[Int]
       L+=3
	   
Insert  n no. of values in particular position************************

scala> l.insert(2,34,55)

scala> l
res56: scala.collection.mutable.ListBuffer[Int] = ListBuffer(12, 41, 34, 55, 45, 22)


map
----------------

scala> map
res68: scala.collection.mutable.Map[Int,Int] = Map(2 -> 23, 1 -> 3, 3 -> 33)

scala> map+=1->32
res69: map.type = Map(2 -> 23, 1 -> 32, 3 -> 33)

scala> map
res70: scala.collection.mutable.Map[Int,Int] = Map(2 -> 23, 1 -> 32, 3 -> 33)

scala> map+=10->32
res71: map.type = Map(2 -> 23, 10 -> 32, 1 -> 32, 3 -> 33)

scala> map
res72: scala.collection.mutable.Map[Int,Int] = Map(2 -> 23, 10 -> 32, 1 -> 32, 3 -> 33)

scala> map+=(10->32,2->5,6->56)
res73: map.type = Map(2 -> 5, 10 -> 32, 1 -> 32, 3 -> 33, 6 -> 56)


Class 20-Jul-2019
****************************
Keep in a file and execute:
-------------------------------------

C:\Users\Satyabrat\Desktop\scala\my program>scala recursion.scala
3628800
9.332621544394415268169923885626670E+157

Recursion:
-------------------
def factorial(n:Int):BigDecimal={
if(n==0 ||n==1)  return 1
else 
return n * factorial(n-1)
}

println (factorial(10))

Tail Recursion
-----------------------------------2q
def tfact(n:BigDecimal,acc:BigDecimal=1):BigDecimal = {
 if(n==0 || n==1) return acc
 else return tfact(n-1,n*acc)
 }
 
 println(tfact(100))    

 Partially applied function
 ---------------------------------------
 
 scala> def add(x:Int,y:Int)=a+b
<console>:10: error: not found: value a
       def add(x:Int,y:Int)=a+b
                            ^
<console>:10: error: not found: value b
       def add(x:Int,y:Int)=a+b
                              ^

scala> def add(x:Int,y:Int)=x+y
add: (x: Int, y: Int)Int

scala> add(10,20)
res0: Int = 30

scala> add(10,_:Int)
res1: Int => Int = <function1>

scala> val add1=add(10,_:Int)
add1: Int => Int = <function1>

scala> def add(x:Int,y:Int)=a+b
<console>:10: error: not found: value a
       def add(x:Int,y:Int)=a+b
                            ^
<console>:10: error: not found: value b
       def add(x:Int,y:Int)=a+b
                              ^

scala> def add(x:Int,y:Int)=x+y
add: (x: Int, y: Int)Int

scala> add(10,20)
res0: Int = 30

scala> add(10,_:Int)
res1: Int => Int = <function1>

scala> val add1=add(10,_:Int)
add1: Int => Int = <function1>


Currying and partial applied function:
----------------------------------------


scala> def mul(a:Int)(b:Int)=a*b
mul: (a: Int)(b: Int)Int

scala> val m=mul(2)
<console>:12: error: missing arguments for method mul;
follow this method with `_' if you want to treat it as a partially applied funct
ion
       val m=mul(2)
                ^

scala> val m=mul(2)(_)
m: Int => Int = <function1>

scala> m(3)
res95: Int = 6

scala> mul(3)(4)
res96: Int = 12

scala> m(5)
res97: Int = 10

Factorof

------------------

scala> def factorOf(x:Int,y:Int)=x%y==0
factorOf: (x: Int, y: Int)Boolean

scala> factorOf(2,4)
res4: Boolean = false

scala> factorOf(12,4)
res5: Boolean = true

scala> factorOf(12,30)
res6: Boolean = false

scala> val isEven=factorOf(2,_:Int)
isEven: Int => Boolean = <function1>

scala> isEven(12)
res7: Boolean = false

scala> isEven(11)
res8: Boolean = false

scala> val multipleOf3=factorOf(3,_:Int)
multipleOf3: Int => Boolean = <function1>

scala> multipleOf3(9)
res9: Boolean = false
 
Tuples:
-------------------
scala> val a=Tuple2(1,"qefqq")
a: (Int, String) = (1,qefqq)

scala> val a=Tuple3(1,"qefqq",23)
a: (Int, String, Int) = (1,qefqq,23)
`	

scala> val a=(12,3,4)
a: (Int, Int, Int) = (12,3,4)


Class- 21-Jul-19
------------------------

Create a class Employee (bydefault it's default which is public)anything in class is called member(property+method)
************************

scala> class Employee{
     | val id=10
     | val name="Scala"
     | val salary=10000.0
     | def displayDetails()={
     | println(s"Employee :$id $name $salary")
     | }
     | }
defined class Employee

scala> val obj=new Employee
obj: Employee = Employee@4628b1d3

scala> obj.id
res0: Int = 10


scala> obj.salary
res2: Double = 10000.0

scala> obj.displayDetails
Employee :10 Scala 10000.0

Create class with Var:
-------------------------------
   

scala> class Employee{
     |       var id=10
     |      var name="Scala"
     |       var salary=10000.0
     |       def displayDetails()={
     |       println(s"Employee :$id $name $salary")
     |       }
     |       }
defined class Employee

scala> val obj= new Employee
obj: Employee = Employee@5f84abe8

scala> obj.id
res4: Int = 10

scala> val obj1=new Employee
obj1: Employee = Employee@4a29f290

scala> obj1.id=100
obj1.id: Int = 100


scala> obj1.displayDetails()
Employee :100 Scala 10000.0
      
	  
Constructor in scala:
------------------------------------	  

scala> class Employee(idp:Int,namep:String,salaryp:Double){
            val id=idp
			val name=namep
			val salary=salaryp
            def displayDetails()={
            println(s"Employee :$id $name $salary")
            }
            }
			
scala> val e1=new Employee(1,"dip",2000.8)
e1: Employee = Employee@541179e7

scala> e1.displayDetails
Employee :1 dip 2000.8

scala> val e2=new Employee(10,"asd",2030.8)
e2: Employee = Employee@204abeff

scala> e2.displayDetails
Employee :10 asd 2030.8	

keep the class in a file and run it
--------------------------------------------

C:\Users\DSahoo\Desktop\scala>scalac Employee.scala

C:\Users\DSahoo\Desktop\scala>javap Employee.class  //to decopmile and see the internal code
Compiled from "Employee.scala"
public class Employee {
  public int id();
  public java.lang.String name();
  public double salary();
  public void displayDetails();
  public Employee(int, java.lang.String, double);
}
C:\Users\Satyabrat\Desktop\scala\my program>scalac em.scala

C:\Users\Satyabrat\Desktop\scala\my program>javap em.class
Compiled from "em.scala"
public class Em {
  public int id();
  public java.lang.String name();
  public double salary();
  public void displayDetails();
  public Em(int, java.lang.String, double);
}
	
C:\Users\DSahoo\Desktop\scala>scalac Employee.scala

C:\Users\DSahoo\Desktop\scala>javap Employee.java
Error: class not found: Employee.java

C:\Users\DSahoo\Desktop\scala>javap Employee.class
Compiled from "Employee.scala"
public class Employee {
  public int id();
  public java.lang.String name();
  public double salary();
  public void displayDetails();
  public Employee(int, java.lang.String, double);
}

class Employee(idp:Int,namep:String,salaryp:Double){
            val id=idp
			val name=namep
			val salary=salaryp
            def displayDetails()={
            println(s"Employee :$id $name $salary")
            }
            }
			
			
class Employee1(val id:Int,val name:String,val salary:Double){
def displayDetails()={
            println(s"Employee :$id $name $salary")
            }
            }			
			
class Employee2(val id:Int,val name:String,var salary:Double){
def displayDetails()={
            println(s"Employee :$id $name $salary")
            }
            }		

class Employee( private val _id:Int,val name:String,var salary:Double){

def id():Int=_id
def displayDetails()={
            println(s"Employee :$id $name $salary")
            }
            }	

auxillary constructor:(constructor overloading)
___________________________________________________

 

scala>  class Person(val firstName:String,val lastName:String,val middleName:String){
     | def this(fName:String,lName:String)={
     | this(fName,lName," Unknown")
     | }}
defined class Person

scala> val p1=new Person("DIP","MAY","dhf")
p1: Person = Person@5384ce66



scala> p1.lastName
res18: String = MAY

scala> p1.firstName
res19: String = DIP

scala> p1.middleName
res20: String = dhf

scala> class Person(val firstName:String,val lastName:String,val middleName:String){
println("primary")
def this(fName:String,lName:String)={
this(fName,lName," Unknown")
println("auxillary")
}
def displayFullName()={
println(s"fullname:$firstName,$middleName,$lastName")
}
}	

scala> val p1=new Person("adawd","TEWGFEWG")
primary
auxillary
p1: Person = Person@3ee6dc82

scala> val p1=new Person("adawd","TEWGFEWG","egftwegew"))
<console>:1: error: ';' expected but ')' found.
val p1=new Person("adawd","TEWGFEWG","egftwegew"))
                                                 ^

scala> val p1=new Person("adawd","TEWGFEWG","egftwegew")
primary
p1: Person = Person@5c25d0d1


scala> class Person(val firstName:String,val lastName:String,val middleName:String){
     | println("primary")
     | def this(fName:String,lName:String)={
     | this(fName,lName," Unknown")
     | println("auxillary")
     | }
     | def displayFullName()={
     | println(s"fullname:$firstName,$middleName,$lastName")
     | }
     | println("after primary")
     | }
defined class Person

scala> val p1=new Person("adawd","TEWGFEWG")
primary
after primary
auxillary
p1: Person = Person@7ce30c0b

from 2nd auxillary call 1st auxillary and then primary;
----------------------------------------------------------

scala> class Person(val firstName:String,val lastName:String,val middleName:String){
      println("primary")
      def this(fName:String,lName:String)={
      this(fName,lName," Unknown")
      println("1st auxillary")
      }
	  def this(fName:String)={
	  this(fName,",")
	  println("inside  2nd auxillary")
      }	  
      def displayFullName()={
      println(s"fullname:$firstName,$middleName,$lastName")
      }
      println("after primary")
      }
	  
	  defined class Person

scala> val p=new Person("asdasf")
primary
after primary
auxillary
inside  2nd auxillary
p: Person = Person@5aac6f9f

scala> val p=new Person("asdasf","adawdawwf")
primary
after primary
auxillary
auxillary
p: Person = Person@f237ae7

scala> val p=new Person("asdasf","adawdawwf","sgsegesg")
primary
after primary
p: Person = Person@299cab08

Default argument constructor: (only middlename as optional)
--------------------------------------

Q-class Person(val firstName:String,val lastName:String,val middleName:String="no midllename "){

def displayFullName()={
      println(s"fullname:$firstName,$middleName,$lastName")
      } }
	  
scala> val p=new Person("eq222","QWRQWRQ") //if u r not giving while creating object,then it will pick default
p: Person = Person@7d49a1a0

scala> p.displayFullName
fullname:DDafd,no midllename ,afawfawf

scala> val p=new Person("eq222","QWRQWRQ","QREQRQ##RQ")  //if u r specifying while creating object it will override
p: Person = Person@44392e64

scala> p.displayFullName
fullname:eq222,QREQRQ##RQ,QWRQWRQ
------------------------------------------
Named Parameter: (bind the parameter with name)
--------------------------------
class Person(val firstName:String="no fname",val lastName:String,val middleName:String="no midllename "){

def displayFullName()={
      println(s"fullname:$firstName,$middleName,$lastName")
      } }
	  
scala> val p=new Person(lastName="aafasfa")
p: Person = Person@34e53c02

scala> p.displayFullName
fullname:no fname,no midllename ,aafasfa	  

If u r not binding with name it will throw error as below:(as bydefault it will bind with 1st parameter) to avoid this go for named parameters:*********************
-------------------------------------------------------------------------------------------------------------------------------------------------------------------

scala> val p=new Person("aafasfa")
<console>:12: error: not enough arguments for constructor Person: (firstName: String, lastName: Stri
ng, middleName: String)Person.
Unspecified value parameter lastName.
       val p=new Person("aafasfa")
	   
	   
Class notes:-22-July-2019
*****************************************************

Case Classes
-------------------------------------------------

scala> class Person(val firstName:String,val lastName:String)
defined class Person

scala> val p=new Person("fName","LName")
p: Person = Person@5b3f61ff

scala> val a:Any=10
a: Any = 10

hashcode:
-------------------                                  	   
p: Person = Person@5b3f61ff

toString: (when u acess object of class internally it calls toString method)
--------------------
scala> p.toString
res0: String = Person@5b3f61ff

equals:(it compares hashcode of objects
------------------------------
scala> val p1=new Person("fName","LName")
p1: Person = Person@7923f5b3

scala> p.equals(p1)
res1: Boolean = false

scala> p.firstName
res2: String = fName
	
scala> p.lastName
res3: String = LName

scala> p1.lastName
res4: String = LName

scala> p1.firstName
res5: String = fName

case class: (toString ,equals,hasCode methods are  well defined/implemented)
-----------------------

scala> class Person1(val firstName:String,val lastName:String)
defined class Person1

scala> case class Person(val firstName:String,val lastName:String)
defined class Person



scala> val p_1=new Person1("fName","lName")
p_1: Person1 = Person1@6d0b0da6

scala> val p1=new Person("fName","lName")
p1: Person = Person(fName,lName)

scala> p_1
res6: Person1 = Person1@6d0b0da6

scala> p1
res7: Person = Person(fName,lName)

scala> val p2=new Person("fName","lName")
p2: Person = Person(fName,lName)

case Class equals mnethod
-------------------------------

scala> p1 equals p2
res8: Boolean = true
case class Hashcode method:
 ----------------
scala> p1.hashCode
res10: Int = 416559252


case Class toString method
----------------------------
scala> p2.hashCode

scala> p1.toString
res13: String = Person(fName,lName)

scala> p2.toString
res14: String = Person(fName,lName)
res12: Int = 416559252
 
difference between normal class and case class
---------------------------------------------

toString,equals ,hashCode is automatically invoked. (well implemented)
no need of new keyword , no need of val and var in constructor arguments
 
scala> case class Person(val firstName:String,val lastName:String)
defined class Person

scala> val p_3=Person("hai","hello")  // object created without new keyword
p_3: Person = Person(hai,hello) 

Case class:
-----------------------------
scala> val p_3=Person("hai","hello")
p_3: Person = Person(hai,hello)

scala> val p_4=Person("hai","hello")
p_4: Person = Person(hai,hello)

comparing state of the objects(both == and equals are same)
-------------------------------------------
scala> p_3 equals p_4ss
res21: Boolean = true

scala> p_3==p_4
res17: Boolean = true

comparing reference 
----------------------------
scala> p_3 eq p_4
res18: Boolean = false

scala> val p_10=p_3    // assigning p_3 to p_10 (object is same so reference is same)
p_10: Person = Person(hai,hello)

scala> p_10 eq p_3   //as reference is same  it's giving true 
res19: Boolean = true


Class- 23-Jul-19
-------------------------

Q- Create a class named arithmeticoperation and expose methods to perform different arithmetic operations: (in Intellij)
********************************************************************************************************************

package learning

/**
  * Created by DSahoo on 7/23/2019.
  */
class ArithOps(val x:Int, val y:Int) {

    def add = x + y

    def sub = x - y

    def mul = x * y

    def div = x / y

    def mod = x % y
  }

object RunApp{
  def main(args: Array[String]): Unit = {
    val input1=args(0).toInt   //  commandline args are string , convert it to int
    val input2=args(1).toInt
    val ob=new ArithOps(input1,input2)
    println(ob.add)
    println(ob.mul)
  }
}

Q- Creat a class for employee attributes like eid,ename,deptid,basic salary. Expose a method for computing gross salary where gross=basic salary +hra(30%of basic)+da(10% of basic)
 and expose one more method to display all attributes of an employee
 ************************************************
 
 Create 2 class one for emp and one for object(main method)
 --------------------------------------------------------------
 EmpDeatils.scala
 ---------------
 package learning

/**
  * Created by DSahoo on 7/23/2019.
  */
class EmpDetails(val eid:Int,val ename:String,val deptid:Int,val basic:Double) {
  val hra = (30 * basic) / 100
  val da = (10 * basic) / 100

  def grossSal():Double = {
    val gross = basic + hra + da
    gross
  }

    def display = println(s"Employeeid: $eid  EmployeeName: $ename DepartmentId:$deptid grosssalary: $grossSal")
  }

  EmpApp.scala
  ---------------
  
  import learning.EmpDetails

/**
  * Created by DSahoo on 7/23/2019.
  */
object EmpApp {
  def main(args: Array[String]): Unit = {
    val id=args(0).toInt
    val name=args(1)
    val dept=args(2).toInt
    val basic= args(3).toDouble
     val ob= new EmpDetails(id,name,dept,basic)
    ob.grossSal()   //this is optional  as in our below display method  indirectly grosssal computation will happen)
    ob.display
  }

}

with hardcode:
--------------
import learning.EmpDetails

/**
  * Created by DSahoo on 7/23/2019.
  */
object EmpDefApp {

  def main(args: Array[String]): Unit = {
    val ob = new EmpDetails(2,"ASSFFWF",12,2666.0)
    ob.display
  }
}

Write a program that has list in its class. Invoke 2 separate anonymous function one prints even number and other prints odd number
------------------------------------------------------------

package learning

/**
  * Created by DSahoo on 7/23/2019.
  */
class EvenOdd(val x:List[Int]) {

  def evod(f: List[Int] => List[Int]) = {

    val out = f(x)
    out.foreach(println)
  }
}

  object RunEvenOdd{
    def main(args: Array[String]): Unit = {
      val ob= new EvenOdd(List.range(1,13))
      println("List of even numbers")
      ob.evod(ele=>ele.filter(ele=>ele%2==0))
      println ("list of odd numbers")
      ob.evod(ele=>ele.filterNot(ele=>ele%2==0))
    }
  }

  Q-(mine)------------------------
  /**
  * Created by DSahoo on 7/23/2019. Take a number and check whether it's even or odd using anonymous function
  */
class EvenOddNumber(x:Int) {

  def evenOddFunc(f:Int =>Boolean) =
  {
    val out = f(x)
    println(out)
  }
}


object RunEvenOdd{

  def main(args: Array[String]): Unit = {
    val inp= args(0).toInt
    val ob= new EvenOddNumber(inp)
    ob.evenOddFunc(x =>x%2==0)
   ob.evenOddFunc(x => x%2!=0)
  }
}


Q- reuse properties of ARithOps class
-----------------------------------------------
Super Class
********************
package learning

/**
  * Created by DSahoo on 7/23/2019.
  */
class ArithOps(val x:Int, val y:Int) {

    def add = x + y

    def sub = x - y

    def mul = x * y

    def div = x / y

    def mod = x % y
  }

object RunApp{
  def main(args: Array[String]): Unit = {
    val input1=args(0).toInt   //  commandline args are string , convert it to int
    val input2=args(1).toInt
    val ob=new ArithOps(input1,input2)
    println(ob.add)
    println(ob.mul)
  }
}



Subclas (we are  overriding parent class add method and 2 new method as min and max)
*****************
package Inheritance

import learning.ArithOps

/**
  * Created by DSahoo on 7/24/2019.
  */
class AllArithmeticOperations(x:Int,y:Int) extends ArithOps(x,y) {

  def min()=x min y
  def max()=x max y

  override def add: Int = x+y+10
}

object RunAllArithOps{
  def main(args: Array[String]): Unit = {
    val input1=args(0).toInt
    val input2=args(1).toInt
    val obj = new AllArithmeticOperations(input1,input2)

    println( s"sum is ${obj.add}")
    println( s"max is ${obj.max()}")
  }
}

Class- 25-July-2019
******************************

currency class: (here main method is not there but extends App is there . 2 ways of executing program.but recommended is main method in spark application)
-------------------------

package org.training.scala.abstractexamples

/**
  * Created by sumantht on 5/24/2017.
  */

abstract class Currency {
  val amount:Long
  def designation:String  //units whether Dollar,INR etc
  override def toString: String = amount + " " + designation  //while acessing object of the class internally toString method gets called. for normal class toString method is not well 
  implemented. so for more information we are overriding toString method)
}

abstract class Dollar extends Currency {
  def designation = "USD"
}

abstract class Euro extends Currency {
  override def designation: String = "Euro"  //override keyword is not mandatory. override must required when we are redefining a method i.e method definition was existing
}

object AbstractTest2 extends App{
  val d = new Dollar { val amount = 50l }  // at the time of object creation we can provide definition for abstract members (scala special feature)
  val e = new Euro { val amount = 67l} //
  val d1 = new Dollar {                       //we are overriding both method and variable while craeting object with reference d1. Not only class level u can redefine members 
    val amount:Long  = 1000000   
    override def designation: String = "test"
  }
  println(d)
  println(e)
  println(d1)
}


Animal class:
package org.training.scala.abstractexamples

/**
  * Created by sumantht on 5/24/2017.
  */

abstract class Animal {
  //val animal_name:String
  val animal_name:String
  def speak:String
  def printAnimalName = println("I am "+ animal_name)
}

class Dog extends Animal{
  val animal_name = "Dog"
  def speak = { "woof....." }
}

class Cat extends Animal {
  val animal_name = "Cat"
  def speak = { "meoww... meoww"}
}

object AbstractTest {

  def main(args: Array[String]): Unit = {

    val d = new Dog
    println(d.speak)
    d.printAnimalName

    val c = new Cat
    println(c.speak)
    c.printAnimalName
	
	 val e:Animal=new Cat  // parent reference but actual object is for cat or child class
    e.printAnimalName
  }
}


class-27-Jul-19
****************************

--We can put restrictions for using trait
--we can add  a trait while creating an object

savings account  (file logger example)
--------------------------------

package org.training.scala.traitsexample.examples3

import java.io.PrintWriter
import java.util.Date

/**
 * Created by hduser on 6/9/16.
 */

trait Logged  {

  def log(msg: String) = {}
}


trait FileLogger extends  Logged {
  override def log(msg: String) = {

    println("Writing to a file")
    new PrintWriter("fileLogger.txt") { write(msg); close }  //PrintWriter inbuilt class of Java to write to file
  }
}

trait ConsoleLogger extends Logged {
  override def log(msg: String) = {println(msg)}
}


trait TimestampLogger extends  Logged {

  override def log(msg: String) = {

    println("TimestampLogger")
    super.log(new Date() + " " + msg)
  }
}

trait  ShortLogger extends  Logged {
  var maxLength = 15

  override def log(msg: String) = {
    println("ShortLogger")
    super.log{
      if (msg.length <= maxLength) msg
      else msg.substring(0, maxLength - 3) + "..."
    }
  }
}

class SavingsAccount extends Logged {

  var balance: Double = 0.0

  def withdraw(amount: Double) = {

    if (amount > balance) log("Insufficient funds")
    else balance -= amount
  }
}

object TraitTesting {

  def main(args: Array[String]) {

    val acc1 = new SavingsAccount with ConsoleLogger with FileLogger  //object level inheritance form more than 1 Trait (total 3 traits,to resolve
    //ambiguity it  gives to filelogger
    acc1.withdraw(500.0)
    val acc2 = new SavingsAccount with FileLogger with ConsoleLogger
    acc2.withdraw(500.0)

    val acc3 = new SavingsAccount with ConsoleLogger with TimestampLogger with ShortLogger {   //always object level trait will get priority over class level
      maxLength = 20
    }

    acc3.withdraw(500.0)
  }

}

fileInputstrea  trait: (Limiting trait to few classes)
_-------------------------------
package org.training.scala.traitsexample.examples1

import java.io.{InputStream, FileInputStream, FileOutputStream}


/*
Marking Traits So They Can Only Be Used by
Subclasses of a Certain Type

*/

trait Buffering {
	this: InputStream =>     // restricting the trait Buffering to InputStream class and subclass   (syntax is this:)   //

	val BUF_SIZE: Int = 5
	private val buf = new Array[Byte](BUF_SIZE)
	private var bufsize: Int = 0
	private var pos: Int = 0

	override def read(): Int = {
		if (pos >= bufsize) {
			bufsize = this.read(buf, 0, BUF_SIZE)
			if (bufsize > 0) -1
			pos = 0
			println("inside overriden read method")
		}
		pos += 1
		buf(pos-1)
	}
}

object BufferingTesting {
	
	def main(args: Array[String]) {

		val f = new FileInputStream("Buffering.txt") with Buffering    //fileinputstream inbult java class to read file

		println(f.read())                 //reading buffering .txt file .read()of fileinputstream is not optimized we are writting enhanced code
		//people who are using FileInputStream can use my trait (Restrict Trait to lomited users who r using FileInputStream read () method

	}
}


Magic apply method: (is used to desing factory design pattern. using this method without new keyword object can be created) //benifit- object creation is the most benifit and automatically called
-----------------------------

scala> val l=List(1,2,3,4)
l: List[Int] = List(1, 2, 3, 4)

scala> val l=List.apply(1,2243,1212)
l: List[Int] = List(1, 2243, 1212)

Using case class: no need of new keyword
-----------------------------------

scala> case class Test(id:Int)
defined class Test

scala> val t1=Test(1)
t1: Test = Test(1)

scala> val t1= new Test(1)
t1: Test = Test(1)
------------------------------------------------

package org.training.scala.magicapply

/**
  * Created by Arjun on 11/1/2016.
  */

class Foo(var x:Int) {
  def apply(y:Int) = x + 10
  def apply(y1:Int, z:Int) = x + y1 + z
}

object MagicApply extends App {
 val l = List(1,2,3)
  val foo = new Foo(10)
  println(foo.apply(20))
  println(foo(20))
  println(foo(20, 30))

}


class -27-Jul-19
****************
companion object (segrregation of static and non static methods thru companion class  nd companion objects)
-----------------------------
package org.training.scala.companionobjects

/**
 * Created by Arjun.
  * Companion object accessing its representative class's private information
 */

class Person(val name: String, private  val superHeroName: String) /*{
  Person.tellMeSomething("test")
}*/

object Person {

  def tellMeTheSecret(x: Person) = x.superHeroName
  //private def tellMeSomething(msg:String) = println(msg)


}
object CompanionExample2 {
  def main(args: Array[String]) {
    //println(args(0),args(1))
    val l = List(1,2,3,4)

    val peter = new Person("Peter Parker", "SpiderMan")

    println(Person.tellMeTheSecret(peter))
  }

}

factory design pattern
****************************

 scala> val l= new List(1,2,3)   //not possible as internally it uses apply method
<console>:10: error: class List is abstract; cannot be instantiated
       val l= new List(1,2,3)
              ^

scala> val l=List(1,2,3)
l: List[Int] = List(1, 2, 3)

scala> val l=List.apply(1,2,3)
l: List[Int] = List(1, 2, 3)


our own factory design pattern*
***************************************











Implicit parameter (ex: sortBy HOF automaticatilly Sorts Integer, strings . But if we write our own then for int less than
greaterthan and for string comareTo. but scala does it implicitly)
***********************


scala> l
res0: List[Int] = List(1, 2, 3)

scala> l.sortBy

def sortBy[B](f: A => B)(implicit ord: scala.math.Ordering[B]): Repr

scala> l.sortBy(x=> -x)
res1: List[Int] = List(3, 2, 1)

scala> l.sortBy(x=> x)
res2: List[Int] = List(1, 2, 3)
 
scala> val ls= List("scala","spark","hadoo")
ls: List[String] = List(scala, spark, hadoo)

scala> ls.sortBy(x=> x)
res3: List[String] = List(hadoo, scala, spark)

scala> "asd" compareTo"asd"
<console>:1: error: ';' expected but string literal found.
"asd" compareTo"asd"
      ^

scala> "asd" compareTo ("asd")
res4: Int = 0

scala> "asd" compareTo ("asdq")
res5: Int = -1

scala> "asdq" compareTo ("asd")
res6: Int = 1

curying: Implicit

---------------
scala> def add(x:Int)(y:Double)=x+y
add: (x: Int)(y: Double)Double

scala> add(10)
<console>:12: error: could not find implicit value for parameter y: Int
       add(10)
          ^

scala> implicit val test="20"
test: String = 20

scala> implicit val dummy= 30

scala> add(10)
res8: Int = 40
dummy: Int = 30

scala> implicit val dummy1=30 // if more than 1 implicit  values defined in shell it wil throw  ambigous error
dummy1: Int = 30

scala> add(10)
<console>:17: error: ambiguous implicit values:
 both value dummy of type => Int
 and value dummy1 of type => Int
 match expected type Int
       add(10)
          ^
implicit 		


Spark Class(4-Aug-19)
-----------------------------

info mysql ----------
error php----------
error mysql-------
warn php----------
info php------------
error mysql--------

to read  text file
*******************
sc.textfie(' ') text file delimiter bydefault \n

hadoop RDD of String type(as log file has numeric ,char etc)
to get errors from php and mysql
steps
**********
Load(using text file method gives RDD 1)
Filter Error Records (starts with Error)
check on 2nd column wther PHP or MYSQL and apply Map 
from map 2 parts 1- Filter on MYSQL basis
                 2-Filter on PHP basis 
Apply count on both records				 
-----------------------------------------------				 
				 
				 scala> val l= List.range(1,10)
l: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)


scala> sc.parallelize
                                                                                                  
def parallelize[T](seq: Seq[T], numSlices: Int)(implicit scala.reflect.ClassTag[T]): rdd.RDD[T]   

scala> sc.parallelize
                                                                                                  
def parallelize[T](seq: Seq[T], numSlices: Int)(implicit scala.reflect.ClassTag[T]): rdd.RDD[T]   

scala> val rdd= sc.parallelize(l)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29

scala> rdd.count
res1: Long = 9                                                                  

scala> rdd.collect
res2: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)

scala> 
scala> rdd.map
map                        mapPartitions              mapPartitionsWithContext   
mapPartitionsWithIndex     mapPartitionsWithSplit     mapWith                    

scala> rdd.map
                                                                    
def map[U](f: T => U)(implicit scala.reflect.ClassTag[U]): RDD[U]   

scala> rdd.map
                                                                    
def map[U](f: T => U)(implicit scala.reflect.ClassTag[U]): RDD[U]   

scala> val rdd1= rdd.map(e=>e*10)
rdd1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:31

scala> rdd1
res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:31

scala> rdd1.toDebugString
res4: String = 
(3) MapPartitionsRDD[1] at map at <console>:31 []
 |  ParallelCollectionRDD[0] at parallelize at <console>:29 []

scala> rdd1.collect
res5: Array[Int] = Array(10, 20, 30, 40, 50, 60, 70, 80, 90)

scala> rdd1.count
res6: Long = 9

scala> val evenRdd=rdd1.filter(e=>e>50)
evenRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at <console>:33

scala> evenRdd
res7: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at <console>:33

scala> evenRdd.collect
res8: Array[Int] = Array(60, 70, 80, 90)

scala> evenRdd.toDebugString
res9: String = 
(3) MapPartitionsRDD[2] at filter at <console>:33 []
 |  MapPartitionsRDD[1] at map at <console>:31 []
 |  ParallelCollectionRDD[0] at parallelize at <console>:29 []

scala> 

scala> val rdd=sc.parallelize(l,2) 
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:29

scala> rdd.partitions.length
res10: Int = 2

scala> rdd.getNumPartitions  // same as partitions.length
res11: Int = 2

scala> val rdd= sc.makeRDD(l,3) // same as parellize method, makeRDD internally uses paralleize method
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at makeRDD at <console>:29

Class- 5-Aug-19
*******************
scala> val fileRDD= sc.textFile("home/cloudera/Desktop/spark.txt")
fileRDD: org.apache.spark.rdd.RDD[String] = home/cloudera/Desktop/spark.txt MapPartitionsRDD[3] at textFile at <console>:27
Above is default which is from HDFS so it will give error while u r doing  action collect. 8080- which is namenode bydefault port.
 
whern u r reading data in local mode we should give (file://) bydefault it will search in hdfs (hdf://) in s3 or cloud ops storage(s3://)
-------------------------------------------------------------------------------------

scala> val fileRDD= sc.textFile("file:///home/cloudera/Desktop/data/spark")  
fileRDD: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/spark MapPartitionsRDD[9] at textFile at <console>:27

scala> fileRDD.getNumPartitions
res10: Int = 2

scala> fileRDD.collect
res11: Array[String] = Array(This is Spark, This is scala class, This is DVS)

scala> fileRDD.foreach(println)  
This is Spark
This is scala class
This is DVS

scala> fileRDD.glom
res13: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at glom at <console>:30

scala> fileRDD.glom.collect
res14: Array[Array[String]] = Array(Array(This is Spark, This is scala class), Array(This is DVS))

Desktop intellij(local mode)
-------------------------------
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Satyabrat on 8/6/2019.
  */
object MyFirstSpark {
  def main(args: Array[String]): Unit = {
    val obj = new SparkConf
    obj.setMaster("local").setAppName("firstspark")
    val sc = new SparkContext(obj)
    val fileRDD = sc.textFile("C:\\Users\\Satyabrat\\Desktop\\input\\inp.txt")
    fileRDD.collect.foreach(println)
  }
}



class-7-Aug-2019 (println internally calls toString method. In Array toString method will give hascode as it is not well defined)
------------------------

Total Wordcount:
----------------------

scala> val fileRdd= sc.textFile("file:///home/cloudera/Desktop/data/test.txt")
fileRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/test.txt MapPartitionsRDD[3] at textFile at <console>:27


scala> val mapRdd= fileRdd.flatMap(e=>e.split(" ")).count
mapRdd: Long = 10

wordcount in sparkcore:
---------------------------------------------
scala>  val fileRdd= sc.textFile("file:///home/cloudera/Desktop/data/spark")
fileRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/spark MapPartitionsRDD[1] at textFile at <console>:27

scala> val mapRdd=fileRdd.flatMap(e=>e.split(" ")).map(e=>(e,1)).reduceByKey(_+_)
mapRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:29

scala> mapRdd.collect
res7: Array[(String, Int)] = Array((scala,1), (is,1), (This,1), (first,1), (my,1), (&,1), (program,1), (spark,1), (in,1), (Hi,1))
-------------------------------------
2nd way:
**********
scala> fileRdd.collect
res0: Array[String] = Array(This is Spark, This is scala class, This is DVS)

scala> fileRdd.foreach(println)
This is DVS
This is Spark
This is scala class



scala> val wordsRdd= fileRdd.map(line=>line.split(" "))  //instead of map step go for flatMap, flatten will not work on RDD
wordsRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:29

scala> wordsRdd.collect
res2: Array[Array[String]] = Array(Array(This, is, Spark), Array(This, is, scala, class), Array(This, is, DVS))

scala> val wordsRdd=fileRdd.flatMap(line=>line.split(" "))
wordsRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:29

scala> wordsRdd.foreach(println)
This
This
is
Spark
Thisx
is
scala
class
is
DVS


scala> val wordsPairRdd=wordsRdd.map(ele=>(ele,1))
wordsPairRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:31

scala> wordsPairRdd.foreach(println)
(This,1)
(is,1)
(Spark,1)
(This,1)
(is,1)
(scala,1)
(class,1)
(This,1)
(is,1)
(DVS,1)

scala> val groupedWordsRdd= wordsPairRdd.groupByKey()
groupedWordsRdd: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[5] at groupByKey at <console>:33

scala> groupedWordsRdd.foreach(println)  // it will give in compact buffer, by default as when u apply shuffle operation it will use compression technique
(scala,CompactBuffer(1))
(Spark,CompactBuffer(1))
(DVS,CompactBuffer(1))
(is,CompactBuffer(1, 1, 1))
(This,CompactBuffer(1, 1, 1))
(class,CompactBuffer(1))

scala> val wordCountRdd= groupedWordsRdd.map(t=>(t._1,t._2.size))
wordCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[6] at map at <console>:35

scala> wordCountRdd.foreach(println)
(scala,1)
(is,3)
(This,3)
(class,1)
(Spark,1)
(DVS,1)
---------------------------------
ReduceByKey (It's like combiner agrregation happens before shuffle phase, so less no. of key,value pair will be generated 
This can't be applied in avg,sub,div opeartions. posiible in case of min,max,sum,mul where assosciative and commitive rules can be used.
----------------------------------
scala> val wordCountRdd_r=wordsPairRdd.reduceByKey(_+_)
wordCountRdd_r: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[7] at reduceByKey at <console>:33



scala> wordCountRdd_r.foreach(println)
(scala,1)
(is,3)
(This,3)
(class,1)
(Spark,1)
(DVS,1)
-----------------
using CountByValue:
**********************
scala> val fileRdd= sc.textFile("file:///home/cloudera/Desktop/data/test.txt")
fileRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/test.txt MapPartitionsRDD[78] at textFile at <console>:27

scala> val out=fileRdd.flatMap(e=>e.split(" ")).countByValue
out: scala.collection.Map[String,Long] = Map(program -> 1, in -> 1, is -> 1, This -> 1, & -> 1, my -> 1, spark -> 1, scala -> 1, first -> 1, Hi -> 1)


total word count in a file:
----------------------------
 load the input
 
scala> val mapRdd=inpRdd.flatMap(e=>e.split(" "))
mapRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at flatMap at <console>:29

scala> mapRdd.count
res9: Long = 10
----------------------------
test.txt:
1,2
1,2
1,3
2,2
2,4
2,6

val testData=sc.textFile("/user/cloudera/spark_ip/test.txt")
val testSplit=testData.map(x=>(x.split(",")(0),x.split(",")(1)))
val groupData=testSplit.groupByKey().sortByKey().map(x=>(x. _1,x. _2.mkString(",")))
--------------------------------
2nd way:
****************
scala> val inp=scala.io.Source.fromFile("//home//cloudera//Desktop//data//new").getLines.toList
inp: List[String] = List(1,2, 1,2, 1,3, 2,2, 2,4, 2,6)

scala> val inpRdd=sc.makeRDD(inp)
inpRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at makeRDD at <console>:29

scala> inpRdd.collect
res3: Array[String] = Array(1,2, 1,2, 1,3, 2,2, 2,4, 2,6)

scala> val mapRdd=inpRdd.map(e=>{val row=e.split(",")(row(0),row(1))})
mapRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[7] at map at <console>:31

scala> mapRdd.collect
res8: Array[(String, String)] = Array((1,2), (1,2), (1,3), (2,2), (2,4), (2,6))

scala> val out= mapRdd.groupByKey
out: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[8] at groupByKey at <console>:33


scala> out.collect
res10: Array[(String, Iterable[String])] = Array((1,CompactBuffer(2, 2, 3)), (2,CompactBuffer(2, 4, 6)))

scala> val out= mapRdd.groupByKey.map(e=>(e._1,(e._2.mkString(","))))
out: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at map at <console>:33


scala> out.collect
res12: Array[(String, String)] = Array((1,2,2,3), (2,2,4,6))



class- 8-Aug-19
-----------------------
src-resources-sales.csv
transactionid, Customerid,itemid,price

Discount:--------
itemwise discount(10%), amount wise discount(>5000) 
--------------------
Data:-
111,1,333,100.0
112,2,222,505.0
113,3,444,510.0
114,4,333,600.0
115,1,222,510.0
116,1,666,520.0
------------------------

Data validation(parsing/data cleansing whether all 4 columns are avilable or not) Data validation is mandtory section.
(for sqoop also same)
data is semistructured ==> convert it to structured data (give schema to raw data  as always we need particular column repeatedly.easy to remember.if u r not givingschema
then t._1 like this we have use.

For good data assign schema (using case class data model impsoing structure in form of case class which is available in form
of csv file.)

1)itemwise discount(10%)
*************=>***********
1-Read the file= RDD of String (sc.textfile())
2-Map transformation (splitby ',') (will be key value pairs)
3-Map(apply discount 10% on each item 3rd column)
4-reduceByKey

org.training.spark
Discount->amountwise
serialization- salesrecord
2) amount wise disount(>5000 give 10% discount)
********************************
1-Read the file= RDD of String (sc.textfile())
2-Map transformation (splitby ',') (will be key value pairs)
3-reduceByKey
4-map transformation(if >5000 give 10%,else  as is)

Itemwise discount without exception:
----------------------------------------------
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Satyabrat on 8/8/2019. Without Exception Handling
  */
object ItemwiseDiscount {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setMaster("local").setAppName("Itemwise Discount")
    val sc= new SparkContext(conf)
    val inputRdd=sc.textFile("C:\\Users\\Satyabrat\\Desktop\\input\\sales.txt") //file loaded in single array
    val splitRdd=inputRdd.map(e=>e.split(","))

    val mapRdd=splitRdd.map(e =>(e(1),e(3).toDouble))  //once map applied it will be nested array,extract using array method not tuple
    val totalAmount=mapRdd.reduceByKey(_+_)
    val out=totalAmount.map(e=>{
       val id=e._1
       val amt=e._2
       if(amt>1000){
        val disamt=amt-(amt*0.1)
        (id,disamt)
        } else
         (id,amt)
    })

  out.collect().foreach(println)


  }

}
----------------------------------------
Parser for good data and bad data
------------------------------  
/**
 * Created by Arjun on 20/1/15. Parser
 */
object SalesRecordParser {

  def parse(record:String): Either[MalformedRecordException,SalesRecord] = { // Eiher[Left Type,Right Type
    val columns = record.toString.split(",")
    if (columns.length == 4) {
      val transactionId: String = columns(0)
      val customerId: String = columns(1)
      val itemId: String = columns(2)
      val itemValue: Double = columns(3).toDouble
      val s = SalesRecord(transactionId, customerId, itemId, itemValue) // creating object for case class
      val r = Right(s)  //success scenario 
      r
    }
    else {
      Left(new MalformedRecordException())  //failure scenario our custom exception    
  }

}
}
------------------------------		
package org.training.spark.apiexamples.serialization

/**
 * Created by Arjun on 20/1/15.
 */
class MalformedRecordException extends Exception{
}
---------------------------------
case class(for schema)
************************
package org.training.spark.apiexamples.serialization

/**
 * Created by Arjun on 20/1/15.
 */
case class SalesRecord(val transactionId: String,
                  val customerId: String,
                  val itemId: String,
                  val itemValue: Double)
{

/*
  override def toString: String = {
    transactionId+","+customerId+","+itemId+","+itemValue
  } */
}
--------------------------------------------

Amountwise discount
--------------------------------
package org.training.spark.apiexamples.discount

import org.training.spark.apiexamples.serialization.{SalesRecord, SalesRecordParser}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._


object AmountWiseDiscount {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("apiexamples").setMaster("local")
    val sc = new SparkContext(conf)
    val dataRDD = sc.textFile(args(0)) //give input path thru command line argments

    val salesRecordRDD = dataRDD.map(row => {
      val parseResult = SalesRecordParser.parse(row)
      parseResult.right.get //getting actual  success data i.e good records.if bad records our job will fail as we have not handeled left case
    })

    val totalAmountByCustomer = salesRecordRDD.map(record => (record.customerId, record.itemValue))
                                              .reduceByKey(_ + _) // making k,v pair in map which can be used in reduceByKey

   // (custId, TotalAmount)
    val discountAmountByCustomer = totalAmountByCustomer.map(record => {
            val custId = record._1
            val totalAmt = record._2
          if(totalAmt > 1600) {
            val afterDiscount = totalAmt - (totalAmt * 10) / 100.0
            (custId, afterDiscount)
          }
          else
            (custId, totalAmt)

   })
    /*val discountAmountByCustomer = totalAmountByCustomer.map {
      case (customerId, totalAmount) => {
        if (totalAmount > 1600) {
          val afterDiscount = totalAmount - (totalAmount * 10) / 100.0
          (customerId, afterDiscount)
        }
        else (customerId, totalAmount)
      }
    }*/

    discountAmountByCustomer.collect().foreach(println)


  }

}
--------------------------------------------

itemwise discount
------------------------
package org.training.spark.apiexamples.discount

import org.training.spark.apiexamples.serialization.{SalesRecord, SalesRecordParser}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._


object ItemWiseDiscount {
//
//
  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster(args(0)).setAppName("apiexamples")
//
//    //following 2 lines are required for spark history server. All DAGs are shown in UI
//    //conf.set("spark.eventLog.enabled", "true")
//    //conf.set("spark.eventLog.dir", "/tmp/spark-events")
    val sc = new SparkContext(conf)
  val dataRDD = sc.textFile(args(1))
//    //val dataRDD = sc.textFile("hdfs://localhost:9000/user/hduser/data/sales/sales.csv")
    val salesRecordRDD = dataRDD.map(record => {
      val parseResult = SalesRecordParser.parse(record)
          parseResult.right.get //actual answer embeded in right i.e salesrcord
    })
//   //salesRecordRDD.sortBy(- _.itemValue)

//    //apply discount for each book from a customer

//rdd is of salesrecord type with 4 members
    val itemDiscountRDD = salesRecordRDD.map(sales => {
      val itemValue = sales.itemValue
      val newItemValue = itemValue - (itemValue * 5) / 100.0
      (sales.customerId, newItemValue) //to apply reduceByKey, so in form of K,v pair
    })


    val totalAmountByCustomer = itemDiscountRDD.reduceByKey(_+_)//.sortBy(- _._2)
//    //val totalAmountByCustomer = itemDiscountRDD.map(row => (row.customerId,row.itemValue)).reduceByKey(_+_)
   totalAmountByCustomer.collect().foreach(println)

 }

}
-------------------------------  

Q- item price>1000 give 10% discount:
*********************************************
scala> val l=List((111,1,333,100.0),(112,2,222,505.0),(113,1,345,10000.0),(112,2,345,15000.0),(116,3,347,1000.0))
l: List[(Int, Int, Int, Double)] = List((111,1,333,100.0), (112,2,222,505.0), (113,1,345,10000.0), (112,2,345,15000.0), (116,3,347,1000.0))

scala> val Rdd=sc.makeRDD(l)
Rdd: org.apache.spark.rdd.RDD[(Int, Int, Int, Double)] = ParallelCollectionRDD[2] at makeRDD at <console>:29

scala> val MRdd=Rdd.map(e=>(e._2,e._3))
MRdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[3] at map at <console>:31

scala> MRdd.collect
res2: Array[(Int, Int)] = Array((1,333), (2,222), (1,345), (2,345), (3,347))

scala> val MRdd=Rdd.map(e=>(e._2,e._4))
MRdd: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[4] at map at <console>:31

scala> MRdd.collect
res3: Array[(Int, Double)] = Array((1,100.0), (2,505.0), (1,10000.0), (2,15000.0), (3,1000.0))

scala> val total=MRdd.reduceByKey(_+_)
total: org.apache.spark.rdd.RDD[(Int, Double)] = ShuffledRDD[5] at reduceByKey at <console>:33

scala> total.collect
res4: Array[(Int, Double)] = Array((3,1000.0), (1,10100.0), (2,15505.0))


scala> val out=total.map(e=>{
     | val id=e._1
     | val amt=e._2
     | if(amt>1000){
     | val disamt=amt-(amt*0.1)
     | (id,disamt)
     | } else
     | (id,amt)}
     | )
out: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[6] at map at <console>:35

scala> out.collect
res5: Array[(Int, Double)] = Array((3,1000.0), (1,9090.0), (2,13954.5))

how to extract record from nested array RDD: (examples)
---------------------------------
scala> inpo.collect
res17: Array[Array[String]] = Array(Array(111, 1, 333, 100.0), Array(112, 2, 222, 505.0), Array(113, 3, 444, 510.0), Array(114, 4, 333, 600.0), Array(115, 1, 222, 510.0), Array(116, 1, 666, 520.0), Array(117, 1, 444, 540.0), Array(118, 1, 666, 4400.0), Array(119, 3, 333, 3300.0), Array(120, 1, 666, 1500.0), Array(121, 1, 222, 2500.0), Array(122, 3, 444, 4500.0), Array(123, 1, 333, 1100.0), Array(124, 3, 222, 5100.0))


scala> val kv=inpo.map(e=>(e(1),e(3)))
kv: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[14] at map at <console>:31

scala> kv.collect
res18: Array[(String, String)] = Array((1,100.0), (2,505.0), (3,510.0), (4,600.0), (1,510.0), (1,520.0), (1,540.0), (1,4400.0), (3,3300.0), (1,1500.0), (1,2500.0), (3,4500.0), (1,1100.0), (3,5100.0))

scala> val kv=inpo.map(e=>(e(1),e(3)))
kv: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[18] at map at <console>:31

scala> kv.collect
res22: Array[(String, String)] = Array((1,100.0), (2,505.0), (3,510.0), (4,600.0), (1,510.0), (1,520.0), (1,540.0), (1,4400.0), (3,3300.0), (1,1500.0), (1,2500.0), (3,4500.0), (1,1100.0), (3,5100.0))

scala> val r=kv.reduceByKey(_+_)
r: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[19] at reduceByKey at <console>:33


scala> r.collect
res24: Array[(String, String)] = Array((4,600.0), (2,505.0), (3,510.03300.04500.05100.0), (1,100.0510.0520.0540.04400.01500.02500.01100.0))

scala> val kv=inpo.map(e=>(e(1),e(3).toDouble))
kv: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[20] at map at <console>:31

scala> kv.collect
res25: Array[(String, Double)] = Array((1,100.0), (2,505.0), (3,510.0), (4,600.0), (1,510.0), (1,520.0), (1,540.0), (1,4400.0), (3,3300.0), (1,1500.0), (1,2500.0), (3,4500.0), (1,1100.0), (3,5100.0))

scala> val r=kv.reduceByKey(_+_)
r: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[21] at reduceByKey at <console>:33

scala> r.collect
res26: Array[(String, Double)] = Array((4,600.0), (2,505.0), (3,13410.0), (1,11170.0))

clas:10-Aug19
----------------
join: (join sales.csv & customers.csv get the customer id and total amount )
***********

/home/cloudera/projects/spark-core/src/main/resources/sales.csv

/home/cloudera/projects/spark-core/src/main/resources/customers.csv

 val salesRdd=sc.textFile("file:///home/cloudera/projects/spark-core/src/main/resources/sales.csv")
 val customersRdd=sc.textFile("file:///home/cloudera/projects/spark-core/src/main/resources/customers.csv")

scala> val salesMapRdd=salesRdd.map(e=>e.split(","))
salesMapRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at map at <console>:29

scala> salesMapRdd.collect
res4: Array[Array[String]] = Array(Array(111, 1, 333, 100.0), Array(112, 2, 222, 505.0), Array(113, 3, 444, 510.0), Array(114, 4, 333, 600.0), Array(115, 1, 222, 510.0), Array(116, 1, 666, 520.0), Array(117, 1, 444, 540.0), Array(118, 1, 666, 4400.0), Array(119, 3, 333, 3300.0), Array(120, 1, 666, 1500.0), Array(121, 1, 222, 2500.0), Array(122, 3, 444, 4500.0), Array(123, 1, 333, 1100.0), Array(124, 3, 222, 5100.0))

scala> val salesMapRdd1=salesMapRdd.map(e=>(e(1),e(3).toDouble))
salesMapRdd1: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[8] at map at <console>:31

scala> salesMapRdd1.collect
res6: Array[(String, Double)] = Array((1,100.0), (2,505.0), (3,510.0), (4,600.0), (1,510.0), (1,520.0), (1,540.0), (1,4400.0), (3,3300.0), (1,1500.0), (1,2500.0), (3,4500.0), (1,1100.0), (3,5100.0))

scala> val customersMapRdd=customersRdd.map(e=>e.split(","))
customersMapRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at map at <console>:29

scala> customersMapRdd.collect
res7: Array[Array[String]] = Array(Array(1, John), Array(2, Clerk), Array(3, Micheal), Array(4, Sample))

scala> val customersMapRdd1=customersMapRdd.map(e=>(e(0),e(1)))
customersMapRdd1: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[10] at map at <console>:31

scala> customersMapRdd1.collect
res8: Array[(String, String)] = Array((1,John), (2,Clerk), (3,Micheal), (4,Sample))
scala> val reduceRdd=salesMapRdd1.reduceByKey(_+_)
reduceRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[11] at reduceByKey at <console>:33

scala> reduceRdd.collect
res9: Array[(String, Double)] = Array((4,600.0), (2,505.0), (3,13410.0), (1,11170.0))

scala> val joinRdd=reduceRdd.join(customersMapRdd1)
joinRdd: org.apache.spark.rdd.RDD[(String, (Double, String))] = MapPartitionsRDD[14] at join at <console>:41

scala> joinRdd.collect
res11: Array[(String, (Double, String))] = Array((4,(600.0,Sample)), (2,(505.0,Clerk)), (3,(13410.0,Micheal)), (1,(11170.0,John)))

scala> val output=out.map(e=>e._2)
output: org.apache.spark.rdd.RDD[(Double, String)] = MapPartitionsRDD[16] at map at <console>:45

scala> output.collect
res15: Array[(Double, String)] = Array((600.0,Sample), (505.0,Clerk), (13410.0,Micheal), (11170.0,John))

scala> output.getNumPartitions
res16: Int = 2

scala> salesMapRdd1.getNumPartitions
res17: Int = 2

scala> customersMapRdd1.getNumPartitions
res18: Int = 2

scala> out.getNumPartitions
res19: Int = 2



--------------------------------------------------
spark submit-----------
spark-submit [options] <app jar | python file> [app arguments]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally ("client") or
                              on one of the worker machines inside the cluster ("cluster")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of local jars to include on the driver
                              and executor classpaths.
------------------------------------------------------
Submit SPark jobs:
*******************************
We have to submit spark jobs where build.sbt is available. From root folder we can't submit.
build.sbt is available in (/home/cloudera/projects/spark-core) location. 

To create/build jar file:----------------
sbt package (for thin jar :creating jar file without dependencies)  
sbt assembly(for fat jar:combination of jar file and dependencies)

jar file will be created under /home/cloudera/projects/spark-core/target/scala.2.10 with name :

spark-core_2.10-0.1.jar {same how did in sbt 1)groupId 2)ArtifactId 3)version}

to clean the jar file -> sbt clean

for spark submit mandatory fields:

1-master
2-class  (to specify which class want to be executed from the jar file as jar file has all class i.e whole project)  
3-jar file name (absolute path)
4- input file/folder path (HDFS path)(if only one file available in folder can give folder, if more than 1 file available give file name
5-deploy mode is optional  as bydefault client mode, if u want to submit in cluster mention that.
 
command to submit spark job:
-----------------------------------
 1) cluster submit: (Driver will be in Application master,println statements in driver can be seen in Application master log)
 //in cluster mode we can see executors,in local mode no executors as single driver will act as everything//
 
spark-submit --master yarn  --deploy-mode cluster --class org.training.spark.apiexamples.discount.AmountWiseDiscount /home/cloudera/projects/spark-core/target/scala-2.10/spark-core_2.10-0.1.jar /spark-input/sales.csv
 2) client submit: (Driver will be in edgenode,println statements of driver can be seen in console)

spark-submit --master yarn --class org.training.spark.apiexamples.discount.AmountWiseDiscount /home/cloudera/projects/spark-core/target/scala-2.10/spark-core_2.10-0.1.jar /spark-input/sales.csv

To kill application 
--------------------------
client mode: just quit from console/edge node

cluster mode:
yarn application -kill application-id
----------------------------- ---
8080-default name node port
8088- default Resource manager (if u submit spark jobs better check in 8088 port as there will be confusion for 4040,4041... )
4040-default spark jobs(if 4040 is busy  check in 4041,4042...)
----------------------------------------
create input files in hdfs
****************************
[cloudera@quickstart spark-core]$ hadoop fs -mkdir /spark-input
[cloudera@quickstart spark-core]$ hadoop fs -put /home/cloudera/projects/spark-core/src/main/resources/sales.csv /spark-input

.sbt files and jar files
***********************************
[cloudera@quickstart spark-core]$ sbt package  // to create thin Jar file (spark-core_2.10-0.1.jar)  
//once jar is created if u want to recreate 1st clean that jar file using [sbt clean] and rebuild the jar

[cloudera@quickstart spark-core]$ ls
build.sbt  project  src  target

[cloudera@quickstart scala-2.10]$ pwd
/home/cloudera/projects/spark-core/target/scala-2.10

[cloudera@quickstart target]$ ls scala-2.10 
classes  resolution-cache  spark-core_2.10-0.1.jar


[cloudera@quickstart target]$ spark-submit --master yarn --class org.training.spark.apiexamples.discount.AmountWiseDiscount /home/cloudera/projects/spark-core/target/scala-2.10/spark-core_2.10-0.1.jar /spark-input/sales.csv

class 11-Aug-19
************************
broadcast join (mapside join, using distributed cache)
--------------------------------------------
scala> salesRdd.foreach(println)
111,1,333,100.0
112,2,222,505.0
113,3,444,510.0
114,4,333,600.0
115,1,222,510.0
116,1,666,520.0
117,1,444,540.0
118,1,666,4400.0
119,3,333,3300.0
120,1,666,1500.0
121,1,222,2500.0
122,3,444,4500.0
123,1,333,1100.0
124,3,222,5100.0

scala> customersRdd.foreach(println)
4,Sample
1,John
2,Clerk
3,Micheal

//converting small d.s to Map collection ,s o that it can be broadcasted
scala> val customersMap=customersRdd.map(e=>{val row=e.split(",")
     | (row(0),row(1))}).collect.toMap
customersMap: scala.collection.immutable.Map[String,String] = Map(1 -> John, 2 -> Clerk, 3 -> Micheal, 4 -> Sample)	

scala> val broadCastVar=sc.broadcast(customersMap)
broadCastVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,String]] = Broadcast(32)

scala> val out= salesRdd.map(x=>{
     | val row=x.split(",")
     | val transId=row(0)
     | val custId=row(1)
     | val itemId=row(2)
     | val price= row(3).toDouble
     |  val bcVariable=broadCastVar.value
     | val customerName=bcVariable.getOrElse(custId,"unknown data")
     | (customerName,custId,itemId,price)})
out: org.apache.spark.rdd.RDD[(String, String, String, Double)] = MapPartitionsRDD[12] at map at <console>:35

scala> out.collect
res18: Array[(String, String, String, Double)] = Array((John,1,333,100.0), (Clerk,2,222,505.0), (Micheal,3,444,510.0), (Sample,4,333,600.0), (John,1,222,510.0), (John,1,666,520.0), (John,1,444,540.0), (John,1,666,4400.0), (Micheal,3,333,3300.0), (John,1,666,1500.0), (John,1,222,2500.0), (Micheal,3,444,4500.0), (John,1,333,1100.0), (Micheal,3,222,5100.0))

scala> out.foreach(println)
(John,1,333,100.0)
(Micheal,3,333,3300.0)
(Clerk,2,222,505.0)
(John,1,666,1500.0)
(Micheal,3,444,510.0)
(John,1,222,2500.0)
(Sample,4,333,600.0)
(Micheal,3,444,4500.0)
(John,1,222,510.0)
(John,1,333,1100.0)
(John,1,666,520.0)
(Micheal,3,222,5100.0)
(John,1,444,540.0)
(John,1,666,4400.0)
--------------------------------
in intelij program*******************

package org.training.spark.apiexamples.joins

import java.io.{File, FileReader, BufferedReader}

import org.training.spark.apiexamples.serialization.SalesRecordParser
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._

import scala.collection.mutable


object BroadcastBased {


  /*def creatCustomerMap(customerDataPath: String) = {

    val customerMap = mutable.Map[String, String]()

    val lines = scala.io.Source.fromFile(customerDataPath).getLines()
    while (lines.hasNext) {
      val values = lines.next().split(",")
    //for(line <- lines) {
      //val values = line.split(",")
      customerMap.put(values(0), values(1))
    }
    customerMap
  }
*/

  def main(args: Array[String]) {

    val conf = new SparkConf().setMaster(args(0)).setAppName("apiexamples")
    val sc = new SparkContext(conf)
    val salesRDD = sc.textFile(args(1))
    val customerDataPath = args(2)

    val customerMap=scala.io.Source.fromFile(customerDataPath).getLines()
      .map(x=>{val fields= x.split(",")
        (fields(0),fields(1))}).toMap

   /* val customerMap=sc.textFile(customerDataPath)
      .map(x=>{val fields= x.split(",")
        (fields(0),fields(1))}).collect.toMap
      */

    //val customerMap = creatCustomerMap(customerDataPath)

    //broadcast data

    val customerBroadCast = sc.broadcast(customerMap)
`
    val joinRDD = salesRDD.map(rec => {
      val salesRecord = SalesRecordParser.parse(rec).right.get
      val customerId = salesRecord.customerId
      val custMap = customerBroadCast.value
    /*  val customerName = custMap.get(customerId) match {
        case None => "Unknonw User"
        case Some(custName) => custName
      }
      (customerName,salesRecord)*/

      val customerName=custMap.getOrElse(customerId,"unknown")
	  (customerName,salesRecord)
    })

    joinRDD.collect().foreach(println)

    joinRDD.saveAsTextFile("src/main/resources/join_output1")

    //.map(x=>(s"${x._1},${x._2},${x._3},${x._4}")).saveAsTextFile("src/main/resources/join_output1")
  }

}
-----------------------------------------------
shuffle join:
---------------------
scala> val salesRdd= sc.textFile("file:///home/cloudera/Desktop/data/sales.csv")
salesRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/sales.csv MapPartitionsRDD[18] at textFile at <console>:27

scala> val customersRdd= sc.textFile("file:///home/cloudera/Desktop/data/customers.csv")
customersRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/customers.csv MapPartitionsRDD[20] at textFile at <console>:27

scala> salesRdd.collect
res15: Array[String] = Array(111,1,333,100.0, 112,2,222,505.0, 113,3,444,510.0, 114,4,333,600.0, 115,1,222,510.0, 116,1,666,520.0, 117,1,444,540.0, 118,1,666,4400.0, 119,3,333,3300.0, 120,1,666,1500.0, 121,1,222,2500.0, 122,3,444,4500.0, 123,1,333,1100.0, 124,3,222,5100.0)

scala> salesRdd.foreach(println)
111,1,333,100.0
112,2,222,505.0
113,3,444,510.0
114,4,333,600.0
115,1,222,510.0
119,3,333,3300.0
120,1,666,1500.0
121,1,222,2500.0
122,3,444,4500.0
123,1,333,1100.0
116,1,666,520.0
117,1,444,540.0
118,1,666,4400.0
124,3,222,5100.0

scala> customersRdd.foreach(println)
4,Sample
1,John
2,Clerk
3,Micheal

                                              

scala> val mapRdd=salesRdd.map(e=>{val row=e.split(",")
     | (row(1),row(3))}
     | )
mapRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[21] at map at <console>:29

scala> mapRdd.collect
res18: Array[(String, String)] = Array((1,100.0), (2,505.0), (3,510.0), (4,600.0), (1,510.0), (1,520.0), (1,540.0), (1,4400.0), (3,3300.0), (1,1500.0), (1,2500.0), (3,4500.0), (1,1100.0), (3,5100.0))

scala> val mapRdd=salesRdd.map(e=>{val row=e.split(",")
     | (row(1),row(3).toDouble)}).reduceByKey(_+_)
mapRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[23] at reduceByKey at <console>:30

scala> mapRdd.collect
res19: Array[(String, Double)] = Array((4,600.0), (2,505.0), (3,13410.0), (1,11170.0))

 
scala> val customersMapRdd=customersRdd.map(e=>{val row=e.split(",")
     | (row(0),row(1))})
customersMapRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[24] at map at <console>:29

scala> customersMapRdd.collect
res20: Array[(String, String)] = Array((1,John), (2,Clerk), (3,Micheal), (4,Sample))

scala> val joinRdd=customersMapRdd.join(mapRdd)
joinRdd: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[27] at join at <console>:35

scala> joinRdd.getNumPartitions
res21: Int = 2

scala> joinRdd.toDebugString
res22: String = 
(2) MapPartitionsRDD[27] at join at <console>:35 []
 |  MapPartitionsRDD[26] at join at <console>:35 []
 |  CoGroupedRDD[25] at join at <console>:35 []
 +-(2) MapPartitionsRDD[24] at map at <console>:29 []
 |  |  file:///home/cloudera/Desktop/data/customers.csv MapPartitionsRDD[20] at textFile at <console>:27 []
 |  |  file:///home/cloudera/Desktop/data/customers.csv HadoopRDD[19] at textFile at <console>:27 []
 |  ShuffledRDD[23] at reduceByKey at <console>:30 []
 +-(2) MapPartitionsRDD[22] at map at <console>:29 []
    |  file:///home/cloudera/Desktop/data/sales.csv MapPartitionsRDD[18] at textFile at <console>:27 []
    |  file:///home/cloudera/Desktop/data/sales.csv HadoopRDD[17] at textFile at <console>:27 []

scala> joinRdd.foreach(println)
(3,(Micheal,13410.0))
(4,(Sample,600.0))
(1,(John,11170.0))
(2,(Clerk,505.0))

scala> val out=joinRdd.map(e=>(e._1,e._2._1,e._2._2))
out: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[28] at map at <console>:37

scala> out.collect
res24: Array[(String, String, Double)] = Array((4,Sample,600.0), (2,Clerk,505.0), (3,Micheal,13410.0), (1,John,11170.0))

scala> out.foreach(println)
(4,Sample,600.0)
(2,Clerk,505.0)
(3,Micheal,13410.0)
(1,John,11170.0)


scala> val output=out.map(e=>(e._1,e._2,e._3))
output: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[29] at map at <console>:39

scala> output.foreach(println)
(3,Micheal,13410.0)
(1,John,11170.0)
(4,Sample,600.0)
(2,Clerk,505.0)


scala> val output=out.map(e=>(s"${e._1},${e._2}"))
output: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[30] at map at <console>:39

scala> output.foreach(println)
4,Sample
2,Clerk
3,Micheal
1,John



shuffled join
********************
package org.training.spark.apiexamples.joins

import org.training.spark.apiexamples.serialization.SalesRecordParser
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._


object ShuffleBased {

  def main(args: Array[String]) {
    val conf = new SparkConf().setMaster(args(0)).setAppName("apiexamples")
    val sc = new SparkContext(conf)

    val salesRDD = sc.textFile(args(1),3)
    val customerRDD = sc.textFile(args(2),2)

    val salesPair = salesRDD.map(row => {
      val salesRecord = SalesRecordParser.parse(row).right.get
      (salesRecord.customerId,salesRecord)
    })

    val customerPair = customerRDD.map(row => {
      val columnValues = row.split(",")
      (columnValues(0),columnValues(1))
    })


    val joinRDD = customerPair.join(salesPair)

       val result = joinRDD.map{
      case (customerId,(customerName,salesRecord)) => {
        (customerName,salesRecord.itemId)
      }
    }

    /*val result = joinRDD.map{record => {

        val customerId = record._1
        val customerName = record._2._1
        val itemId = record._2._2.itemId
        (customerName,itemId)
      }
    }*/

    println(result.collect().toList) // Not recommended to use colllect after join....

    val joinRDD1 = salesPair.join(customerPair)

    val result1 = joinRDD1.map{
      case (customerId,(salesRecord,customerName)) => {
        (customerName,salesRecord.itemId)
      }
    }

    println(result1.collect().toList)

    Thread.sleep(100000)

  }


}
---------------------------------------
error handling:-
counters:
-----------
package org.training.spark.apiexamples.errorhandling

import org.training.spark.apiexamples.serialization.SalesRecordParser
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext._


object Counters {
  def main(args: Array[String]) {

    val conf = new SparkConf().setMaster(args(0)).setAppName("apiexamples")
    val sc = new SparkContext(conf)
    //sc.setLogLevel("ERROR")
    val dataRDD = sc.textFile(args(1),2)
    val malformedRecords = sc.accumulator(0)


     //println("Partitions: "  + dataRDD.partitions.length)
    // foreach is an action but runs at executor side

    dataRDD.foreach(record => {
      val parseResult = SalesRecordParser.parse(record)
      if(parseResult.isLeft){
        malformedRecords += 1
      }
    })

    println("No of malformed records is =  " + malformedRecords.value)

    /*val test = dataRDD.map(record => {
      val parseResult = SalesRecordParser.parse(record)
      if(parseResult.isLeft){
        malformedRecords += 1
      }
    })
    println(test.collect.toList)*/

    //dataRDD.foreach(println(_))
    //print the counter
    //println("No of malformed records is =  " + malformedRecords.value)

    //Thread.sleep(50000)

  }

}
----------------------
countersTest
-----------------
package org.training.spark.apiexamples.errorhandling

import org.apache.spark.{SparkConf, SparkContext}
import org.training.spark.apiexamples.serialization.SalesRecordParser

object CountersTest{

  def main(args: Array[String]): Unit = {


    val conf = new SparkConf().
      setMaster("local").
      setAppName("counters test")

    val sc = new SparkContext(conf)

    val fileRDD = sc.textFile(args(0))

    val missingFieldRecords = sc.accumulator(0)

    val rdd1 = fileRDD.foreach(rec => {
      val parseResult = SalesRecordParser.parse(rec)
      if (parseResult.isLeft) missingFieldRecords += 1
    })

    //println(rdd1.count)
    println(missingFieldRecords.value)

    //Thread.sleep(100000)
  }
}




--------------------------
error handling:
MalformedRecords
********************
package org.training.spark.apiexamples.errorhandling

import org.apache.spark.storage.StorageLevel
import org.training.spark.apiexamples.serialization.{SalesRecord, SalesRecordParser}
import org.apache.spark.{SparkConf, SparkContext}


object HandleMalformedRecords {

  def main(args: Array[String]) {

    val conf = new SparkConf().setMaster(args(0)).setAppName("apiexamples")
    val sc = new SparkContext(conf)
    val dataRDD = sc.textFile(args(1))

    val parsedRdd = dataRDD.map(record => {
      val parseResult = SalesRecordParser.parse(record)
      if(parseResult.isRight){
        (true,parseResult.right.get)
      }
      else (false,record)
    })

    parsedRdd.persist(StorageLevel.MEMORY_AND_DISK)

    val malformedRecords = parsedRdd.filter(_._1 == false).map(_._2).cache
     val normalRecords = parsedRdd.filter(_._1 == true)
       .map(x => x._2 match {
      case y:SalesRecord => y
    })
    normalRecords


    //val normalRecords1 = parsedRdd.map(_._2).subtract(malformedRecords)
    //val salesRecordRDD = normalRecords.map(row => SalesRecordParser.parse(row).right.get)

    println(malformedRecords.collect().toList)
    println(malformedRecords.count())
    println(normalRecords.collect().toList)

    Thread.sleep(500000)

  }

}
----------------------------------
3)	Given two datasets,
a.	User information (id, email, language, location)
1 email@test.com  EN  US
2 email@test2.com EN  GB
3 email@test3.com FR  FR

b.	Transaction information (transaction-id, product-id, user-id, purchase-amount, item-description)
1 1 1 300 item1
2 1 2 300 item2
3 1 2 300 item3
4 2 3 100 item4
5 1 3 300 item5
Use Spark-core (RDD) to find the number of unique locations in which each product has been sold.
-------------------------------------------------------
each location how many product sold:
--------------------------------------------
scala> val usersRdd=sc.textFile("file:///home/cloudera/Desktop/data/users")
usersRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/users MapPartitionsRDD[14] at textFile at <console>:27

scala> usersRdd.collect
res20: Array[String] = Array(1 email@test.com  EN  US, 2 email@test2.com EN  GB, 3 email@test3.com FR  FR, "")

scala> val transRdd=sc.textFile("file:///home/cloudera/Desktop/data/trans")
transRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/trans MapPartitionsRDD[16] at textFile at <console>:27


scala> val usersMapRdd=usersRdd.map(e=>{val row=e.split(" ")
      (row(0),row(3))})
usersMapRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at map at <console>:29


scala> val transMapRdd=transRdd.map(e=>{val row=e.split(" ")
      (row(),row(4))})
transMapRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[7] at map at <console>:29

scala> transMapRdd.foreach(println)
(1,item1)
(2,item2)
(2,item3)
(3,item4)
(3,item5)

scala> val joinRdd= transMapRdd.leftOuterJoin(usersMapRdd)
joinRdd: org.apache.spark.rdd.RDD[(String, (String, Option[String]))] = MapPartitionsRDD[15] at leftOuterJoin at <console>:35

scala> joinRdd.foreach(println)
(3,(item4,Some(FR)))
(2,(item2,Some(GB)))
(2,(item3,Some(GB)))
(3,(item5,Some(FR)))
(1,(item1,Some(US)))

scala> val out= joinRdd.map(e=>(e._2._2.get,e._2._1)).reduceByKey(_+_)
out: org.apache.spark.rdd.RDD[(String, String)] = ShuffledRDD[26] at reduceByKey at <console>:37

scala> out.foreach(println)
(GB,item2item3)
(FR,item4item5)
(US,item1)
------------------------------------------------------
4)	Consider the file – social_friends.csv and below is the description of columns in the file,
Column 1: User ID
Column 2: User Name
Column 3: Age of the User
Column 4: Number of Friends with that User

Write Spark-core (RDD) code to calculate the average number of friends based on their age.
------------------------------------------------------------

scala> val socialRdd=sc.textFile("file:///home/cloudera/Desktop/data/social_friends.csv")
socialRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/social_friends.csv MapPartitionsRDD[28] at textFile at <console>:27

scala> socialRdd.collect
res13: Array[String] = Array(0,Will,33,385, 1,Jean-Luc,26,2, 2,Hugh,55,221, 3,Deanna,40,465, 4,Quark,68,21, 5,Weyoun,59,318, 6,Gowron,37,220, 7,Will,54,307, 8,Jadzia,38,380, 9,Hugh,27,181, 10,Odo,53,191, 11,Ben,57,372, 12,Keiko,54,253, 13,Jean-Luc,56,444, 14,Hugh,43,49, 15,Rom,36,49, 16,Weyoun,27,323, 17,Odo,35,13, 18,Jean-Luc,37,455, 19,Geordi,60,246, 20,Odo,67,220, 21,Miles,19,268, 22,Quark,40,72, 23,Keiko,51,271)

scala> socialRdd.take(3).foreach(println)
0,Will,33,385
1,Jean-Luc,26,2
2,Hugh,55,221

scala> val socialMapRdd=socialRdd.map(e=>{
     | val row=e.split(",")
     | (row(2).toInt,row(3).toInt)})
socialMapRdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[44] at map at <console>:29



scala> socialMapRdd.take(5).foreach(println)
(33,385)
(26,2)
(55,221)
(40,465)
(68,21)

scala> val groupRdd=socialMapRdd.groupByKey()
out: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[30] at groupByKey at <console>:31

scala> groupRdd.take(3).foreach(println)
(56,CompactBuffer(444))
(54,CompactBuffer(307, 253))
(36,CompactBuffer(49))

scala> val out=groupRdd.map(e=>{
     | val len=(e._2).size
     | val avg=(e._2).sum/len
     | (e._1,avg)})
out: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[6] at map at <console>:33

scala> out.take(3).foreach(println)
(56,444)
(54,280)
(36,49)


converting any type of list toInt:
--------------------------------
scala> val l=List("1","2")
l: List[String] = List(1, 2)

scala> val l1=l.map(e=>e.toString.toInt)
l1: List[Int] = List(1, 2)


class- 19-Aug-19
**********************

repartition  vs coalesce  (coalesce used for decrease  partitions , repartition for both increasase and decrease, but mainly for increase)
-------------------------------

scala> val rdd= sc.parallelize(List.range(1,11),4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> rdd.glom.collect
res0: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4, 5), Array(6, 7), Array(8, 9, 10))

scala> val colaescedRdd=rdd.coalesce(2)
colaescedRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[2] at coalesce at <console>:29

scala> val colaescedRdd=rdd.coalesce(1)
colaescedRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[3] at coalesce at <console>:29

scala> val colaescedRdd=rdd.coalesce(2)
colaescedRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[4] at coalesce at <console>:29



scala> rdd.getNumPartitions
res2: Int = 4

scala> colaescedRdd.getNumPartitions
res3: Int = 2

scala> colaescedRdd.glom
res4: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[5] at glom at <console>:32

scala> colaescedRdd.glom.collect
res5: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5), Array(6, 7, 8, 9, 10))

scala> val colaescedRdd=rdd.coalesce(1)
colaescedRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[7] at coalesce at <console>:29

scala> colaescedRdd.getNumPartitions
res6: Int = 1

scala> val colaescedRdd=rdd.coalesce(2)
colaescedRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[8] at coalesce at <console>:29

scala> colaescedRdd.glom.collect
res7: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5), Array(6, 7, 8, 9, 10))

scala> val repartitionRdd=rdd.repartition(2)
repartitionRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at repartition at <console>:29


scala> repartitionRdd.glom.collect
res11: Array[Array[Int]] = Array(Array(1, 3, 5, 6, 8, 10), Array(2, 4, 7, 9))

scala> repartitionRdd.getNumPartitions
res12: Int = 2

     

scala> val coalescedRdd1=rdd.coalesce(6)
coalescedRdd1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[15] at coalesce at <console>:29


scala> coalescedRdd1.glom.collect
res15: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4, 5), Array(6, 7), Array(8, 9, 10))



scala> val repartitionRdd1=rdd.repartition(6)
repartitionRdd1: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[20] at repartition at <console>:29

scala> repartitionRdd1.getNumPartitions
res16: Int = 6

scala> repartitionRdd1.glom.collect
res17: Array[Array[Int]] = Array(Array(5, 7), Array(1), Array(2), Array(8), Array(3, 9), Array(4, 6, 10))



Movies assignment:
----------------------
File Name    Description / Schema
movies.dat   MovieID – Title – Genres
ratings.dat  UserID – MovieID – Rating – Timestamp
users.dat    UserID – Gender – Age – Occupation – ZipCode

Q-Top ten most viewed movies with their movies Name (Ascending or Descending order)
-----------------------------
steps:
-------
1- load both movies and ratings file
2-do equi join on movie id (ratings and movies )
3- groupBy movieid 
4- get the userview count and movie name
----------------------------
scala> val moviesRdd=sc.textFile("file:///home/cloudera/Desktop/data/movies.txt",10)


scala> val ratingsRdd=sc.textFile("file:///home/cloudera/Desktop/data/ratings.txt",10)

scala> val moviesMap=moviesRdd.map(e=>{
     | val row=e.split("::")
     | (row(0),row(1))})
moviesMap: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[4] at map at <console>:29



scala> val ratingsMap=ratingsRdd.map(e=>{
     | val row=e.split("::")
     | (row(1),row(0))})
ratingsMap: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[19] at map at <console>:29


scala> val joinRdd=ratingsMap.join(moviesMap)
joinRdd: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[22] at leftOuterJoin at <console>:35



scala> val groupRdd=joinRdd.map(e=>(e._1,e._2)).groupByKey().map(e=>(e._2.size,e._2.toList(0)._2))
groupRdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[36] at map at <console>:37


scala> groupRdd.top(2)foreach(println)
(2,Apollo 13 (1995))
(1,Toy Story (1995))

-----------------------------
File Name    Description / Schema
movies.dat   MovieID – Title – Genres
ratings.dat  UserID – MovieID – Rating – Timestamp
users.dat    UserID – Gender – Age – Occupation – ZipCode

Top twenty rated movies (Condition: The movie should be rated/viewed by at least 40 users)
------------------------------------
scala> val moviesRdd=sc.textFile("file:///home/cloudera/Desktop/data/movies.txt",10)


scala> val ratingsRdd=sc.textFile("file:///home/cloudera/Desktop/data/ratings.txt",10)

scala> val moviesMap=moviesRdd.map(e=>{
     | val row=e.split("::")
     | (row(0).toInt,row(1))})
moviesMap: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[4] at map at <console>:29



scala> val ratingsMap=ratingsRdd.map(e=>{
     | val row=e.split("::")
     | (row(1).toInt,row(2).toInt)})


scala> val joinRdd=ratingsMap.fullOuterJoin(moviesMap)
joinRdd: org.apache.spark.rdd.RDD[(Int, (Option[Int], Option[String]))] = MapPartitionsRDD[15] at fullOuterJoin at <console>:35

scala> joinRdd.take(2).foreach(println)
(1260,(Some(5),Some(M (1931))))                                                 
(1260,(Some(5),Some(M (1931))))

scala> val filterRdd=joinRdd.filter(t=>(t._2._1!=None))
filterRdd: org.apache.spark.rdd.RDD[(Int, (Option[Int], Option[String]))] = MapPartitionsRDD[27] at filter at <console>:37

scala> val outRdd=filterRdd.map(t=>(t._1,(t._2._1.get,1)))
outRdd: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[19] at map at <console>:37

scala> outRdd.take(2).foreach(println)
(1260,(5,1))
(1260,(5,1))

scala> val reduceRdd=outRdd.reduceByKey((k,v)=>(k._1+v._1,k._2+v._2))
reduceRdd: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[29] at reduceByKey at <console>:41

scala> reduceRdd.take(2).foreach(println)
(1260,(1325,308))                                                               
(1410,(258,90))

scala> val averageRdd=reduceRdd.filter(t=>t._2._2>40).map(t=>(t._1,t._2._1/t._2._2))
averageRdd: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[33] at map at <console>:43

scala> averageRdd.sortBy(e=>(-e._2)).take(20).foreach(println)
(1260,4)
(1080,4)
(1900,4)
(2010,4)
(1280,4)
(1250,4)
(2330,4)
(2360,4)
(3730,4)
(2940,4)
(3000,4)
(1210,4)
(150,4)
(1300,4)
(910,4)
(1200,4)
(3030,4)
(750,4)
(720,4)
(3470,4)

best way: Top 20 movies with name (movie should be rated/viewed more than 40 users)
***************************************************

scala> val moviesRdd=sc.textFile("file:///home/cloudera/Desktop/data/movies.txt",10)
moviesRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/movies.txt MapPartitionsRDD[51] at textFile at <console>:27

scala> val ratingsRdd=sc.textFile("file:///home/cloudera/Desktop/data/ratings.txt",10)
ratingsRdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/Desktop/data/ratings.txt MapPartitionsRDD[53] at textFile at <console>:27



scala> val ratingsMap=ratingsRdd.map(e=>{
     | val row=e.split("::")
     | (row(1).toInt,(row(2).toInt,1))})
ratingsMap: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[56] at map at <console>:29

scala> ratingsMap.take(2).foreach(println)
(1193,(5,1))
(661,(3,1))


scala> val averageRdd=ratingsMap.reduceByKey((k,v)=>(k._1+v._1,k._2+v._2))
averageRdd: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = ShuffledRDD[57] at reduceByKey at <console>:31

scala> averageRdd.take(2).foreach(println)
(1260,(1325,308))                                                               
(1410,(258,90))



scala> val output=averageRdd.filter(t=> t._2._2>40).map(t=>(t._1,(t._2._1/t._2._2)))
output: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[96] at map at <console>:33



scala> val moviesMap=moviesRdd.map(e=>{
     | val row=e.split("::")
     | (row(0).toInt,row(1))})
moviesMap: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[84] at map at <console>:29



scala> val finalOutPut=output.leftOuterJoin(moviesMap)
finalOutPut: org.apache.spark.rdd.RDD[(Int, (Int, Option[String]))] = MapPartitionsRDD[99] at leftOuterJoin at <console>:39


scala> finalOutPut.sortBy(e=>(-e._2._1)).map(e=>(e._1,e._2._1,e._2._2.get)).take(20).foreach(println)

(1260,4,M (1931))
(1080,4,Monty Python's Life of Brian (1979))
(1900,4,Children of Heaven, The (Bacheha-Ye Aseman) (1997))
(2010,4,Metropolis (1926))
(1280,4,Raise the Red Lantern (1991))
(1250,4,Bridge on the River Kwai, The (1957))
(2330,4,Hands on a Hard Body (1996))
(2360,4,Celebration, The (Festen) (1998))
(3730,4,Conversation, The (1974))
(2940,4,Gilda (1946))
(3000,4,Princess Mononoke, The (Mononoke Hime) (1997))
(1210,4,Star Wars: Episode VI - Return of the Jedi (1983))
(150,4,Apollo 13 (1995))
(1300,4,My Life as a Dog (Mitt liv som hund) (1985))
(910,4,Some Like It Hot (1959))
(1200,4,Aliens (1986))
(3030,4,Yojimbo (1961))
(750,4,Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963))
(720,4,Wallace & Gromit: The Best of Aardman Animation (1996))
(3470,4,Dersu Uzala (1974))


no. of movies on rating basis(5 rating howmany movies, 4 rating how many movies........)
***********************************
scala> ratingsMapRdd.take(20).foreach(println)
(5,1193)
(3,661)
(3,914)
(4,3408)
(5,2355)
(3,1197)
(5,1287)
(5,2804)
(4,594)
(4,919)
(5,595)
(4,938)
(4,2398)
(4,2918)
(5,1035)
(4,2791)
(3,2687)
(4,2018)
(5,3105)
(4,2797)

scala> val ratingsMapRdd=ratingsRdd.map(e=>e.split("::")(2))
ratingsMapRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at map at <console>:29

scala> ratingsMapRdd.take(20).foreach(println)
5
3
3
4
5
3
5
5
4
4
5
4
4
4
5
4
3
4
5
4


scala> val ratingsMapRdd=ratingsRdd.map(e=>(e.split("::")(2),1))
ratingsMapRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[59] at map at <console>:29

scala> ratingsMapRdd.take(20).foreach(println)
(5,1)
(3,1)
(3,1)
(4,1)
(5,1)
(3,1)
(5,1)
(5,1)
(4,1)
(4,1)
(5,1)
(4,1)
(4,1)
(4,1)
(5,1)
(4,1)
(3,1)
(4,1)
(5,1)
(4,1)

scala> val out=ratingsMapRdd.reduceByKey(_+_)
out: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[60] at reduceByKey at <console>:31

scala> out.take(10).foreach(println)
(2,107557)                                                                      
(3,261197)
(4,348971)
(5,226310)
(1,56174)

2nd way(using countByValue)  {instead of map + reduceByKey in above step use CountByValue )----------------------
---------------------------------------------------------------------
scala> val ratingsMapRdd=ratingsRdd.map(e=>e.split("::")(2)).countByValue()
ratingsMapRdd: scala.collection.Map[String,Long] = Map(4 -> 348971, 5 -> 226310, 1 -> 56174, 2 -> 107557, 3 -> 261197)


scala> ratingsMapRdd.toSeq.sorted
res41: Seq[(String, Long)] = ArrayBuffer((1,56174), (2,107557), (3,261197), (4,348971), (5,226310))

scala> ratingsMapRdd.toSeq.sorted.foreach(println)
(1,56174)
(2,107557)
(3,261197)
(4,348971)
(5,226310)

scala> ratingsMapRdd.toList.sorted.foreach(println)
(1,56174)
(2,107557)
(3,261197)
(4,348971)
(5,226310)

scala> ratingsMapRdd.toList.sorted.take(3)foreach(println)
(1,56174)
(2,107557)
(3,261197)


--------------------------------------


instead of sortBy and take we can go for top:(will give top results according to 1st element)
*************************************

scala> val groupRdd=joinRdd.map(e=>(e._1,e._2)).groupByKey().map(e=>(e._2.size,e._1))
groupRdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[40] at map at <console>:37

scala> groupRdd.top(4).foreach(println)
(3428,2858)                                                                     
(2991,260)
(2990,1196)
(2883,1210)



class (20-Aug-19)
************************
scala> val rdd=sc.parallelize(List.range(1,11),4)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> rdd.glom.collect
res0: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4, 5), Array(6, 7), Array(8, 9, 10))



scala> rdd.aggregate(0)(_ max _ , _ + _)
res3: Int = 24

scala> rdd.aggregate(10)(_ max _ , _ + _)
res4: Int = 50


get average
****************

scala> val markList= List(("A",20),("B",30),("C",50),("A",25),("B",45),("C",25),("A",40),("B",70),("C",40))
markList: List[(String, Int)] = List((A,20), (B,30), (C,50), (A,25), (B,45), (C,25), (A,40), (B,70), (C,40))

  
scala> val marksRdd= sc.parallelize(markList)
marksRdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[2] at parallelize at <console>:29

scala> val groupRdd=marksRdd.groupByKey()
groupRdd: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[3] at groupByKey at <console>:31

scala> groupRdd.getNumPartitions
res5: Int = 3


scala> val averageRdd=groupRdd.map(e=>{
     | val len=e._2.size
     | val avg=(e._2).sum/len
     | (e._1,avg)})
averageRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:33

scala> averageRdd.collect
res6: Array[(String, Int)] = Array((B,48), (C,38), (A,28))

best way using groupByKey:-----

scala> val averageRdd=groupRdd.mapValues(v=>(1.0*v.sum)/v.size)
averageRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[5] at mapValues at <console>:33

scala> averageRdd.collect
res7: Array[(String, Double)] = Array((B,48.333333333333336), (C,38.333333333333336), (A,28.333333333333332))

using aggregateByKey:(get average)
--------------------------

scala> val aggRdd=marksRdd.aggregateByKey((0,0))((acc,itr)=>(acc._1+itr,acc._2+1),(acc1,itr1)=>(acc1._1+itr1._1,acc1._2+itr1._2))
aggRdd: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[7] at aggregateByKey at <console>:31

scala> aggRdd.foreach(println)
(C,(115,3))
(A,(85,3))
(B,(145,3))

scala> val finalRdd=aggRdd.mapValues(t=>t._1.toDouble/t._2)
finalRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[8] at mapValues at <console>:33

scala> finalRdd.collect
res11: Array[(String, Double)] = Array((B,48.333333333333336), (C,38.333333333333336), (A,28.333333333333332))

scala> finalRdd.foreach(println)
(B,48.333333333333336)
(A,28.333333333333332)
(C,38.333333333333336)


class- 22-Aug-19
**********************
spark SQL
*****************
In spark 1.x version csv file reading was not nativly available, so we have to add dependencies while launching Spark shell.

[cloudera@quickstart ~]$ spark-shell --jars /home/cloudera/.ivy2/cache/com.databricks/spark-csv_2.10/jars/spark-csv_2.10-1.5.0.jar,/home/cloudera/.ivy2/cache/org.apache.commons/commons-csv/jars/commons-csv-1.1.jar

scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@8beb0dd

scala> sqlContext   // as in our VM we have hive-site.xml integrated in conf with Spark-sql
res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@3f6fd33b


Before doing read/write operation in spark-sql start hadoop and hive services

scala> val df=sqlContext.read.format("csv").load("file:///home/cloudera/projects/spark-core/src/main/resources/sales.csv")
df: org.apache.spark.sql.DataFrame = [C0: string, C1: string, C2: string, C3: string]

scala> df.collect
res3: Array[org.apache.spark.sql.Row] = Array([111,1,333,100.0], [112,2,222,505.0], [113,3,444,510.0], [114,4,333,600.0], [115,1,222,510.0], [116,1,666,520.0], [117,1,444,540.0], [118,1,666,4400.0], [119,3,333,3300.0], [120,1,666,1500.0], [121,1,222,2500.0], [122,3,444,4500.0], [123,1,333,1100.0], [124,3,222,5100.0])

scala> df.show
+---+---+---+------+
| C0| C1| C2|    C3|
+---+---+---+------+
|111|  1|333| 100.0|
|112|  2|222| 505.0|
|113|  3|444| 510.0|
|114|  4|333| 600.0|
|115|  1|222| 510.0|
|116|  1|666| 520.0|
|117|  1|444| 540.0|
|118|  1|666|4400.0|
|119|  3|333|3300.0|
|120|  1|666|1500.0|
|121|  1|222|2500.0|
|122|  3|444|4500.0|
|123|  1|333|1100.0|
|124|  3|222|5100.0|
+---+---+---+------+

scala> df.count
res5: Long = 14 

scala> val df1=df.select("C0","C1")
df1: org.apache.spark.sql.DataFrame = [C0: string, C1: string]

scala> df1.show
+---+---+
| C0| C1|
+---+---+
|111|  1|
|112|  2|
|113|  3|
|114|  4|
|115|  1|
|116|  1|
|117|  1|
|118|  1|
|119|  3|
|120|  1|
|121|  1|
|122|  3|
|123|  1|
|124|  3|
+---+---+


scala> val df1=df.where("C1='1'").select("C0","C1")
df1: org.apache.spark.sql.DataFrame = [C0: string, C1: string]

scala> df1.show
+---+---+
| C0| C1|
+---+---+
|111|  1|
|115|  1|
|116|  1|
|117|  1|
|118|  1|
|120|  1|
|121|  1|
|123|  1|
+---+---+

scala> val df2=df.select("*")
df2: org.apache.spark.sql.DataFrame = [C0: string, C1: string, C2: string, C3: string]

using SQL query:
---------------------

scala> df.registerTempTable("test")  //directly from dataframe we can't use sql query,just create temp table

scala> val df_1=sqlContext.sql("select * from test")
df_1: org.apache.spark.sql.DataFrame = [C0: string, C1: string, C2: string, C3: string]

scala> val df_1=sqlContext.sql("select C0,C1 from test where C1='1'")
df_1: org.apache.spark.sql.DataFrame = [C0: string, C1: string]

scala> df_1.show
+---+---+
| C0| C1|
+---+---+
|111|  1|
|115|  1|
|116|  1|
|117|  1|
|118|  1|
|120|  1|
|121|  1|
|123|  1|
+---+---+

enable Schema discovery:(using option-inferSchema) {option to customize read properties)
scala> val df=sqlContext.read.format("csv").option("inferSchema","true").load("file:///home/cloudera/projects/spark-core/src/main/resources/sales.csv")
df: org.apache.spark.sql.DataFrame = [C0: int, C1: int, C2: int, C3: double]



scala> df.printSchema
root
 |-- C0: integer (nullable = true)
 |-- C1: integer (nullable = true)
 |-- C2: integer (nullable = true)
 |-- C3: double (nullable = true)


class-23-Aug-19
*****************************
scala> val df=sqlContext.read.format("csv").option("inferSchema","true").load("file:////home/cloudera/projects/spark-sql/src/main/resources/sales.csv")
df: org.apache.spark.sql.DataFrame = [C0: string, C1: string, C2: string, C3: string]

scala> df.show
+-------------+----------+------+----------+
|           C0|        C1|    C2|        C3|
+-------------+----------+------+----------+
|transactionId|customerId|itemId|amountPaid|
|          111|         1|     1|     100.0|
|          112|         2|     2|     505.0|
|          113|         3|     3|     510.0|
|          114|         4|     4|     600.0|
|          115|         1|     2|     510.0|
|          116|         1|     2|     520.0|
|          117|         1|     2|     540.0|
|          118|         1|     2|    4400.0|
|          119|         2|     3|    3300.0|
|          120|         1|     2|    1500.0|
|          121|         1|     4|    2500.0|
|          122|         1|     2|    4500.0|
|          123|         1|     4|    1100.0|
|          124|         1|     2|    5100.0|
+-------------+----------+------+----------+

scala> val df=sqlContext.read.format("csv").option("header","true").option("inferSchema","true").load("file:////home/cloudera/projects/spark-sql/src/main/resources/sales.csv")
df: org.apache.spark.sql.DataFrame = [transactionId: int, customerId: int, itemId: int, amountPaid: double]

scala> df.show
+-------------+----------+------+----------+
|transactionId|customerId|itemId|amountPaid|
+-------------+----------+------+----------+
|          111|         1|     1|     100.0|
|          112|         2|     2|     505.0|
|          113|         3|     3|     510.0|
|          114|         4|     4|     600.0|
|          115|         1|     2|     510.0|
|          116|         1|     2|     520.0|
|          117|         1|     2|     540.0|
|          118|         1|     2|    4400.0|
|          119|         2|     3|    3300.0|
|          120|         1|     2|    1500.0|
|          121|         1|     4|    2500.0|
|          122|         1|     2|    4500.0|
|          123|         1|     4|    1100.0|
|          124|         1|     2|    5100.0|
+-------------+----------+------+----------+


scala> df.printSchema
root
 |-- transactionId: integer (nullable = true)
 |-- customerId: integer (nullable = true)
 |-- itemId: integer (nullable = true)
 |-- amountPaid: double (nullable = true)

csv reader program in intellij-----------------------
*****************************

def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setMaster("local").setAppName("csv read")
    val sc=new SparkContext(conf)
    val sqlContext=new SQLContext(sc)

    val optionsMap= Map("header"-> "true" , "inferSchema"-> "true")

    val df=sqlContext.read.format("csv").options(optionsMap)
      .load("src/main/resources/sales.csv")

   // df.printSchema()

    //df.show

    val selectDf=df.where("customerId=1")
      .select("itemId","amountPaid")

    selectDf.show

    df.registerTempTable("test")
    //sqlContext.sql("select itemId,amountPaid from test where customerId=1").show

  Thread.sleep(5000000)
  }

----------------------------------------

class-24-Aug-19
**********************
how to read xml file  (SparkXmlReader)
----------------------------------

root
 |-- age: struct (nullable = true)
 |    |-- _VALUE: long (nullable = true)
 |    |-- _birthplace: string (nullable = true)
 |    |-- _born: string (nullable = true)
 |-- name: string (nullable = true)

+--------------------+-------+
|                 age|   name|
+--------------------+-------+
|[25,Bang,1990-02-24]|Hyukjin|
|[30,null,1985-01-01]|   Lars|
|[30,null,1980-01-01]|   Lion|
+--------------------+-------+


Q- based upon age group deive new column
------------------------------------------------

package dvs.learning
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SQLContext}
import org.apache.spark.sql.functions._

object SparkXmlReader {
  def main(args: Array[String]): Unit = {
    val conf=new SparkConf().setMaster("local").setAppName("XML Reader")
    val sc=new SparkContext(conf)
    val sqlContext=new SQLContext(sc)

    val xmlDf=sqlContext.read.format("xml").option("rowTag" , "person")
      .load("src/main/resources/ages.xml")

    xmlDf.printSchema()
    xmlDf.show

    //accesing columns using string but can't give alias name to the columns
    val flatDf=xmlDf.select("name","age._VALUE","age._birthplace","age._born")

    import sqlContext.implicits._ //getting implicits into scope which is using $ method
    //variants using columns

   val flatDf1=xmlDf.select(xmlDf("name"),col("age._VALUE").alias("age"),
     column("age._born").as("borndate"),
      $"age._birthplace".as("birthplace"))

    flatDf.printSchema()
    flatDf.show()

    flatDf1.printSchema()
    flatDf1.show() 

    //using select expression as sql flavour

    val flatsqlDf=xmlDf.selectExpr("name","age._VALUE as age",
      "age._birthplace as birthplace","age._born as borndate")

    flatsqlDf.printSchema()
    flatsqlDf.show()

    val newColDf= flatsqlDf.withColumn("age_cat",lit("unknown")) //to populate a constant value for column age_cat

   newColDf.show()

    val newColDf1= flatsqlDf.withColumn("age_cat",
      when(col("age")< 30,"young")
        .when(col("age")>=30 && col ("age") <= 50,"middle")
        .otherwise("old"))


    newColDf1.show()

    val castDf=newColDf1.withColumn("borndate",
      col("borndate").cast("date"))

    castDf.show()

    val renameDf=newColDf1.withColumnRenamed("age_cat","age_category")


    renameDf.drop("age").drop("borndate").show()
  }

}
------------------------------------------
Library program:(get separate entry for publish date for each book id ,based upon latest publish date get distict records)
****************************************
package dvs.learning

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.hive.HiveContext


object bookApp {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName("Library")
    val sc = new SparkContext(conf)
    val sqlContext = new HiveContext(sc)

    val bookDf = sqlContext.read.format("xml").option("rowTag", "book")
      .load("src/main/resources/books-nested-array.xml")

    //bookDf.show()
   // bookDf.printSchema()

    val flattenDf=bookDf.withColumn("publish_date",
      explode(col("publish_date")))

   // flattenDf.show()

    val rankDf=flattenDf.withColumn("rank",rank()
      .over(Window.partitionBy("_id").orderBy(desc("publish_date"))))

		
  }
}
---------------------------------------------
class- 25-Aug-2019
************************
read json file and find out customerwise total amount and no. of items
------------------------------------------------------------------------------

package dvs.learning

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}

object jsonReader {
  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setMaster("local").setAppName("jsonReader")
    val sc=new SparkContext(conf)
    val sqlContext= new SQLContext(sc)

   // val jsonDf= sqlContext.read.format("json").load("src/main/resources/sales.json")

    //for natively available formats
 val jsonDf=sqlContext.read.json("src/main/resources/sales.json")
    jsonDf.show()

    val countDf=jsonDf.groupBy("customerId").count()
    val sumDf= jsonDf.groupBy("customerId").sum("amountPaid")

    countDf.show()
    sumDf.show()

  }
}
--------------------------------------

join sales.csv and sales.json file and get total amount paid by each customer
****************************************

package dvs.learning

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.functions._

object unionApp {

  def main(args: Array[String]): Unit = {

    val conf=new SparkConf().setMaster("local").setAppName("jsonReader")
    val sc=new SparkContext(conf)
    val sqlContext= new SQLContext(sc)

    val jsonDf=sqlContext.read.json("src/main/resources/sales.json")

    val optionsMap= Map("header"-> "true" , "inferSchema"-> "true")

    val csvDf=sqlContext.read.format("csv").options(optionsMap)
      .load("src/main/resources/sales.csv")

    /*jsonDf.show()
    csvDf.show()

    jsonDf.printSchema
    csvDf.printSchema()*/

//json file is not in same order as csv so make it in same order before applying union

    val jsonDf1=jsonDf.select("transactionId","customerId","itemId","amountPaid")

    val unionDf=jsonDf1.unionAll(csvDf).distinct()

    jsonDf1.show()
    unionDf.show()

    val aggregateDf=unionDf.groupBy("customerId").agg(sum("amountPaid"))

    aggregateDf.show()
  }
}
-------------------------------------

class- 26-Aug-19
********************
how to handle malformed records :
program to give our customized schema:
-------------------------------------
package dvs.learning

import org.apache.spark.sql.{SQLContext, types}
import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}
import org.apache.spark.{SparkConf, SparkContext}


object sprarkExplicitSchema {
  def main(args: Array[String]): Unit = {
    val conf= new SparkConf().setAppName("CustomizedSchema").setMaster("local")
    val sc=new SparkContext(conf)
    val sqlContext=new SQLContext(sc)

    val s=StructType(Array(
      StructField("tid",IntegerType),
      StructField("cid",IntegerType),
      StructField("itemId",IntegerType,false),
      StructField("price",DoubleType)
      ))

   //mode 3 types- PERMISSIVE(default: mising tokens fill with null but exception when type mismatc)
   //DROPMALFORMED(process with good records and drop bad records)
   //FAILFAST(for missing tokens and type mismatch it fails with exception)
    
    val csvDf=sqlContext.read.format("csv").schema(s)
      .option("header","true")
      .option("mode" ,"PERMISSIVE" )
      .load("src/main/resources/sales-error.csv")

     csvDf.show()
    csvDf.printSchema()
  }

}
---------------------------------------------------------------------

converting Rdd to dataframte
2 ways- 1-if u have RDD +case class using toDf(implicit fun) or creteDataFrame method
2- Rdd[Row]+ Struct Data Type schema(using createDataFrame)
-------------------------

package dvs.learning

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

case class Sales(tid:Int,cid:Int,itid:Int,price:Double)

object SparkRDDToDataFrame {

  def main(args: Array[String]): Unit = {
 
 //getClass.getName gives projectname.objectname(so that it will be unique)
    val conf = new SparkConf().setAppName(getClass.getName)
      .setMaster("local")

    val sc = new SparkContext(conf)

    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._

    val rdd = sc.textFile("src/main/resources/sales.csv")

    val noHeaderRDD = rdd.filter(rec => !rec.startsWith("transactionId"))

    // alternative way of removing header from RDD
    //val headerRec = rdd.first()
    //val noHeaderRDD1 = rdd.filter(rec => rec != headerRec)

    def rddtoDF_CaseClass(rdd: RDD[String]) = {
      // tranforming RDD[String] to RDD[Sales]
      val schemaRDD = noHeaderRDD.map(rec => {
        val fieldArr = rec.split(",")
        Sales(fieldArr(0).toInt, fieldArr(1).toInt,
          fieldArr(2).toInt, fieldArr(3).toDouble)
      })

      val df = schemaRDD.toDF

      val df1 = sqlContext.createDataFrame(schemaRDD)
      df1.printSchema()
      df1.show()

      df.printSchema()
      df.show()
	  
	  
    }

    rddtoDF_CaseClass(noHeaderRDD)

    def rddToDF_Row_StructType(rdd: RDD[String]) = {
      // transforming RDD[String] to RDD[Row]
      val rowRDD = noHeaderRDD.map(rec => {
        val fieldArr = rec.split(",")
        Row(fieldArr(0).toInt, fieldArr(1).toInt,
          fieldArr(2).toInt, fieldArr(3).toDouble)
      })

      val s = StructType(Array(
        StructField("tid",IntegerType), StructField("cid",IntegerType),
        StructField("itid",IntegerType), StructField("price",DoubleType)
      ))

      val df = sqlContext.createDataFrame(rowRDD, s)
      df.printSchema()
      df.show()
	  
	  val rdd=df.rdd // converting DF to Rdd it gives RDD[ROW]
      rdd.foreach(x=> println(x.getDouble(3))) //print  4th column for each record as it's 
    }

    rddToDF_Row_StructType(noHeaderRDD)

  }

}
---------------------------------------------------------
convertRdd to Dataframe:1) RDD[String]+caseClass
---------------------------------------------------

scala> val rdd=sc.textFile("file:///home/cloudera/projects/spark-sql/src/main/resources/sales.csv")
rdd: org.apache.spark.rdd.RDD[String] = file:///home/cloudera/projects/spark-sql/src/main/resources/sales.csv MapPartitionsRDD[1] at textFile at <console>:27

scala> rdd.foreach(println)
[Stage 0:>                                                          (0 + 0) / 2]117,1,2,540.0
transactionId,customerId,itemId,amountPaid
118,1,2,4400.0
119,2,3,3300.0
111,1,1,100.0
112,2,2,505.0
120,1,2,1500.0
121,1,4,2500.0
122,1,2,4500.0
123,1,4,1100.0
124,1,2,5100.0
113,3,3,510.0
114,4,4,600.0
115,1,2,510.0
116,1,2,520.0
                                                                                
scala> val header=rdd.first()
header: String = transactionId,customerId,itemId,amountPaid


scala> header.foreach(print)
transactionId,customerId,itemId,amountPaid

scala> header.foreach(print)
transactionId,customerId,itemId,amountPaid
scala> val actualDataRdd=rdd.filter(e=>e!=header)
actualDataRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:31

scala> actualDataRdd.foreach(println)
111,1,1,100.0
112,2,2,505.0
113,3,3,510.0
114,4,4,600.0
115,1,2,510.0
116,1,2,520.0
117,1,2,540.0
118,1,2,4400.0
119,2,3,3300.0
120,1,2,1500.0
121,1,4,2500.0
122,1,2,4500.0
123,1,4,1100.0
124,1,2,5100.0

1)Define Caseclass for RDD(for Schema)
--------------------------
scala> case class Sales(tId:Int,cId:Int,itId:Int,price:Double)
defined class Sales

scala> val schemaRdd=actualDataRdd.map(e=>{
     | val f=e.split(",")
     | Sales(f(0).toInt,f(1).toInt,f(2).toInt,f(3).toDouble)
     | })
schemaRdd: org.apache.spark.rdd.RDD[Sales] = MapPartitionsRDD[3] at map at <console>:35

scala> schemaRdd.foreach(println)
Sales(117,1,2,540.0)
Sales(118,1,2,4400.0)
Sales(119,2,3,3300.0)
Sales(120,1,2,1500.0)
Sales(121,1,4,2500.0)
Sales(122,1,2,4500.0)
Sales(123,1,4,1100.0)
Sales(124,1,2,5100.0)
Sales(111,1,1,100.0)
Sales(112,2,2,505.0)
Sales(113,3,3,510.0)
Sales(114,4,4,600.0)
Sales(115,1,2,510.0)
Sales(116,1,2,520.0)

2 ways to convert using implcit Df or CreateDataFrame(hadoop services should start)
----------------------------------------------------------
scala> val Df=sqlContext.createDataFrame(schemaRdd)
Df: org.apache.spark.sql.DataFrame = [tId: int, cId: int, itId: int, price: double]

scala> Df.show()
+---+---+----+------+
|tId|cId|itId| price|
+---+---+----+------+
|111|  1|   1| 100.0|
|112|  2|   2| 505.0|
|113|  3|   3| 510.0|
|114|  4|   4| 600.0|
|115|  1|   2| 510.0|
|116|  1|   2| 520.0|
|117|  1|   2| 540.0|
|118|  1|   2|4400.0|
|119|  2|   3|3300.0|
|120|  1|   2|1500.0|
|121|  1|   4|2500.0|
|122|  1|   2|4500.0|
|123|  1|   4|1100.0|
|124|  1|   2|5100.0|
+---+---+----+------+


scala> import sqlContext.implicits._   // as toDF is implicit method u have to import it
import sqlContext.implicits._

scala> val Df= schemaRdd.toDF
Df: org.apache.spark.sql.DataFrame = [tId: int, cId: int, itId: int, price: double]

scala> Df.show()
+---+---+----+------+
|tId|cId|itId| price|
+---+---+----+------+
|111|  1|   1| 100.0|
|112|  2|   2| 505.0|
|113|  3|   3| 510.0|
|114|  4|   4| 600.0|
|115|  1|   2| 510.0|
|116|  1|   2| 520.0|
|117|  1|   2| 540.0|
|118|  1|   2|4400.0|
|119|  2|   3|3300.0|
|120|  1|   2|1500.0|
|121|  1|   4|2500.0|
|122|  1|   2|4500.0|
|123|  1|   4|1100.0|
|124|  1|   2|5100.0|
+---+---+----+------+

convertRdd to Dataframe:2) RDD[ROW]+StructTypeSchema  (import Row Package)
--------------------------------------------------------

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> val rowRdd=actualDataRdd.map(e=>{
     | val f= e.split(",")
     | Row(f(0).toInt,f(1).toInt,f(2).toInt,f(3).toDouble)})
rowRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[11] at map at <console>:43


scala> import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}
import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}

//assigning our own schema
scala> val s=StructType(Array(
     | StructField("itId",IntegerType),StructField("custId",IntegerType),
     | StructField("ItemId",IntegerType),StructField("amount",DoubleType)))
s: org.apache.spark.sql.types.StructType = StructType(StructField(itId,IntegerType,true), StructField(custId,IntegerType,true), StructField(ItemId,IntegerType,true), StructField(amount,DoubleType,true))

//creating dataframe (Row[RDD] +structType Schema
scala> val Df=sqlContext.createDataFrame(rowRdd,s)
Df: org.apache.spark.sql.DataFrame = [itId: int, custId: int, ItemId: int, amount: double]


scala> Df.show()
+----+------+------+------+
|itId|custId|ItemId|amount|
+----+------+------+------+
| 111|     1|     1| 100.0|
| 112|     2|     2| 505.0|
| 113|     3|     3| 510.0|
| 114|     4|     4| 600.0|
| 115|     1|     2| 510.0|
| 116|     1|     2| 520.0|
| 117|     1|     2| 540.0|
| 118|     1|     2|4400.0|
| 119|     2|     3|3300.0|
| 120|     1|     2|1500.0|
| 121|     1|     4|2500.0|
| 122|     1|     2|4500.0|
| 123|     1|     4|1100.0|
| 124|     1|     2|5100.0|
+----+------+------+------+


converting Df to RDD
--------------------------

scala> val Rdd1=Df.rdd
Rdd1: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[14] at rdd at <console>:50



scala> Rdd1.foreach(println)
[117,1,2,540.0]
[118,1,2,4400.0]
[119,2,3,3300.0]
[120,1,2,1500.0]
[121,1,4,2500.0]
[122,1,2,4500.0]
[123,1,4,1100.0]
[124,1,2,5100.0]
[111,1,1,100.0]
[112,2,2,505.0]
[113,3,3,510.0]
[114,4,4,600.0]
[115,1,2,510.0]
[116,1,2,520.0]



---------------------------------------------

class- 27-Aug-19
-------------------

join
*****
start hadoop and hive services

launch spark shell with dependencies as we have to load csv file

spark-shell --jars /home/cloudera/.ivy2/cache/com.databricks/spark-csv_2.10/jars/spark-csv_2.10-1.5.0.jar,/home/cloudera/.ivy2/cache/org.apache.commons/commons-csv/jars/commons-csv-1.1.jar

val dfjson=sqlContext.read.format("json").load("file:///home/cloudera/projects/spark-sql/src/main/resources/sales.json")

val dfcsv=sqlContext.read.format("csv").option("header","true").load("file:///home/cloudera/projects/spark-sql/src/main/resources/customers.csv")

If both tables have same column name we can go for below syntax using seq(),joinType.
If both tables have different column then go for join expression using "==="
-------------------------------------------------------------------------------
scala> val joinDf=dfjson.join(dfcsv,Seq("customerId"),"inner")
joinDf: org.apache.spark.sql.DataFrame = [customerId: bigint, amountPaid: double, itemId: bigint, transactionId: bigint, name: string]

scala> joinDf.show
+----------+----------+------+-------------+-------+                            
|customerId|amountPaid|itemId|transactionId|   name|
+----------+----------+------+-------------+-------+
|         1|     100.0|     1|          111|   John|
|         1|     500.0|     2|          115|   John|
|         1|     500.0|     2|          116|   John|
|         1|     500.0|     2|          117|   John|
|         1|     500.0|     2|          118|   John|
|         1|     500.0|     2|          120|   John|
|         1|     500.0|     4|          121|   John|
|         1|     500.0|     2|          122|   John|
|         1|     500.0|     4|          123|   John|
|         1|     500.0|     2|          124|   John|
|         2|     505.0|     2|          112|  Clerk|
|         2|     500.0|     3|          119|  Clerk|
|         3|     510.0|     3|          113|Micheal|
|         4|     600.0|     4|          114| Sample|
+----------+----------+------+-------------+-------+


scala> joinDf.explain
== Physical Plan ==
Project [customerId#1L,amountPaid#0,itemId#2L,transactionId#3L,name#7]
+- BroadcastHashJoin [cast(customerId#1L as double)], [cast(customerId#6 as double)], BuildLeft
   :- Scan JSONRelation[amountPaid#0,customerId#1L,itemId#2L,transactionId#3L] InputPaths: file:/home/cloudera/projects/spark-sql/src/main/resources/sales.json
   +- ConvertToUnsafe
      +- Scan CsvRelation(<function0>,Some(file:///home/cloudera/projects/spark-sql/src/main/resources/customers.csv),true,,,",null,#,PERMISSIVE,COMMONS,false,false,false,null,false,null,,null,100000)[customerId#6,name#7]

broadcast join using broadcast function:(mapside)(when 1 dataframe is less than 10 MB bydefault it does broadcast join automatically)
---------------------------------------
If u want to do forcefuully brodcast join use broadcast method or whle spark submit u can give change the default size 
-----------------------------------------------------------------------
scala> val joinDf=dfjson.join(broadcast(dfcsv),Seq("customerId"),"inner")
joinDf: org.apache.spark.sql.DataFrame = [customerId: bigint, amountPaid: double, itemId: bigint, transactionId: bigint, name: string]

scala> val joinDf=broadcast(dfjson).join(dfcsv,Seq("customerId"),"inner")
joinDf: org.apache.spark.sql.DataFrame = [customerId


class- 28-Aug=19
********************

scala> val salesDf=sqlContext.read.format("json").load("file:///home/cloudera/projects/spark-sql/src/main/resources/sales.json")
salesDf: org.apache.spark.sql.DataFrame = [amountPaid: double, customerId: bigint, itemId: bigint, transactionId: bigint]

scala> val custDf=sqlContext.read.format("csv").option("header","true").load("file:///home/cloudera/projects/spark-sql/src/main/resources/customers.csv")
custDf: org.apache.spark.sql.DataFrame = [customerId: string, name: string]

scala> val joinDf=salesDf.join(custDf,Seq("customerId"),"inner")
joinDf: org.apache.spark.sql.DataFrame = [customerId: bigint, amountPaid: double, itemId: bigint, transactionId: bigint, name: string]

scala> joinDf.show
+----------+----------+------+-------------+-------+
|customerId|amountPaid|itemId|transactionId|   name|
+----------+----------+------+-------------+-------+
|         1|     100.0|     1|          111|   John|
|         1|     500.0|     2|          115|   John|
|         1|     500.0|     2|          116|   John|
|         1|     500.0|     2|          117|   John|
|         1|     500.0|     2|          118|   John|
|         1|     500.0|     2|          120|   John|
|         1|     500.0|     4|          121|   John|
|         1|     500.0|     2|          122|   John|
|         1|     500.0|     4|          123|   John|
|         1|     500.0|     2|          124|   John|
|         2|     505.0|     2|          112|  Clerk|
|         2|     500.0|     3|          119|  Clerk|
|         3|     510.0|     3|          113|Micheal|
|         4|     600.0|     4|          114| Sample|
+----------+----------+------+-------------+-------+

join as expression when 2 columns have diff. name in diff dataset: (in spark SQL comparision is  triple equalto "===")
--------------------------------
when u use join as expression join column will appear twice (ex-customerId)
----------------------------------------------------------------
scala> val joinDf1=salesDf.join(custDf,salesDf("customerId")===custDf("customerId"),"Inner")
joinDf1: org.apache.spark.sql.DataFrame = [amountPaid: double, customerId: bigint, itemId: bigint, transactionId: bigint, customerId: string, name: string]

scala> joinDf1.show
+----------+----------+------+-------------+----------+-------+
|amountPaid|customerId|itemId|transactionId|customerId|   name|
+----------+----------+------+-------------+----------+-------+
|     100.0|         1|     1|          111|         1|   John|
|     500.0|         1|     2|          115|         1|   John|
|     500.0|         1|     2|          116|         1|   John|
|     500.0|         1|     2|          117|         1|   John|
|     500.0|         1|     2|          118|         1|   John|
|     500.0|         1|     2|          120|         1|   John|
|     500.0|         1|     4|          121|         1|   John|
|     500.0|         1|     2|          122|         1|   John|
|     500.0|         1|     4|          123|         1|   John|
|     500.0|         1|     2|          124|         1|   John|
|     505.0|         2|     2|          112|         2|  Clerk|
|     500.0|         2|     3|          119|         2|  Clerk|
|     510.0|         3|     3|          113|         3|Micheal|
|     600.0|         4|     4|          114|         4| Sample|
+----------+----------+------+-------------+----------+-------+

scala> joinDf1.drop("customerId")  //drops both custid from both Dataframe
res5: org.apache.spark.sql.DataFrame = [amountPaid: double, itemId: bigint, transactionId: bigint, name: string]

scala> joinDf1.drop(custDf("customerId")) //drops only single custId as we have provided dataframe name
res6: org.apache.spark.sql.DataFrame = [amountPaid: double, customerId: bigint, itemId: bigint, transactionId: bigint, name: string]

to check whether a column is null or not:
-------------------------------------
scala> val newDf=joinDf.withColumn("test",when(col("amountPaid").isNull,"NA").
     | otherwise("exists"))
newDf: org.apache.spark.sql.DataFrame = [customerId: bigint, amountPaid: double, itemId: bigint, transactionId: bigint, name: string, test: string]
------------------------------------------------------

Yelp Assignment:
---------------------
package Yelplearning

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.expressions.Window
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.hive.HiveContext

/**
  * Created by Satyabrat on 8/26/2019.
  */
object yelpAssignment {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setMaster("local").setAppName(getClass.getName)
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val inpDf = sqlContext.read.
      json("C:\\Users\\Satyabrat\\Desktop\\DVS-MAR-2019-SWs\\Arjun_Spark_Scala\\Spark_Material\\Assignments\\Spark_SQL\\yelp_academic_dataset_business.json")


    inpDf.printSchema()
    //inpDf.take(1).foreach(println)
    //inpDf.first
    inpDf.show(5) //Q1-shows first 5 lines


    //Q2-get top 10 States and cities in total no of reviews
    //inpDf.selectExpr("city","state").orderBy(desc("review_count")).show(10)

    //Q3-average no. of reviews per business star rating
    //inpDf.groupBy("stars").agg(avg("review_count")).orderBy("stars").show()

     //Q4-Top Business with review>1000 in sorted order
    val topBusinessDf=inpDf.selectExpr("categories","review_count")
      .where("review_count > '1000'").orderBy(desc("review_count"))

   println(topBusinessDf.count())
    //Q5-Display Saturday open and close timings for few business
    //inpDf.selectExpr("categories","hours").
    //  equals("hours.Saturday").show(2)

      }
}
------------------------------------------------
clas:29-Aug-19
***************************
Read csv file and sales file from database:
----------------------------------------------

package dvs.learning

import dvs.learning.SparkRDDToDataFrame.getClass
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

object SparkJdbcReader {

  def main(args: Array[String]): Unit = {
    
    val conf = new SparkConf().setAppName(getClass.getName)
      .setMaster("local")

    val sc = new SparkContext(conf)

    val sqlContext = new SQLContext(sc)

    val connProps= new java.util.Properties()
    connProps.setProperty("user","root")
    connProps.setProperty("password","cloudera")

  //  val jdbcDf= sqlContext.read
     // .jdbc("jdbc:mysql://localhost:3306/ecommerce","sales",connProps)


    val jdbcDf= sqlContext.read
      .jdbc("jdbc:mysql://localhost:3306/ecommerce",
        "(select * from sales where customerId = 1) tmp",connProps)

  jdbcDf.show()
    jdbcDf.printSchema()
    println(jdbcDf.rdd.getNumPartitions)

    jdbcDf.write.format("parquet").save("src/main/resources/sales-parquet")

  }

}
-----------------------------------------------------------------------------------
parallel Read using jdbc (if any shuffle phase then 200 partitions, while writing to Db also 200 connections which
creates bottleneck so, always go for coalesce and be careful)
--------------------------------------
4 properties:(all should be used together)*******
1-partition column(unique col like splitBy in sqoop
2- lowerbound - in real time give dynamically(use select min(col) from table and store it in some variable and pass it)
3- upperbound  - in real time give dynamically
4-numpartititons - no. of tasks
--------------------

package dvs.learning

import dvs.learning.SparkRDDToDataFrame.getClass
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{SQLContext, SaveMode}

object SparkJdbcReader {

  def main(args: Array[String]): Unit = {
    //getClass.getName gives projectname.objectname(so that it will be unique)
    val conf = new SparkConf().setAppName(getClass.getName)
      .setMaster("local")

    val sc = new SparkContext(conf)

    val sqlContext = new SQLContext(sc)

    val connProps= new java.util.Properties()
    connProps.setProperty("user","root")
    connProps.setProperty("password","cloudera")

  //  val jdbcDf= sqlContext.read
     // .jdbc("jdbc:mysql://localhost:3306/ecommerce","sales",connProps)


   /* val jdbcDf= sqlContext.read
      .jdbc("jdbc:mysql://localhost:3306/ecommerce",
        "(select * from sales where customerId = 1) tmp",connProps) */

    //alternate way to load
    val jdbcDf=sqlContext.read.format("jdbc")
        .option("url","jdbc:mysql://localhost:3306/ecommerce")
        .option("dbTable","(select * from sales) tmp")
        .option("user","root")
         .option("password","cloudera")
        .option("partitionColumn","customerId")
        .option("lowerBound","1")
        .option("upperBound","4")
        .option("numPartitions","4")
       .load()

  jdbcDf.show()
    jdbcDf.printSchema()
    println(jdbcDf.rdd.getNumPartitions)

   // jdbcDf.write.mode("append").format("parquet")
     // .save("src/main/resources/sales-parquet")

    jdbcDf.cache()

    //path using savemode constant writing to lfs
    jdbcDf.write.mode(SaveMode.Overwrite).format("parquet")
      .option("compression","gzip")
      .partitionBy("customerId")
     .save("src/main/resources/sales-parquet")

    //jdbcDf has 4 partitions,coleascing to 1 partition to have
    //save to rdbms
    /*jdbcDf.coalesce(1).write.format("jdbc")
      .option("url","jdbc:mysql://localhost:3306/ecommerce")
      .option("dbTable","sales_test")
      .option("user","root")
      .option("password","cloudera")
      .save()*/ // above in 2.x but better always use direct jdbc read and write like below

    jdbcDf.coalesce(1).write.mode("overwrite")
      .jdbc("jdbc:mysql://localhost:3306/ecommerce","sales_test",connProps)

  }

}
-----------------------------------------------
class- 30-Ag-19
cassandra:
*************************____________________________________
[cloudera@quickstart ~]$ sudo service cassandra start
Starting Cassandra: OK
[cloudera@quickstart ~]$ sudo jps
3042 RemoteMavenServer
2930 Main
3299 Jps
[cloudera@quickstart ~]$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> help

Documented shell commands:
===========================
CAPTURE  CLS          COPY  DESCRIBE  EXPAND  LOGIN   SERIAL  SOURCE   UNICODE
CLEAR    CONSISTENCY  DESC  EXIT      HELP    PAGING  SHOW    TRACING

CQL help topics:
================
AGGREGATES               CREATE_KEYSPACE           DROP_TRIGGER      TEXT     
ALTER_KEYSPACE           CREATE_MATERIALIZED_VIEW  DROP_TYPE         TIME     
ALTER_MATERIALIZED_VIEW  CREATE_ROLE               DROP_USER         TIMESTAMP
ALTER_TABLE              CREATE_TABLE              FUNCTIONS         TRUNCATE 
ALTER_TYPE               CREATE_TRIGGER            GRANT             TYPES    
ALTER_USER               CREATE_TYPE               INSERT            UPDATE   
APPLY                    CREATE_USER               INSERT_JSON       USE      
ASCII                    DATE                      INT               UUID     
BATCH                    DELETE                    JSON            
BEGIN                    DROP_AGGREGATE            KEYWORDS        
BLOB                     DROP_COLUMNFAMILY         LIST_PERMISSIONS
BOOLEAN                  DROP_FUNCTION             LIST_ROLES      
COUNTER                  DROP_INDEX                LIST_USERS      
CREATE_AGGREGATE         DROP_KEYSPACE             PERMISSIONS     
CREATE_COLUMNFAMILY      DROP_MATERIALIZED_VIEW    REVOKE          
CREATE_FUNCTION          DROP_ROLE                 SELECT          
CREATE_INDEX             DROP_TABLE                SELECT_JSON     

cqlsh> cqlsh -help
   ... 
   ... 
   ... 
   ... 
Statements are terminated with a ';'.  You can press CTRL-C to cancel an incomplete statement.
   ... ;
SyntaxException: line 1:0 no viable alternative at input 'cqlsh' ([cqlsh]...)
cqlsh> 
cqlsh> describe keyspaces;

system_schema  system_auth  system  system_distributed  system_traces  ecommerce

cqlsh> use ecommerce
   ... ;
cqlsh:ecommerce> describe table;
Improper describe command.
cqlsh:ecommerce> describe tables;

test  sales  sales1

cqlsh:ecommerce> select * from sales1;

 customerid | date | itemid
------------+------+--------
          1 | null |      4
          2 | null |      3
          4 | null |      4
          3 | null |      3

(4 rows)
cqlsh:ecommerce> describe table sales1;

CREATE TABLE ecommerce.sales1 (
    customerid int PRIMARY KEY,
    date timestamp,
    itemid int
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh:ecommerce> drop table sales;

Q- Read sales.csv and customers.csv file find out  customer Name & total amount paid  and wite output to cassandra:
---------------------------------------------------------------------------------------------------------------
for writting to cassandra: u should create empty table first(in all databse all schema is in small case)
-----------------------------------------------------
write mode- 1-by default error if exists
2-append (will ovrride data)
3-overwrite (will override data, but u must specify options("confirm.truncate","true")
--------------------------
package dvs.learning

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.functions._

object cassandraApp {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName(getClass.getName)
    val sc = new SparkContext(conf)

    conf.set("spark.cassandra.connection.host", "localhost")
    val sqlContext = new SQLContext(sc)

    //read file from cassandra
   /* val cDf = sqlContext.read.format("org.apache.spark.sql.cassandra")
      .option("keyspace", "ecommerce")
      .option("table", "sales1").load()

    cDf.printSchema()
    cDf.show() */

    val salesDf=sqlContext.read.format("csv").option("header","true")
      .option("inferSchema","true").load("src/main/resources/sales.csv")

    val custDf=sqlContext.read.format("csv").option("header","true")
      .option("inferSchema","true").load("src/main/resources/customers.csv")

    //salesDf.show()
    //custDf.show()

    val joinDf=salesDf.join(custDf,Seq("customerId"),"inner")

    joinDf.show()
    val newjoinDf=joinDf.withColumnRenamed("name","custname")


    val aggDf=newjoinDf.groupBy("custname")
      .agg(sum("amountPaid").as("totalamount"))

    aggDf.show()

    val outDf= aggDf.select("custname","totalamount"  )
    outDf.show()

    //write to cassandra

 outDf.write
   //.mode("overwrite")
     .mode("append")
   .format("org.apache.spark.sql.cassandra")
   .option("keyspace", "ecommerce")
  .option("table", "sales_agg")
     // .option("confirm.truncate","true")
      .save()

}
}
---------------------------------------------
---------------------------------------------
